{"cells":[{"cell_type":"markdown","metadata":{"id":"IEvqT7ibs91z"},"source":["# Table of Contents\n","\n","\n","[Definition of Utility Functions](#util_func) <br/>\n","[Definition of Faster R-CNN Wrapper Class](#faster_rcnn_class) <br/>\n","[Definition of Model Pipeline](#model_pipeline) <br/>\n","[Faster R-CNN Implementations](#fasterRCNN): <br/>\n","- [Baseline Implementation](#fasterRCNN_baseline) <br/>\n","- [Custom backbones](#custom_backbones) <br/>\n","\n","<br/><br/>\n","[YOLOv11 Implementation](#YOLOv11): <br/>\n","[YOLO with Image Slicing (SAHI)](#yolo_sahi): <br/>\n","[Appendix A](#AppenA): <br/>"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"PnBVLmZo69V_","executionInfo":{"status":"ok","timestamp":1738238433240,"user_tz":0,"elapsed":181,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["#set up github"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1738238439910,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"uLIWdd_hctyT","outputId":"96753859-3823-4415-9038-446b741dc2ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Object-Detection-Neural-Networks\n","* \u001b[32mmain\u001b[m\n"]}],"source":["%cd '/content/Object-Detection-Neural-Networks'\n","!git branch"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1738238440995,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"mkwGtsS-Xm7F","outputId":"c12afbe4-647a-4495-f349-019f99d7968c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Branch 'custom_backbones' set up to track remote branch 'custom_backbones' from 'origin'.\n","Switched to a new branch 'custom_backbones'\n"]}],"source":["#CURRENT BRANCH\n","!git switch \"custom_backbones\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"RLFEC068ecPq","executionInfo":{"status":"ok","timestamp":1738178552551,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["git_push(\"initial implementation of custom resnet backbone\", \"custom_backbones\")"]},{"cell_type":"markdown","metadata":{"id":"2UDsTyiRurPL"},"source":["## Definition of Utility Functions: <a class=\"anchor\" name=\"util_func\"></a>"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gY2GwNcpyJD0","executionInfo":{"status":"ok","timestamp":1738178553789,"user_tz":0,"elapsed":2,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["import shutil\n","\n","# !git pull\n","# !git status\n","\n","#check remotes\n","#!git remote -v\n","\n","\n","def git_push(message, branch):\n","  #move to git repo before using git commands\n","  %cd '/content/Object-Detection-Neural-Networks'\n","\n","  #copy current notebook to cloned git repo\n","  file_to_copy = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/UoL Final Project based on CM3015 template \"deep learning on public dataset\".ipynb'\n","  git_repo = '/content/Object-Detection-Neural-Networks'\n","\n","  #overwrite the file in the local cloned github repo\n","  shutil.copy(file_to_copy, git_repo)\n","\n","  !git add 'UoL Final Project based on CM3015 template \"deep learning on public dataset\".ipynb'\n","\n","  !git commit -a -m \"{message}\"\n","  !git push origin \"{branch}\"\n","\n","def publish_branch(branch):\n","  %cd '/content/Object-Detection-Neural-Networks'\n","  !git push -u origin \"{branch}\"\n","\n","def create_branch(branch):\n","  %cd '/content/Object-Detection-Neural-Networks'\n","  !git checkout -b \"{branch}\""]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":771,"status":"ok","timestamp":1738178554558,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"16yjq-CUqS3I","outputId":"8e944874-127d-480c-a26a-b685009de9e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import random\n","import numpy as np\n","import torch\n","\n","#print(torch.__version__)\n","device = torch.device(\"cuda:0\")\n","print(device)\n","\n","#the google style guide suggest that a docstring is not necessary for the set_seeds() function\n","#as it does not meet any of the following critera\n","#A docstring is mandatory for every function that has one or more of the following properties:\n","  # - being part of the public API\n","  # - nontrivial size\n","  # - non-obvious logic\n","#https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings\n","\n","#THE SET_SEEDS FUNCTION IS SOURCED FROM:\n","#https://learnopencv.com/fine-tuning-faster-r-cnn/#aioseo-code-walkthrough\n","def set_seeds():\n","  # fix random seeds\n","  SEED_VALUE = 42\n","  random.seed(SEED_VALUE)\n","  np.random. seed(SEED_VALUE)\n","  torch.manual_seed(SEED_VALUE)\n","  if torch.cuda.is_available:\n","    torch.cuda.manual_seed(SEED_VALUE)\n","    torch.cuda.manual_seed_all(SEED_VALUE)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","import numpy as np\n","import os\n","import cv2\n","from torchvision import transforms\n","\n","class UAVImageDataset(Dataset):\n","    \"\"\"Overrides the PyTorch Dataset class\n","\n","      Defines how raw data should be read and transformed ready for model training\n","\n","      Attributes:\n","          data_dir_imgs: file string path to raw images\n","          data_dir_targets: file string path to raw target data\n","          imgs: the formatted images ready for training\n","          targets: the formatted targets ready for training\n","    \"\"\"\n","\n","    def __init__(self, data_dir_imgs: str, data_dir_targets: str, transform=None) -> None:\n","        \"\"\"Loads image file paths and targets into class (instance) attributes\n","\n","          Img file paths and img targets are stored in separate arrays: 'self.all_imgs' and 'self.all_targets'\n","          Targets (boxes and labels) are compiled into a dictionary for each image\n","          An image file path and it's targets are stored at the same index in their respective arrays\n","\n","        Args:\n","          data_dir_imgs: a string representing the file path to the image folder of the dataset\n","          data_dir_targets: a string representing the file path to the 'targets' folder of the dataset\n","        \"\"\"\n","\n","        self.data_dir_imgs = data_dir_imgs\n","        self.data_dir_targets = data_dir_targets\n","        self.transforms = transforms\n","\n","        file_names = os.listdir(path=data_dir_imgs)\n","        number_of_images = len(file_names)\n","\n","        self.all_imgs = []\n","        self.all_targets = []\n","\n","        for i in range(number_of_images):\n","            img = data_dir_imgs + '/'+ file_names[i]\n","            self.all_imgs.append(img)\n","\n","            # use 'with open()' context manager so that the file is automatically closed after block of code\n","            #remove .jpg or .png file extension and retrieve corresponding labels\n","            with open(data_dir_targets + '/' + file_names[i][:-4]+ '.txt','r') as f:\n","                file_lines = f.read().splitlines()\n","\n","                img_targets = {\n","                        \"boxes\": [],\n","                        \"labels\": []\n","                    }\n","\n","                #.txt label files with no annotations cause training error;\n","                if len(file_lines) ==0:\n","                    #follow 2 lines of code sourced from:\n","                    #https://discuss.pytorch.org/t/fasterrcnn-images-with-no-objects-present-cause-an-error/117974/13\n","                    #to address the issue of images with no ground truth bounding boxes\n","                    img_targets[\"boxes\"] = torch.as_tensor(np.array(np.zeros((0, 4)), dtype=float))\n","                    img_targets[\"labels\"] = torch.as_tensor(np.array([], dtype=int), dtype=torch.int64)\n","                else:\n","                    for file_line in file_lines:\n","                        indiv_items = file_line.split(\" \")\n","                        label, x1, y1, width, height = indiv_items\n","                        label = int(label)\n","                        #YOLO label format is x-centre, y-centre, width, height\n","                        img_targets['boxes'].append([x1, y1, width, height])\n","                        img_targets['labels'].append(label)\n","\n","                self.all_targets.append(img_targets)\n","\n","    def __len__(self):\n","        return len(self.all_imgs)\n","\n","    def __getitem__(self, idx: int) -> tuple:\n","      \"\"\"Returns image and target at the specified index\n","\n","          Images are loaded and then converted to tensors (as required by PyTorch for model training)\n","          Raw bbox coordinates are in YOLOv8 format (x,y,w,h) and are relative to img width and height\n","          __getitem__ converts these coordinates to x,y,x2,y2 ready for input to Faster R-CNN\n","\n","        Args:\n","          idx: the index of the img (and targets) to retrieve\n","\n","        Returns:\n","            tuple of the img and targets at the specified index\n","\n","        Raises: AssertionError if a bbox with negative height or width is found\n","        \"\"\"\n","      img = self.all_imgs[idx]\n","      img_read = cv2.imread(img)\n","      image_transform = transforms.ToTensor()  #cv library reads as a numpy array, needs to be a pytorch tensor to be compatible\n","      img = image_transform(img_read)\n","\n","      # identify if the data point has been pre-processed\n","      # this accounts for validation set which is loaded multiple times\n","      if len(self.all_targets[idx]['labels'])!= 0:\n","        if isinstance(self.all_targets[idx]['boxes'][0][0], str):\n","\n","          list_tensors = []\n","          for i in range(len(self.all_targets[idx]['boxes'])):\n","            x1, y1, width, height = self.all_targets[idx]['boxes'][i]\n","            x1, width,  = float(x1)*img_read.shape[1], float(width)*img_read.shape[1],\n","            y1, height = float(y1)*img_read.shape[0], float(height)*img_read.shape[0]\n","            x2 = x1 + width\n","            y2 = y1 + height\n","\n","            # check for invalid bboxes\n","            if x2 < x1 or x1 == x2 or y2 < y1 or y1 == y2:\n","                raise AssertionError('Invalid Ground Truth Bounding Box')\n","                print(x1, y1, x2, y2)\n","\n","            list_tensors.append(torch.tensor([x1-width/2, y1-height/2, x2-width/2, y2-height/2]))\n","\n","          self.all_targets[idx]['boxes'] = torch.stack(list_tensors, dim=0)\n","          self.all_targets[idx]['labels'] = torch.as_tensor(self.all_targets[idx]['labels'])\n","\n","\n","      return img, self.all_targets[idx]\n"],"metadata":{"id":"zxh5-TKu7tPQ","executionInfo":{"status":"ok","timestamp":1738178555873,"user_tz":0,"elapsed":1317,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","def format_batch(batch: torch.utils.data.dataset.Subset ,\n","                 model: torchvision.models.detection) -> dict:\n","  \"\"\"Formats batch for processing on GPU\n","\n","    Gathers all images and targets into separate lists ready to feed into the model\n","    Also moves all images and targets to the same device as the model ready for processing\n","\n","    Args:\n","        batch: a subset of the PyTorch dataset\n","\n","        model: used to determine which device the batch data should be sent to\n","\n","    Returns:\n","        dictionary of prepared batch data ; has 2 keys 'images and targets'\n","\n","    Raises:\n","        None\n","    \"\"\"\n","\n","  images = []\n","  targets = []\n","  device = next(model.parameters()).device\n","  for img, targets_dict in batch:\n","      images.append(img)\n","      targets.append(targets_dict)\n","\n","      # Data type conversions required by Mac GPU 'MPS'\n","      #format to tensor of dtype float 32 as supported by MPS\n","      # targets_dict['boxes'] = targets_dict['boxes'].type(torch.float32)\n","      # targets_dict['labels'] = targets_dict['labels'].type(torch.int64)\n","\n","  #move relevant data to GPU\n","  #otherwise error: RuntimeError: Mismatched Tensor types in NNPack convolutionOutput\n","  images = [ t.to(device) for t in images]\n","  targets = [ {'boxes':d['boxes'].to(device), 'labels':d['labels'].to(device)} for d in targets]\n","\n","  return {'images': images, 'targets': targets}"],"metadata":{"id":"fqZ4zaUT7lG2","executionInfo":{"status":"ok","timestamp":1738178555874,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Function check_bboxes is sourced from the forward() method in PyTorch source code file generalized_rcnn.py\n","# https://github.com/pytorch/vision/blob/f40c8df02c197d1a9e194210e40dee0e6a6cb1c3/torchvision/models/detection/generalized_rcnn.py\n","# The source code for this code block had the following comment:\n","# \"TODO: Move this to a function\"\n","# So I have acted on this comment and added a docstring to the function and type hinting\n","def check_bboxes(targets:list) -> None:\n","  \"\"\"Checks that bounding boxes are of positive height and width\n","\n","    Args:\n","        targets: a list of dictionaries ; each dictionary has 2 keys 'boxes' and 'labels'\n","                (keys boxes and lables both have tensor values)\n","\n","    Returns:\n","        None\n","\n","    Raises:\n","        Value Error if bounidng boxes have negative height or width\n","    \"\"\"\n","  if targets is not None:\n","        for target_idx, target in enumerate(targets):\n","            boxes = target[\"boxes\"]\n","            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n","            if degenerate_boxes.any():\n","                # print the first degenerate box\n","                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                degen_bb: List[float] = boxes[bb_idx].tolist()\n","                raise ValueError(\n","                    \"All bounding boxes should have positive height and width.\"\n","                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n","                )\n","            else:\n","              return\n"],"metadata":{"id":"bJisk5MlApg6","executionInfo":{"status":"ok","timestamp":1738178555874,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NDIbHuImB1ih","executionInfo":{"status":"ok","timestamp":1738178555874,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_loss(epochs: list, training_losses: list, val_losses: list) -> None:\n","  \"\"\"Creates line graph of both training and validation loss after all epochs have been completed\n","\n","    Implemented using matplotlib plot()\n","\n","    Args:\n","        epochs: list containing values ranging from 1 to <num_epochs>,\n","\n","        training_losses: list containing training loss for each epoch,\n","\n","        val_losses: list containing validation loss for each epoch,\n","\n","    Returns:\n","        No value returned ; only plt.show()\n","\n","    Raises:\n","        AssertionError: Different number of training loss, validation loss and epochs\n","    \"\"\"\n","  assert len(epochs) == len(training_losses) == len(val_losses), \"Each argument must be the same length\"\n","\n","  fig, ax = plt.subplots()\n","  ax.set_xlabel(\"Epochs\")\n","  ax.set_ylabel(\"Epoch loss\")\n","  training_plot, = ax.plot(epochs, training_losses, label=\"Training Loss\")\n","  val_plot, = ax.plot(epochs, val_losses, label=\"Validation Loss\")\n","  ax.set_title(\"Training and Validation Loss\")\n","  handles, labels = ax.get_legend_handles_labels()\n","  ax.legend(handles, labels)\n","\n","  fig.show()"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import torch\n","\n","def visualisePredictions(img_id: int,\n","                         test_dataset: torch.utils.data.dataset.Subset,\n","                         predicted: list) -> None:\n","\n","  \"\"\"Visualises model predictions (bounding boxes) on top of the corresponding image\n","\n","    Retrieves the original image from the test set and plots bouning boxes onto the image.\n","    Each classes is represented by a different colour bounding box.\n","\n","    Args:\n","        img_id: the number of the image within the batch,\n","\n","        test_dataset: Subset of PyTorch dataset ; each item in the array is a tuple consisting of\n","        a tensor representing the image and a dictionary containing 'boxes' and 'labels' keys,\n","\n","        precicted: array of model predictions for set of images ; the array consists of\n","        dictionaries each with 2 keys: boxes and labels\n","\n","    Returns:\n","        No value returned ; only plt.show()\n","\n","    Raises:\n","        IndexError: An error occurred accessing the image at the specified index.\n","    \"\"\"\n","\n","  try:\n","    img = test_dataset[img_id][0].permute(1,2,0)\n","    predictions = predicted[img_id]\n","\n","    fig, ax = plt.subplots()\n","    ax.imshow(img)\n","\n","    for (index, box) in enumerate(predictions['boxes']):\n","      #move data from GPU to CPU ready for visualisation\n","      cpu_box = box.cpu()\n","      predicted_label = predictions['labels'][index]\n","\n","      #class names: ['migrating_single', 'plastic', 'water_hyacinth']\n","      colour_mappings = ['y', 'white', 'orange']\n","      #draw bounding box\n","      try:\n","        bbox = patches.Rectangle((cpu_box[0], cpu_box[1]), cpu_box[2], cpu_box[3], linewidth=1, edgecolor=colour_mappings[predicted_label], facecolor='none')\n","      except IndexError:\n","        print(\"Bounding box does not have 4 coordinates\")\n","        print(cpu_box)\n","        raise\n","\n","      # Add the patch to the Axes\n","      ax.add_patch(bbox)\n","    plt.show()\n","\n","  except IndexError:\n","      print(f'Provided dataset is of length {len(test_dataset)} - image index {img_id} not within range')\n","      raise\n"],"metadata":{"id":"IoSG5F2Iv9_m","executionInfo":{"status":"ok","timestamp":1738178555874,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Definition of Faster R-CNN Wrapper Class: <a class=\"anchor\" name=\"faster_rcnn_class\"></a>"],"metadata":{"id":"IfF-WOVV6p9j"}},{"cell_type":"code","source":["!pip install torchmetrics\n","!pip install faster-coco-eval\n","!pip install pycocotools\n","\n","#https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from torchvision.models.detection import FasterRCNN\n","from typing import Tuple, List, Dict, Optional\n","import torch\n","from torch import Tensor\n","from torch.utils.data import Dataset, DataLoader\n","from collections import OrderedDict\n","from torchvision.models.detection.roi_heads import fastrcnn_loss\n","from torchvision.models.detection.rpn import concat_box_prediction_layers\n","import numpy as np\n","import datetime\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","class FasterRCNNWrapper():\n","  \"\"\"A wrapper class that adds to the functionality of the PyTorch FasterRCNN class\n","\n","    The FasterRCNN model initialised by PyTorch FasterRCNN is stored in the 'model' attribute\n","    The methods of this class connect to the 'model' attribute to train and test the model\n","\n","    Attributes:\n","        model: the object initialiased by PyTorch FasterRCNN init()\n","    \"\"\"\n","  # Call the init method of FasterRCNN within the init() of this subclass\n","  # so that the model can be saved as an instance attribute\n","  # pass the default values of parameters to FasterRCNN init()\n","  # in case class is instantiate without specifying all values\n","  def __init__(self,\n","              backbone,\n","              num_classes=None,\n","              # transform parameters\n","              min_size=800,\n","              max_size=1333,\n","              image_mean=None,\n","              image_std=None,\n","              # RPN parameters\n","              rpn_anchor_generator=None,\n","              rpn_head=None,\n","              rpn_pre_nms_top_n_train=2000,\n","              rpn_pre_nms_top_n_test=1000,\n","              rpn_post_nms_top_n_train=2000,\n","              rpn_post_nms_top_n_test=1000,\n","              rpn_nms_thresh=0.7,\n","              rpn_fg_iou_thresh=0.7,\n","              rpn_bg_iou_thresh=0.3,\n","              rpn_batch_size_per_image=256,\n","              rpn_positive_fraction=0.5,\n","              rpn_score_thresh=0.0,\n","              # Box parameters\n","              box_roi_pool=None,\n","              box_head=None,\n","              box_predictor=None,\n","              box_score_thresh=0.05,\n","              box_nms_thresh=0.5,\n","              box_detections_per_img=100,\n","              box_fg_iou_thresh=0.5,\n","              box_bg_iou_thresh=0.5,\n","              box_batch_size_per_image=512,\n","              box_positive_fraction=0.25,\n","              bbox_reg_weights=None,\n","              **kwargs,) -> None:\n","    \"\"\"Initialises the 'model' attribute to be a FasterRCNN model\n","\n","    The 'model' attribute it utilised by all methods in this class\n","\n","    Args: All parameters required by the init() of PyTorch FasterRCNN\n","\n","    Returns: None\n","\n","    Raies: AssertionError is model is going to be initialised as NoneType\n","    \"\"\"\n","\n","    self.model = FasterRCNN(\n","        backbone,\n","        num_classes,\n","        # transform parameters\n","        min_size,\n","        max_size,\n","        image_mean,\n","        image_std,\n","        # RPN parameters\n","        rpn_anchor_generator,\n","        rpn_head,\n","        rpn_pre_nms_top_n_train,\n","        rpn_pre_nms_top_n_test,\n","        rpn_post_nms_top_n_train,\n","        rpn_post_nms_top_n_test,\n","        rpn_nms_thresh,\n","        rpn_fg_iou_thresh,\n","        rpn_bg_iou_thresh,\n","        rpn_batch_size_per_image,\n","        rpn_positive_fraction,\n","        rpn_score_thresh,\n","        # Box parameters\n","        box_roi_pool,\n","        box_head,\n","        box_predictor,\n","        box_score_thresh,\n","        box_nms_thresh,\n","        box_detections_per_img,\n","        box_fg_iou_thresh,\n","        box_bg_iou_thresh,\n","        box_batch_size_per_image,\n","        box_positive_fraction,\n","        bbox_reg_weights,\n","        **kwargs,)\n","\n","    assert self.model is not None, \"Model cannot be initialised as NoneType\"\n","\n","  def override_prediction_heads(self, num_classes: int) -> None:\n","    # update prediction head so that it's outputs are aligned with number of classes in dataset\n","    in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n","    self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","  def load_pretrained_weights(self, weights):\n","    # the .verify() method is inherited from the WeightsEnum class\n","    # verify checks that are the weights are not None and are the correct type\n","    weights = FasterRCNN_ResNet50_FPN_Weights.verify(weights)\n","    # load the weights into the model\n","    self.model.load_state_dict(weights.get_state_dict(progress=True, check_hash=True))\n","    # the following function call is required to address\n","    # the BC-breaking change introduced by the bug-fix at pytorch/vision#2933\n","    # overwrite_eps is only required when loading pre-trained weights\n","    overwrite_eps(self.model, 0.0)\n","\n","  def train(self, num_epochs: int,\n","                  train_dataset: torch.utils.data.DataLoader,\n","                  val_dataset: torch.utils.data.DataLoader,\n","                  optimizer: torch.optim) -> Dict[str, list]:\n","    \"\"\"Trains the model for the specified number of epochs\n","\n","      Calls function train_batch to train the model on each batch in an epoch.\n","      Validation loss is calculated at the end of each epoch\n","      Training time is recorded for the purposes of evaluation.\n","      Epoch loss is calculates as the average training loss across batches\n","\n","      Args:\n","          num_epochs: the number of epochs to train for,\n","\n","          train_dataset: the PyTorch DataLoader for the training set,\n","\n","          val_dataset: the PyTorch DataLoader for the validation set,\n","\n","          optimizer : any optimizer imported from torch.optim\n","\n","      Returns:\n","          A dictionary containing 2 lists - one for training loss and another validation loss\n","\n","      Raises:\n","          None\n","      \"\"\"\n","\n","    # track losses across each epoch ready to produce graph\n","    training_losses = []\n","    val_losses = []\n","\n","    for epoch in range(num_epochs):\n","        # simple progress monitor\n","        print(f'Epoch {epoch} of {num_epochs}')\n","        # Keep running total of batch losses ready to take average across epoch\n","        epoch_loss_train = 0\n","        for batch in train_dataset:\n","          batch_training_loss = self.__train_batch(batch, optimizer)\n","          epoch_loss_train += batch_training_loss\n","\n","        # divide total epoch loss by the total number of batches per epoch\n","        epoch_training_loss = epoch_loss_train/len(train_dataset)\n","        training_losses.append(epoch_training_loss)\n","        print(f'Epoch Training loss: {epoch_training_loss}')\n","\n","        val_loss = self.__calc_val_loss(val_dataset_loader)\n","        val_losses.append(val_loss)\n","        print(f'Epoch Validation loss: {val_loss}')\n","\n","    plot_loss([i+1 for i in range(num_epochs)], training_losses, val_losses)\n","\n","    return {'training_losses': training_losses, 'validation_losses': val_losses}\n","\n","  #memory management\n","  # images = [ t.cpu() for t in images]\n","  # targets = [ {'boxes':d['boxes'].cpu(), 'labels':d['labels'].cpu()} for d in targets]\n","  # del images\n","  # del targets\n","\n","  def __train_batch(self, train_batch: torch.utils.data.dataset.Subset,\n","                  optimizer: torch.optim) -> float:\n","    \"\"\"Trains the model on a single batch and returns the batch loss\n","\n","      Calls function format_batch() before training the model on the batch data.\n","      Total loss is calculated across each of the 4 losses returned by Faster R-CNN\n","      A backward pass is then completed\n","\n","      Args:\n","          train_batch: a subset of the PyTorch dataset\n","\n","          optimizer : any optimizer imported from torch.optim\n","\n","      Returns:\n","          batch loss\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    # enforce correct model mode as model will be set to .eval mode to calculate\n","    # validation loss during training\n","    self.model.train()\n","    # Calculate training loss\n","    data_dict = format_batch(train_batch, self.model)\n","    images, targets = data_dict['images'], data_dict['targets']\n","    loss_dict = self.model(images, targets)\n","    #IS THIS APPROACH COMPARABLE TO LOSS CALCULATION IN YOLO?\n","    #IN ORDER TO MAKE COMPARISONS BETWEEN ARCHITECTURES\n","    training_loss = sum(v for v in loss_dict.values())\n","\n","\n","    # backward pass\n","    optimizer.zero_grad()\n","    training_loss.backward()\n","\n","    optimizer.step()\n","\n","    #  Convert to a Python Number ready for plotting\n","    return training_loss.item()\n","\n","    #TO DO: CALCULATE mAP FOR EACH EPOCH ?\n","    # however the PyTorch implementation of FasterRCNN only returns losses when in .train() mode\n","    # is this avg calc correct ?\n","    # https://datascience.stackexchange.com/questions/92673/should-i-report-loss-on-the-last-batch-or-the-entire-training-dataset\n","\n","  ##THE CODE IN FUNCTION EVAL_FORWARD HAS BEEN MODIFIED FROM SOURCE:\n","  #https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n","  #the majority of this function is composed of PyTorch source code that has been slightly modififed so that both losses and predictions are returned during inference\n","  #PyTorch source files (which part of code was sourced from) can be seen here\n","  #https://github.com/pytorch/vision/blob/f40c8df02c197d1a9e194210e40dee0e6a6cb1c3/torchvision/models/detection/generalized_rcnn.py#L46\n","\n","  #analyse using this reference\n","  #https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn\n","\n","  def __eval_forward(self, images, targets):\n","      # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","      \"\"\"\n","      Args:\n","          images (list[Tensor]): images to be processed\n","          targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","      Returns:\n","          result (list[BoxList] or dict[Tensor]): the output from the model.\n","              It returns list[BoxList] contains additional fields\n","              like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","      \"\"\"\n","      #Added (generalized_rcnn.py forward())\n","      self.model.eval()\n","\n","      #*******************************START:Sourced from generalized_rcnn.py forward()\n","      # lines 72-98 of generalized_rcnn.py\n","      original_image_sizes: List[Tuple[int, int]] = []\n","      for img in images:\n","          val = img.shape[-2:]\n","          assert len(val) == 2\n","          original_image_sizes.append((val[0], val[1]))\n","\n","      images, targets = self.model.transform(images, targets)\n","\n","      # MODIFICATION: Refactored a block of code into the function check_bboxes\n","      # Check for degenerate boxes\n","      check_bboxes(targets)\n","\n","      features = self.model.backbone(images.tensors)\n","      if isinstance(features, torch.Tensor):\n","          features = OrderedDict([(\"0\", features)])\n","      #*******************************END:Sourced from generalized_rcnn.py forward()\n","\n","      # MODIFICATION: COMMENTED OUT THESE LINES OF CODE\n","      # AS RPN AND ROI DO NOT NEED TO BE TRAINING DURING INFERENCE ?\n","      # model.rpn.training=True\n","      # model.roi_heads.training=True\n","      # END OF MODIFICATION\n","\n","\n","      # ************Following section of code implements similar functionality to:\n","      # proposals, proposal_losses = model.rpn(images, features, targets)\n","      # from generalized_rcnn.py forward():\n","      # Code relating to the RPN (Region Proposal Network)\n","      #*******************************START:Sourced from rpn.py\n","      #lines 325-340 in rpn.py\n","      features_rpn = list(features.values())\n","      objectness, pred_bbox_deltas = self.model.rpn.head(features_rpn)\n","      anchors = self.model.rpn.anchor_generator(images, features_rpn)\n","\n","      num_images = len(anchors)\n","      num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n","      num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n","      objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n","      # Following 3 lines are comments from PyTorch source code\n","      # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n","      # note that we detach the deltas because Faster R-CNN do not backprop through\n","      # the proposals\n","      proposals = self.model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n","      proposals = proposals.view(num_images, -1, 4)\n","      proposals, scores = self.model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n","      proposal_losses = {}\n","\n","      #lines 342-351 in rpn.py\n","      assert targets is not None\n","      labels, matched_gt_boxes = self.model.rpn.assign_targets_to_anchors(anchors, targets)\n","      regression_targets = self.model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n","      loss_objectness, loss_rpn_box_reg = self.model.rpn.compute_loss(\n","          objectness, pred_bbox_deltas, labels, regression_targets\n","      )\n","      proposal_losses = {\n","          \"loss_objectness\": loss_objectness,\n","          \"loss_rpn_box_reg\": loss_rpn_box_reg,\n","      }\n","      #*******************************END:Sourced from rpn.py\n","\n","      #image size required by ROI head\n","      image_shapes = images.image_sizes\n","\n","      # ************Following section of code implements similar functionality to:\n","      #detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n","      # from generalized_rcnn.py forward():\n","      # Code relating to the ROI heads (Region of Interest)\n","      #*******************************START:Sourced from roi_heads.py\n","      # lines 745-771 of roi_heads.py\n","      proposals, matched_idxs, labels, regression_targets = self.model.roi_heads.select_training_samples(proposals, targets)\n","\n","      box_features = self.model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n","      box_features = self.model.roi_heads.box_head(box_features)\n","      class_logits, box_regression = self.model.roi_heads.box_predictor(box_features)\n","\n","      result: List[Dict[str, torch.Tensor]] = []\n","      detector_losses = {}\n","\n","      # The following 2 lines of code were original inside a 'if self.training' condition\n","      # they have been taken out of this if statment so that\n","      # losses can be returned when in inference mode (to get val loss)\n","      loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n","      detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n","\n","      # the code below was the corresponding 'else' statement (ie if not self.training)\n","      # it was taken out if the else statement\n","      # so that both predictions and losses could be returned when in inference mode\n","      boxes, scores, labels = self.model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n","      num_images = len(boxes)\n","      for i in range(num_images):\n","          result.append(\n","              {\n","                  \"boxes\": boxes[i],\n","                  \"labels\": labels[i],\n","                  \"scores\": scores[i],\n","              }\n","          )\n","      #*******************************END:Sourced from roi_heads.py\n","\n","      # rename results as detection so that it is compatible with the subsequent\n","      # line of code sourced from generalized_rcnn.py\n","      detections = result\n","      # Following line of code sourced from generalized_rcnn.py (line 100)\n","      detections = self.model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","      # MODIFICATION: REMOVED THE FOLLOWING 2 LINES OF CODE\n","      # model.rpn.training=False\n","      # model.roi_heads.training=False\n","\n","      #*******************************START:Sourced from generalized_rcnn.py forward()\n","      # lines 102-104 in generalized_rcnn.py\n","      losses = {}\n","      losses.update(detector_losses)\n","      losses.update(proposal_losses)\n","      #*******************************END:Sourced from generalized_rcnn.py forward()\n","\n","      #Added (generalized_rcnn.py forward())\n","      return losses, detections\n","\n","  #Function 'evaluate_loss' is modified from source:\n","  #https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n","  # this function has been modified to become a private class method\n","  # meaining the arg list and some of the internal code has changed\n","  #A docstring has been added for the modified function\n","  def __calc_val_loss(self, data_loader: torch.utils.data.DataLoader) -> float:\n","      \"\"\"Calculates average validation loss across batches in a PyTorch DataLoader\n","\n","      Calls function eval_forward() to caluclate validation loss for each batch.\n","      Each batch validation loss is added to a running total which is divided by the\n","      number of batches to get average validation loss across batches.\n","      Calls format_batch to prep input data and move it to the same device as the model (usually GPU)\n","\n","      Args:\n","          data_loader: a Pytorch DataLoader\n","\n","      Returns:\n","          The average validation loss across batches\n","\n","      Raises:\n","          None\n","      \"\"\"\n","      val_loss = 0\n","      with torch.no_grad():\n","        for batch in data_loader:\n","          #*************************************START OF MODIFICATIONS 1\n","          formatted_batch = format_batch(batch, self.model)\n","          images, targets = formatted_batch['images'], formatted_batch['targets']\n","          losses_dict, detections = self.__eval_forward(images, targets)\n","          #*************************************END OF MODIFICATIONS 1\n","          losses = sum(loss for loss in losses_dict.values())\n","          val_loss += losses\n","\n","      validation_loss = val_loss/ len(data_loader)\n","      #*************************************START OF MODIFICATIONS 2\n","      # use .item() to convert loss to a Python Number ready for plotting\n","      return validation_loss.item()\n","\n","  def test(self, test_dataset_loader: torch.utils.data.DataLoader) -> dict:\n","    \"\"\"Calculates model mAP at different IoU thresholds on the given test set\n","\n","      Sets the model to eval mode and runs the model to get inferences.\n","      Non-maximum suppression is applied before mAPs are calculated.\n","      mAP50 and mAP75 are printed for convenience\n","      Model predictions are returned so that they can be visualised by\n","      a follow up call to visualise_predictions()\n","\n","      Args:\n","          test_dataset_loader: the Pytorch DataLoader for the test set\n","\n","      Returns:\n","          Dictionary with 2 keys ; model predictions and model evaluation metrics (mAP)\n","\n","      Raises:\n","          None\n","      \"\"\"\n","\n","    # according to the PyTorch docs:\n","    # \"be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode.\n","    # Failing to do this will yield inconsistent inference results.\"\n","    # https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#:~:text=Saving%20and%20Loading%20Model%20Weights&text=Using%20weights_only%3DTrue%20is%20considered%20a%20best%20practice%20when%20loading%20weights.&text=be%20sure%20to%20call%20model,will%20yield%20inconsistent%20inference%20results.\n","    self.model.eval()\n","\n","    start_time = time.time()\n","    # memory management\n","    torch.cuda.empty_cache()\n","\n","    test_images = []\n","    test_targets = []\n","\n","    for batch in test_dataset_loader:\n","        # gather all images into a single array\n","        formatted_data = format_batch(batch, self.model)\n","        batch_images, batch_targets = formatted_data['images'], formatted_data['targets']\n","        # add prepped images to cumulative list\n","        test_images = [*test_images, *batch_images]\n","        test_targets = [*test_targets, *batch_targets]\n","\n","    #reduce memory usage by not storing intermediate tensors needed to calculate gradients\n","    #torch.no_grad reportedly speeds up computations\n","    with torch.no_grad():\n","      predicted = self.model(test_images)\n","\n","      # calculate average inference time for an image\n","      end_time = time.time()\n","      duration = execution_time = end_time - start_time\n","      test_size = len(test_dataset_loader)*test_dataset_loader.batch_size\n","      avg_time = duration / test_size\n","\n","      metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5, 0.75], class_metrics=True)\n","\n","      # Update metric with predictions and respective ground truth\n","      metric.update(predicted, test_targets)\n","\n","      # Compute the results\n","      result = metric.compute()\n","\n","      print('mAP50:')\n","      print(result['map_50'])\n","\n","      print('mAP75:')\n","      print(result['map_75'])\n","\n","      # fig_, ax_ = metric.plot()\n","      return {\"predictions\": predicted, \"metrics\": result}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rQOGEdDdADVe","executionInfo":{"status":"ok","timestamp":1738178565183,"user_tz":0,"elapsed":9312,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"79982143-6778-438a-f019-709556f39b58"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Requirement already satisfied: faster-coco-eval in /usr/local/lib/python3.11/dist-packages (1.6.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from faster-coco-eval) (1.26.4)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from faster-coco-eval) (5.24.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from faster-coco-eval) (2.2.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from faster-coco-eval) (11.1.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->faster-coco-eval) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->faster-coco-eval) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->faster-coco-eval) (2025.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->faster-coco-eval) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly->faster-coco-eval) (24.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->faster-coco-eval) (1.17.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.55.6)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n"]}]},{"cell_type":"markdown","source":["## Definition of Model Pipeline: <a class=\"anchor\" name=\"model_pipeline\"></a>"],"metadata":{"id":"ksp4GJNuAXY1"}},{"cell_type":"code","source":["!pip install wandb -qU\n","!pip install onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZBUsIr_1QSoq","executionInfo":{"status":"ok","timestamp":1738178568765,"user_tz":0,"elapsed":3586,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"df7d9b61-500f-4a5c-e02f-5fd77e10251a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (1.26.4)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.25.6)\n"]}]},{"cell_type":"code","source":["import wandb\n","wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2k2bLF8_BCM","outputId":"4ee598d2-f17d-4f4f-89be-ab9722a78e08","executionInfo":{"status":"ok","timestamp":1738178571843,"user_tz":0,"elapsed":3081,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33motterhian\u001b[0m (\u001b[33motterhian-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n","import time\n","import onnx\n","\n","def model_pipeline(modelWrapper: FasterRCNNWrapper,\n","                   optimizer: torch.optim,\n","                   num_epochs: int,\n","\n","                   train_dataset_loader,\n","                   val_dataset_loader,\n","                   test_dataset_loader,\n","\n","                   device: torch.device,\n","                   isLogging: bool,\n","                   projectName: str) -> None:\n","\n","  # Set up TensorBoard and Wandb logging\n","  if isLogging:\n","    log_dir = \"/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/tensorboard_runs/FasterRCNN/\"\n","\n","    # Prep identifiers for the run\n","    train_size = len(train_dataset_loader) * train_dataset_loader.batch_size\n","    val_size = len(val_dataset_loader) * val_dataset_loader.batch_size\n","    test_size = len(test_dataset_loader) * test_dataset_loader.batch_size\n","    time_stamp = datetime.datetime.now().strftime(\"%m_%d-%H_%M\")\n","\n","    writer = SummaryWriter(f\"{log_dir}/{train_size}_{val_size}_{test_size}____{time_stamp}\")\n","\n","    # gather hyperparameters used in this run\n","    optimizer_hyperparams = optimizer.state_dict()['param_groups'][0] #dict is first item in array\n","\n","    model = modelWrapper.model\n","    model_anchor_generator = model.rpn.anchor_generator\n","    train_set_size = train_dataset_loader.batch_size*len(train_dataset_loader)\n","    val_set_size = val_dataset_loader.batch_size*len(val_dataset_loader)\n","    test_set_size = test_dataset_loader.batch_size*len(test_dataset_loader)\n","\n","    if optimizer_hyperparams.get('momentum') == None: #ADAM does not have property momentum\n","      learning_rate, weight_decay = optimizer_hyperparams['lr'], optimizer_hyperparams['weight_decay']\n","      hyperparameters = {\n","          \"num_epochs\": num_epochs,\n","          \"train_val_test_split\": f\"{train_set_size}/{val_set_size}/{test_set_size}\",\n","          \"optimizer_lr\": learning_rate,\n","          \"optimizer_weight_decay\": weight_decay,\n","          # TensorBoard only accepts datatypes int, float, str, bool, or torch.Tensor\n","          # for the method add_hparams()\n","          # convert the following tuples and lists to strings\n","          \"anchor_aspect_ratios\": str(model_anchor_generator.aspect_ratios),\n","          \"num_anchors_per_loc\": str(model_anchor_generator.num_anchors_per_location()),\n","          \"anchor_sizes\": str(model_anchor_generator.sizes)\n","\n","      }\n","    else: #SGD does have property momentum\n","      learning_rate, momentum, weight_decay = optimizer_hyperparams['lr'], optimizer_hyperparams['momentum'], optimizer_hyperparams['weight_decay']\n","      hyperparameters = {\n","          \"num_epochs\": num_epochs,\n","          \"train_val_test_split\": f\"{train_set_size}/{val_set_size}/{test_set_size}\",\n","          \"optimizer_lr\": learning_rate,\n","          \"optimizer_momentum\": momentum,\n","          \"optimizer_weight_decay\": weight_decay,\n","          # TensorBoard only accepts datatypes int, float, str, bool, or torch.Tensor\n","          # for the method add_hparams()\n","          # convert the following tuples and lists to strings\n","          \"anchor_aspect_ratios\": str(model_anchor_generator.aspect_ratios),\n","          \"num_anchors_per_loc\": str(model_anchor_generator.num_anchors_per_location()),\n","          \"anchor_sizes\": str(model_anchor_generator.sizes)\n","\n","      }\n","\n","    # initialise wandb before training so that it monitors CPU usage\n","    # use 'with wandb.init' context manager so that wandb.finish() is automatically\n","    # called at the end of the block\n","    with wandb.init(project=projectName, config=hyperparameters):\n","      # track gradients and parameters\n","      # wandb.watch(modelWrapper.model, log=\"all\") #NOT WORKING WITH CUSTOM BACKBONE\n","\n","      # train the model and measure training time to log\n","      start_time = time.time()\n","      losses_dict = modelWrapper.train(num_epochs, train_dataset_loader, val_dataset_loader, optimizer)\n","      end_time = time.time()\n","      execution_time = end_time - start_time\n","      # log training time\n","      wandb.log({\"Training time\": execution_time})\n","      writer.add_scalar(\"Training time\", execution_time)\n","\n","      training_losses, val_losses = losses_dict['training_losses'], losses_dict['validation_losses']\n","      # log training data\n","      for i in range(len(training_losses)):\n","            # tensorboard log\n","            writer.add_scalar('Training Loss', training_losses[i], i)\n","            writer.add_scalar('Validation Loss', val_losses[i], i)\n","            writer.add_scalars( 'Training vs. Validation Loss',\n","                                {'Training' : training_losses[i], 'Validation' : val_losses[i] },\n","                                i)\n","      # Wandb log\n","      # log train vs val loss separately so they they can be visually compared with different runs\n","      wandb.log({\"train_loss\" : wandb.plot.line_series(\n","                  xs=list(range(num_epochs)),\n","                  ys=[training_losses],\n","                  title=\"Training loss\",\n","                  xname=\"Epochs\")})\n","\n","      wandb.log({\"val_loss\" : wandb.plot.line_series(\n","                  xs=list(range(num_epochs)),\n","                  ys=[val_losses],\n","                  title=\"Validation loss\",\n","                  xname=\"Epochs\")})\n","\n","      # plot together on same graph for direct comparison of train and val loss for a run\n","      wandb.log({\"train_vs_val_loss\" : wandb.plot.line_series(\n","                  xs=list(range(num_epochs)),\n","                  ys=[training_losses, val_losses],\n","                  keys=[\"Training\", \"Validation\"],\n","                  title=\"Train vs val loss\",\n","                  xname=\"Epochs\")})\n","\n","      # test the model and measure execution time so avg inference time can be calculated\n","      start_time = time.time()\n","      test_results = modelWrapper.test(test_dataset_loader)\n","      end_time = time.time()\n","      execution_time = end_time - start_time\n","      # log testing data\n","\n","      # Save the model in the exchangeable ONNX format\n","      test_images = [item[0].to(device=next(model.parameters()).device) for batch in test_dataset_loader for item in batch]\n","      torch.onnx.export(modelWrapper.model, test_images, \"model.onnx\")\n","      wandb.save(\"model.onnx\")\n","      # writer.add_graph(modelWrapper.model, test_images)\n","\n","      # calculate average inference time and log\n","      wandb.log({\"Inference time\": execution_time/test_set_size})\n","      writer.add_scalar(\"Inference\", execution_time/test_set_size)\n","\n","      # TensorBoard Log\n","      writer.add_hparams(hyperparameters, {\"mAP50\": test_results['metrics']['map_50'], \"mAP75\": test_results['metrics']['map_75']})\n","      # Wandb log\n","      wandb.log({\"mAP50\": test_results['metrics']['map_50'], \"mAP75\": test_results['metrics']['map_75']})\n","      # log code files in case extra info is needed for analysis\n","      wandb.run.log_code(\"/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test\", include_fn=lambda path: path.endswith(\".ipynb\"))\n","\n","        # Write image data to TensorBoard log dir\n","        # writer.add_image( 'Four Fashion-MNIST Images', img_grid)\n","\n","      # free up memory as writer no longer needed\n","      writer.close()\n","\n","  else:\n","    modelWrapper.train(num_epochs, train_dataset_loader, val_dataset_loader, optimizer)\n","    test_results = modelWrapper.test(test_dataset_loader)\n","\n","\n","  # visualise samples of predictions\n","  # model_predictions = test_results[\"predictions\"]\n","  # for i in range(len(model_predictions)):\n","  #   visualisePredictions(i, test_dataset, model_predictions)"],"metadata":{"id":"g3Nxq1HOAX2O","executionInfo":{"status":"ok","timestamp":1738179623783,"user_tz":0,"elapsed":996,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2LvEmShNuFJR"},"source":["## Faster R-CNN Implementation: <a class=\"anchor\" name=\"fasterRCNN\"></a>"]},{"cell_type":"markdown","metadata":{"id":"jqwB9h8IuqTs"},"source":["### Base line Implementation: <a class=\"anchor\" name=\"fasterRCNN_baseline\"></a>"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"OySnCC0JQueK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738178576608,"user_tz":0,"elapsed":1777,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"d9ac0699-47c6-4d88-a20d-c4107fc05d75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# set deterministic behaviour\n","set_seeds()"],"metadata":{"id":"bfTNhPgXP-xY","executionInfo":{"status":"ok","timestamp":1738178576608,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"fRUC7DfhUSdO","executionInfo":{"status":"ok","timestamp":1738178576608,"user_tz":0,"elapsed":2,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["# smaller dataset\n","root_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/train'\n","image_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/train/images'\n","label_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/train/labels'\n","\n","# larger dataset\n","# root_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/681 annotation milestone/raw_data'\n","# image_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/681 annotation milestone/raw_data/images'\n","# label_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/681 annotation milestone/raw_data/labels'"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"e74c039c-b215-4024-8f5b-88a6d12e61c3","executionInfo":{"status":"ok","timestamp":1738178576608,"user_tz":0,"elapsed":2,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["# dataset = UAVImageDataset(data_dir_imgs=image_folder_path, data_dir_targets=label_folder_path)\n","\n","# #set shuffle to True so that the training of the model is not dependent on the order of the data\n","# #only have shuffle=False for validation/testing and inference\n","# train_dataset_loader = torch.utils.data.DataLoader(train_dataset, collate_fn=lambda x: x, batch_size=16, shuffle=True, pin_memory=True,\n","#                                                    num_workers=6, prefetch_factor=4)\n","\n","# val_dataset_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=lambda x: x, batch_size=16)\n","\n","# test_dataset_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=lambda x: x, batch_size=16)\n","\n","# from torch.utils.data import random_split\n","\n","# #TO DO: MULTIPLE VALIDATION SETS ?\n","# train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"GVze7O40DbBJ","executionInfo":{"status":"ok","timestamp":1738178576608,"user_tz":0,"elapsed":2,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["#VISUALISE DATA AS SANITY CHECK"]},{"cell_type":"code","source":["train_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/train/images\",\n","                                data_dir_targets=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/train/labels\")\n","\n","val_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/valid/images\",\n","                                data_dir_targets=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/valid/labels\")\n","\n","test_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/test/images\",\n","                                data_dir_targets=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/test/labels\")"],"metadata":{"id":"nvmssv3hxzXj","executionInfo":{"status":"ok","timestamp":1738178578522,"user_tz":0,"elapsed":1916,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["batch_size = 16\n","\n","train_dataset_loader = torch.utils.data.DataLoader(train_dataset, collate_fn=lambda x: x, batch_size=batch_size, shuffle=True, pin_memory=True,\n","                                                   num_workers=12, prefetch_factor=10)\n","\n","val_dataset_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=lambda x: x, batch_size=batch_size,\n","                                                 num_workers=6)\n","\n","test_dataset_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=lambda x: x, batch_size=batch_size,\n","                                                  num_workers=6)\n","\n","# train_dataset_loader = torch.utils.data.DataLoader(train_dataset, collate_fn=lambda x: x, batch_size=8, shuffle=True, pin_memory=True\n","#                                                    )\n","\n","# val_dataset_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=lambda x: x, batch_size=8\n","#                                                  )\n","\n","# test_dataset_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=lambda x: x, batch_size=8\n","#                                                   )"],"metadata":{"id":"nrRB0qqh-HxI","executionInfo":{"status":"ok","timestamp":1738178843837,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# torch.save(train_dataset, '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/train.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"C2LFbI5mp62B","executionInfo":{"status":"error","timestamp":1738181056211,"user_tz":0,"elapsed":574,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"79061994-46e7-4b34-8588-bc7bea685911"},"execution_count":60,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"cannot pickle 'module' object","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-60-337dffa24ce6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/train.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: cannot pickle 'module' object"]}]},{"cell_type":"code","source":["# train_dataset = torch.load('./train.pt')\n","# train_dataset_loader = torch.utils.data.DataLoader(train_dataset, collate_fn=lambda x: x, batch_size=batch_size, shuffle=True, pin_memory=True,\n","#                                                    num_workers=12, prefetch_factor=10)"],"metadata":{"id":"JukuYSr8qMis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"lCqiqeemOvG-","executionInfo":{"status":"error","timestamp":1738178578522,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"cf5de35e-bfb5-4b93-d72f-0f145d978194"},"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'stop' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-4f76a9dad686>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"]}]},{"cell_type":"code","source":["from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models._utils import _ovewrite_value_param\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights, FastRCNNPredictor\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers\n","from torchvision.models.detection._utils import overwrite_eps\n","from torchvision.ops import misc as misc_nn_ops\n","import torch.nn as nn\n","\n","num_trainable_backbone_layers = 3 # valid range for resnet50 is 0-5 according to PyTorch docs\n","backbone = resnet50(weights=None, progress=True, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","backbone = _resnet_fpn_extractor(backbone, num_trainable_backbone_layers)\n","\n","#use default values for a baseline implementation\n","anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n","\n","# anchor_sizes = ((16,), (32,), (64,), (128,), (512,))\n","\n","# default values\n","anchor_aspect_ratios = ((0.5, 1.0, 2.0),) * len (anchor_sizes)\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=anchor_sizes,\n","    aspect_ratios=anchor_aspect_ratios\n",")\n","#use default values for a baseline implementation\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","custom_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=91,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","# use default weights trained on COCO image dataset\n","weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","custom_model_wrapper.load_pretrained_weights(weights)\n","#override the prediction heads to have the correct number of classes\n","num_classes=4 #3 classes +1 to account for background\n","custom_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"id":"UcNClpum_Rij","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Must move model (and it's parameters) to the GPU before initialisating the optimizer\n","# #according to a video on the official PyTorch Youtube Channel\n","# #https://www.youtube.com/watch?v=jF43_wj_DCQ&t=213s\n","# #otherwise the optimiser will update the wrong set of model parameters (on the CPU)\n","# custom_model_wrapper.model.to(device)\n","# # try this for optimizer\n","# # filter(lambda p: p.requires_grad, model.parameters())\n","# # https://medium.com/we-talk-data/guide-to-freezing-layers-in-pytorch-best-practices-and-practical-examples-8e644e7a9598#:~:text=To%20avoid%20this%2C%20verify%20the%20requires_grad%20status%20of%20each%20layer.&text=By%20running%20this%20code%2C%20you,ve%20frozen%20the%20correct%20ones.&text=If%20you%20don't%20filter,parameters%20that%20won't%20change.\n","optimizer = torch.optim.SGD(custom_model_wrapper.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","num_epochs = 15\n","model_pipeline(custom_model_wrapper, optimizer, num_epochs, train_dataset_loader,  val_dataset_loader, test_dataset_loader, device, False, \"Faster R-CNN Tracker\")"],"metadata":{"id":"vvGMgIbP_UbT","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(custom_model_wrapper.model)\n","stop"],"metadata":{"id":"HYxxkChrR-6e","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install tensorboard\n","# # Load the TensorBoard notebook extension\n","# %load_ext tensorboard\n","\n","# %tensorboard --logdir \"/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/tensorboard_runs\"\n","# # start local host for tensorboard using\n","# # %tensorboard --logdir logs"],"metadata":{"id":"j7pACf_X_SES","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhJM1h1Mx40w"},"source":["### Custom Backbones: <a class=\"anchor\" name=\"custom_backbones\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MCp3yqUzSHC","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["#**********************************NEW\n","\n","# frame of reference\n","# import torchvision\n","# from torchvision.models.detection import FasterRCNN\n","# from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# # load a pre-trained model for classification and return\n","# # only the features\n","# backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# # ``FasterRCNN`` needs to know the number of\n","# # output channels in a backbone. For mobilenet_v2, it's 1280\n","# # so we need to add it here\n","# backbone.out_channels = 1280\n","\n","# # let's make the RPN generate 5 x 3 anchors per spatial\n","# # location, with 5 different sizes and 3 different aspect\n","# # ratios. We have a Tuple[Tuple[int]] because each feature\n","# # map could potentially have different sizes and\n","# # aspect ratios\n","# anchor_generator = AnchorGenerator(\n","#     sizes=((32, 64, 128, 256, 512),),\n","#     aspect_ratios=((0.5, 1.0, 2.0),)\n","# )\n","\n","# # let's define what are the feature maps that we will\n","# # use to perform the region of interest cropping, as well as\n","# # the size of the crop after rescaling.\n","# # if your backbone returns a Tensor, featmap_names is expected to\n","# # be [0]. More generally, the backbone should return an\n","# # ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n","# # feature maps to use.\n","# roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","#     featmap_names=['0'],\n","#     output_size=7,\n","#     sampling_ratio=2\n","# )\n","\n","# # put the pieces together inside a Faster-RCNN model\n","# model = FasterRCNN(\n","#     backbone,\n","#     num_classes=2,\n","#     rpn_anchor_generator=anchor_generator,\n","#     box_roi_pool=roi_pooler\n","# )"]},{"cell_type":"code","source":["from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models._utils import _ovewrite_value_param\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights, FastRCNNPredictor\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers\n","from torchvision.models.detection._utils import overwrite_eps\n","from torchvision.ops import misc as misc_nn_ops\n","import torch.nn as nn\n","\n","\n","#work around: AttributeError: 'ResNet' object has no attribute 'features'\n","# backbone = torchvision.models.resnet50(weights=\"DEFAULT\").features\n","# #https://discuss.pytorch.org/t/change-resnet50s-number-of-output-filters/146644\n","# backbone.out_channels = 2048\n","\n","# backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# # ``FasterRCNN`` needs to know the number of\n","# # output channels in a backbone. For mobilenet_v2, it's 1280\n","# # so we need to add it here\n","# backbone.out_channels = 1280\n","\n","# following code results in the following error\n","# AssertionError: Anchors should be Tuple[Tuple[int]] because each feature map could potentially have different sizes and aspect ratios.\n","# There needs to be a match between the number of feature maps passed and the number of sizes / aspect ratios specified.\n","# currently thinking this is due to the fact the code below would result in a model\n","# without a feature pyramid network - so would not be capable of identifying objects of differnet scales within same image:\n","# resnet_net = resnet50(pretrained=True)\n","# modules = list(resnet_net.children())[:-2]\n","# backbone = nn.Sequential(*modules)\n","# backbone.out_channels = 2048\n","\n","\n","num_trainable_backbone_layers = 3 # valid range for resnet50 is 0-5 according to PyTorch docs\n","backbone = resnet50(weights=None, progress=True, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","backbone = _resnet_fpn_extractor(backbone, num_trainable_backbone_layers)"],"metadata":{"id":"4D9WPFpUDU5F","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#use default values for a baseline implementation\n","anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n","\n","# anchor_sizes = ((16,), (32,), (64,), (128,), (512,))\n","\n","# default values\n","anchor_aspect_ratios = ((0.5, 1.0, 2.0),) * len (anchor_sizes)\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=anchor_sizes,\n","    aspect_ratios=anchor_aspect_ratios\n",")\n","#use default values for a baseline implementation\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","custom_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=91,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","# use default weights trained on COCO image dataset\n","weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","custom_model_wrapper.load_pretrained_weights(weights)\n","#override the prediction heads to have the correct number of classes\n","num_classes=4 #3 classes +1 to account for background\n","custom_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"id":"w71VuhczDrZi","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test from docs\n","# https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html#defining-your-model\n","\n","# load a pre-trained model for classification and return\n","# only the features\n","backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# ``FasterRCNN`` needs to know the number of\n","# output channels in a backbone. For mobilenet_v2, it's 1280\n","# so we need to add it here\n","backbone.out_channels = 1280\n","\n","# let's make the RPN generate 5 x 3 anchors per spatial\n","# location, with 5 different sizes and 3 different aspect\n","# ratios. We have a Tuple[Tuple[int]] because each feature\n","# map could potentially have different sizes and\n","# aspect ratios\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256, 512),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","\n","# let's define what are the feature maps that we will\n","# use to perform the region of interest cropping, as well as\n","# the size of the crop after rescaling.\n","# if your backbone returns a Tensor, featmap_names is expected to\n","# be [0]. More generally, the backbone should return an\n","# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n","# feature maps to use.\n","# roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","#     featmap_names=['0'],\n","#     output_size=7,\n","#     sampling_ratio=2\n","# )\n","\n","# snippet from my code\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","# put the pieces together inside a Faster-RCNN model\n","custom_backbone_model_wrapper = FasterRCNNWrapper(\n","    backbone,\n","    num_classes=4,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","# custom_model_wrapper = FasterRCNNWrapper(\n","#     backbone=backbone,\n","#     num_classes=91,\n","#     rpn_anchor_generator=anchor_generator,\n","#     box_roi_pool=roi_pooler\n","# )\n","\n","\n","optimizer = torch.optim.SGD(custom_backbone_model_wrapper.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","num_epochs = 15\n","model_pipeline(custom_backbone_model_wrapper , optimizer, num_epochs, train_dataset_loader,  val_dataset_loader, test_dataset_loader, device, False)"],"metadata":{"id":"p2PanQKuVJ32","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":8,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["custom_backbone_model_wrapper.model.backbone."],"metadata":{"id":"BL_ydA6TXLyw","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":8,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #- 0.1568 mAP50\n","#model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True) - 0.1080 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) - 0.0434 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True) - 0.0161 mAP50\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","num_trainable_backbone_layers = 3 # valid range for resnet50 is 0-5 according to PyTorch docs\n","backbone = resnet50(weights=None, progress=True, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","backbone = _resnet_fpn_extractor(backbone, num_trainable_backbone_layers)\n","\n","#use default values for a baseline implementation\n","anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n","# default values\n","anchor_aspect_ratios = ((0.5, 1.0, 2.0),) * len (anchor_sizes)\n","anchor_generator = AnchorGenerator(\n","    sizes=anchor_sizes,\n","    aspect_ratios=anchor_aspect_ratios\n",")\n","#use default values for a baseline implementation\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","custom_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=91,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","# use default weights trained on COCO image dataset\n","weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","custom_model_wrapper.load_pretrained_weights(weights)\n","#override the prediction heads to have the correct number of classes\n","num_classes=4 #3 classes +1 to account for background\n","custom_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"id":"x4pS-pmUvxh-","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":8,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# layers will not be frozen if a non pre-trained backbone is supplied\n","# https://github.com/pytorch/vision/issues/2164"],"metadata":{"id":"IEwz1Bj3GDzx","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":8,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Agv3XRLtLTx7","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":8,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scraps\n","\n","\n","# backboneV3= torchvision.models.inception_v3(pretrained=False)\n","# backboneV3_nofc  = nn.Sequential(*list(backboneV3.children())[:-1])\n","# backboneV3_nofc.out_channels = 192"],"metadata":{"id":"rqdJgESeuEk6","executionInfo":{"status":"aborted","timestamp":1738178578523,"user_tz":0,"elapsed":8,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WORKING EDIT:"],"metadata":{"id":"lhnkHUQVBDfy"}},{"cell_type":"code","source":["from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models._utils import _ovewrite_value_param\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights, FastRCNNPredictor\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers\n","from torchvision.models.detection._utils import overwrite_eps\n","from torchvision.ops import misc as misc_nn_ops\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.nn import Conv2d, BatchNorm2d, LeakyReLU, LazyConv2d\n","\n","# CODE IN THE FOLLOWING CELLS IS INFORMED BY\n","# https://debuggercafe.com/traffic-sign-detection-using-pytorch-faster-rcnn-with-custom-backbone/\n","# https://debuggercafe.com/traffic-sign-recognition-using-custom-image-classification-model-in-pytorch/\n","\n","#  output size of conv layer = [(input size + 2(padding) - kernel size)/stride]  +1\n","\n","# the ResNet block proposed in the original paper https://arxiv.org/pdf/1512.03385\n","# 3x3 convolutional layers with stride of 1 and padding of 1\n","# followed by batch normalisation and activation function\n","# then 3x3 convolutional layer\n","# batch normalisation\n","# residual connection (add block inputs to current set of features)\n","# final activation function\n","\n","class IdentityBlock(nn.Module):\n","\n","  # input channels and output channels must be the same for the identity block ?\n","  # always has kernel 3 and padding and stride 1\n","  def __init__(self,\n","              #  num_input_channels : int,\n","               num_filters : int,\n","               kernel : int,\n","               input_padding : int\n","               ) -> None:\n","    \"\"\"\n","    kernel size is consistent across the 2 convolutional layers\n","    (kernel size must be same otherwise input dimensions would not match output dimensions)\n","    stride is 1 for each conv layer - otherwise iput shape will change and raw input dimension\n","    will not be compatible with latest layer output. This means residual connection will fail.\n","\n","    \"\"\"\n","    super().__init__()\n","\n","    #formula for output shape of conv2d:\n","    # o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n","    # i= input dim, p=padding, k=kernel, d=dilation , s=stride\n","    # source: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n","\n","    # in order for input dimensions to remain the same:\n","    # if stride is 1:  o = [i + 2*p - k - (k-1)*(d-1)] + 1\n","    # for input.dim to equal output.dim, then 2*p - k - (k-1)*(d-1) = -1 ; so that o =  i-1+1\n","    # dilation is 1 by default: 2*p - k = -1\n","    # so 2p = k-1  in order for input shape not to change ?\n","    assert 2*input_padding == kernel-1, \"Padding and kernel size not suitable for identity block, these p and k values will change input dimensions\"\n","\n","    self.conv1 = LazyConv2d(\n","                        out_channels = num_filters,\n","                        kernel_size = kernel,\n","                        stride = 1,\n","                        padding = input_padding\n","                        )\n","\n","    self.conv2 = LazyConv2d(\n","                        out_channels = num_filters,\n","                        kernel_size = kernel,\n","                        stride = 1,\n","                        padding = input_padding\n","                        )\n","    # batch normalisation has learnable parameters ('affine parameters') so create\n","    # separate batch normalisation layers for each conv layer\n","    self.bn1 = BatchNorm2d(num_filters)\n","    self.bn2 = BatchNorm2d(num_filters)\n","\n","    # try ReLU or ELU ?\n","    # ReLU not great for intermediate layers in a network as\n","    # it suffers from 'dying ReLUs'\n","    # https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/\n","    # x = F.relu(self.bn1(self.conv1(x)))\n","    self.leaky_relu = LeakyReLU()\n","\n","  def forward(self, x) -> torch.tensor:\n","    # save original input ready to add at residual connection\n","    identity = x\n","\n","    # LAYER 1\n","    x = self.conv1(x)\n","    x = self.bn1(x)\n","    x = self.leaky_relu(x)\n","\n","    # LAYER 2\n","    x = self.conv2(x)\n","    x = self.bn2(x)\n","\n","    x += identity #residual connection\n","\n","    x = self.leaky_relu(x)\n","    return x"],"metadata":{"id":"N83VwbvzNb_4","executionInfo":{"status":"ok","timestamp":1738180442434,"user_tz":0,"elapsed":392,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["class ConvBlock(nn.Module):\n","\n","  def __init__(self,\n","              #  num_input_channels : int,\n","               num_filters : int,\n","               kernel : int,\n","               stride_val : int ,\n","               input_padding : int\n","               ) -> None:\n","\n","    \"\"\"\n","    kernel size is consistent across the 2 convolutional layers\n","    (kernel size must be same otherwise input dimensions would not match output dimensions)\n","\n","    \"\"\"\n","    super().__init__()\n","    self.conv1 = LazyConv2d(\n","                        out_channels = num_filters,\n","                        kernel_size = kernel,\n","                        stride = stride_val,\n","                        padding = input_padding\n","                        )\n","\n","    # GET CUSTOM KERNEL SIZE WORKING HERE\n","    self.conv2 = LazyConv2d(out_channels = num_filters,\n","                            kernel_size = kernel,\n","                            # stride = stride_val,\n","                            padding = input_padding\n","                            )\n","\n","    # PLUS ADD MORE LAYERS ?\n","    # use a 1x1 conv layer for 'cross channel downsampling'\n","    # this reduce the number of channels\n","    # this layer will be used to effectively reshape the inputs ('identity') so that\n","    # it can be added to the output of the layer preceding the residual connection\n","    self.conv_input_transformer = LazyConv2d(\n","                                        out_channels = num_filters, #match output shape of last layer,\n","                                        kernel_size = 1,\n","                                        stride = stride_val,\n","                                        padding = 0\n","                                        )\n","    # batch normalisation has learnable parameters ('affine parameters') so create\n","    # separate batch normalisation layers for each conv layer\n","    self.bn1 = BatchNorm2d(num_filters)\n","    self.bn2 = BatchNorm2d(num_filters)\n","\n","    self.leaky_relu = LeakyReLU()\n","\n","  def forward(self, x) -> torch.Tensor:\n","    # save original input ready to add at residual connection\n","    identity = x\n","\n","    # LAYER 1\n","    x = self.conv1(x)\n","    x = self.bn1(x)\n","    x = self.leaky_relu(x)\n","\n","    # LAYER 2\n","    x = self.conv2(x)\n","    x = self.bn2(x)\n","\n","    x += self.conv_input_transformer(identity) #residual connection\n","    x = self.leaky_relu(x)\n","\n","    return x\n"],"metadata":{"id":"UPRgJcltRP-u","executionInfo":{"status":"ok","timestamp":1738180443114,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["\n","class CustomResNet(nn.Module):\n","    def __init__(self,\n","                 input_channels: int =3, # datasets imgs are in colour so default input channels is 3\n","                 num_classes: int =4) -> None:\n","\n","        super().__init__()\n","        # a conv layer with kernel=3, stride=1, padding=1 is known as 'same convolution' meaning the output size won't change\n","        # first block of resnet is defined as follows\n","        self.block1 = nn.Sequential(\n","            Conv2d(in_channels=input_channels,\n","                      out_channels=16, #output channels = number of filters\n","                      kernel_size=7,\n","                      stride=2,\n","                      padding=3),\n","\n","            nn.BatchNorm2d(16),\n","            LeakyReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        )\n","\n","\n","        # self.block2 = nn.Sequential(*create_resnet_block(16, 32, 2))\n","        self.block2 = nn.Sequential(ConvBlock(#num_input_channels=16,\n","                                              num_filters=32,\n","                                              kernel =3,\n","                                              stride_val=2,\n","                                              input_padding =1 ),\n","\n","                                    IdentityBlock(#num_input_channels=32,\n","                                                  num_filters=32,\n","                                                  kernel=3,\n","                                                  input_padding=1))\n","\n","        # TEST ************************\n","        # Define Layer Normalization and Multi-head Attention layers\n","        # # self.norm = nn.LayerNorm(32)\n","        # self.mha = nn.MultiheadAttention(16, num_heads=1, batch_first=True)\n","        # self.scale = nn.Parameter(torch.zeros(1))\n","\n","        # self.block3 = nn.Sequential(*create_resnet_block(32, 64, 2))\n","        self.block3 = nn.Sequential(ConvBlock(#num_input_channels=32,\n","                                              num_filters=64,\n","                                              kernel =3,\n","                                              stride_val=2,\n","                                              input_padding =1 ),\n","\n","                                    IdentityBlock(#num_input_channels=64,\n","                                                  num_filters=64,\n","                                                  kernel=3,\n","                                                  input_padding=1))\n","\n","        # self.block4 = nn.Sequential(*create_resnet_block(64, 128, 2))\n","        self.block4 = nn.Sequential(ConvBlock(#num_input_channels=64,\n","                                              num_filters=128,\n","                                              kernel =3,\n","                                              stride_val=2,\n","                                              input_padding =1 ),\n","\n","                                    IdentityBlock(#num_input_channels=128,\n","                                                  num_filters=128,\n","                                                  kernel=3,\n","                                                  input_padding=1))\n","\n","        # self.block5 = nn.Sequential(*create_resnet_block(128, 256, 2))\n","        self.block5 = nn.Sequential(ConvBlock(#num_input_channels=128,\n","                                              num_filters=256,\n","                                              kernel =3,\n","                                              stride_val=2,\n","                                              input_padding =1 ),\n","\n","                                    IdentityBlock(#num_input_channels=256,\n","                                                  num_filters=256,\n","                                                  kernel=3,\n","                                                  input_padding=1))\n","\n","        # could try replacing FC with global average pooling\n","        # nn.AvgPool2d or F.avg_pool2d with kernel_size equal to the dimensions of the feature maps (in this case, 8).\n","        # https://discuss.pytorch.org/t/global-average-pooling-in-pytorch/6721/\n","        # https://blog.paperspace.com/network-in-network-utility-of-1-x-1-convolution-layers/#:~:text=As%20we%20know%20by%20now,called%201%20x%201%20convolution.\n","        # self.fc1 = nn.Linear(256, num_classes) #'fc' = fully connected\n","        # self.fc1 = nn.AvgPool2d(3) #was 256\n","\n","\n","    def use_attention(self, x):\n","      # Reshape input for multi-head attention\n","      bs, c, h, w = x.shape\n","      x_att = x.reshape(bs, c, h * w).transpose(1, 2)  # BSxHWxC\n","\n","      # Apply Layer Normalization\n","      # x_att = self.norm(x_att)\n","      # Apply Multi-head Attention\n","      att_out, att_map  = self.mha(x_att, x_att, x_att)\n","      return att_out.transpose(1, 2).reshape(bs, c, h, w), att_map\n","\n","    def forward(self, x) -> torch.Tensor :\n","        x = self.block1(x)\n","        # TEST\n","        # Apply self-attention mechanism and add to the input\n","        # x = self.scale * self.use_attention(x)[0] + x\n","\n","        x = self.block2(x)\n","        # TEST\n","        # Apply self-attention mechanism and add to the input\n","        # x = self.scale1 * self.use_attention(x)[0] + x\n","\n","        x = self.block3(x)\n","        # TEST\n","        # Apply self-attention mechanism and add to the input\n","        # x = self.scale1 * self.use_attention(x)[0] + x\n","\n","        x = self.block4(x)\n","        # TEST\n","        # Apply self-attention mechanism and add to the input\n","        # x = self.scale1 * self.use_attention(x)[0] + x\n","\n","        x = self.block5(x)\n","        bs, _, _, _ = x.shape\n","        # use adaptive pooling to account for different size images\n","        # the PyTorch implementation of Faster R-CNN resizes images\n","        # differently based on whether the model is in training or eval mode\n","        # adaptive_avg_pool takes variable sized image and downsizes it to a fixed size\n","        # this layer works by 'adaptively changing the pooling filter size based on the input image size'\n","        # https://discuss.pytorch.org/t/how-to-create-convnet-for-variable-size-input-dimension-images/1906/3\n","        # x = F.adaptive_avg_pool2d(input=x, output_size=1).reshape(bs, -1)\n","        # x = self.fc1(x)\n","\n","        return x"],"metadata":{"id":"xEk3YiuuRVUv","executionInfo":{"status":"ok","timestamp":1738181319287,"user_tz":0,"elapsed":995,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","backbone = CustomResNet(num_classes=4)\n","backbone.out_channels = 256\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256, 512),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=['0'],\n","    output_size=7,\n","    sampling_ratio=2\n",")\n","\n","model = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=4,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=4 #3 classes +1 to account for background\n","model.override_prediction_heads(num_classes)\n","\n","\n","model.model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1cddb4d1-fc00-4797-97b3-9aa04071bcd9","id":"NoIkHz0sBIxD","executionInfo":{"status":"ok","timestamp":1738181319689,"user_tz":0,"elapsed":7,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): CustomResNet(\n","    (block1): Sequential(\n","      (0): Conv2d(3, 16, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n","      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): LeakyReLU(negative_slope=0.01)\n","      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    )\n","    (block2): Sequential(\n","      (0): ConvBlock(\n","        (conv1): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv_input_transformer): LazyConv2d(0, 32, kernel_size=(1, 1), stride=(2, 2))\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","      (1): IdentityBlock(\n","        (conv1): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","    )\n","    (block3): Sequential(\n","      (0): ConvBlock(\n","        (conv1): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv_input_transformer): LazyConv2d(0, 64, kernel_size=(1, 1), stride=(2, 2))\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","      (1): IdentityBlock(\n","        (conv1): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","    )\n","    (block4): Sequential(\n","      (0): ConvBlock(\n","        (conv1): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv_input_transformer): LazyConv2d(0, 128, kernel_size=(1, 1), stride=(2, 2))\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","      (1): IdentityBlock(\n","        (conv1): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","    )\n","    (block5): Sequential(\n","      (0): ConvBlock(\n","        (conv1): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv_input_transformer): LazyConv2d(0, 256, kernel_size=(1, 1), stride=(2, 2))\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","      (1): IdentityBlock(\n","        (conv1): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (conv2): LazyConv2d(0, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (leaky_relu): LeakyReLU(negative_slope=0.01)\n","      )\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["# model.model.backbone"],"metadata":{"id":"LYW66u_XEpw9","executionInfo":{"status":"ok","timestamp":1738181319689,"user_tz":0,"elapsed":6,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# optimizer = torch.optim.SGD(model.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.Adam(model.model.parameters(), lr=0.0005)\n","num_epochs = 15\n","model_pipeline(model , optimizer, num_epochs, train_dataset_loader,\n","               val_dataset_loader, test_dataset_loader, device,\n","               False, \"Custom Backbone Faster R-CNN Tracker\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"SYMGLpe7D-5g","executionInfo":{"status":"error","timestamp":1738181340292,"user_tz":0,"elapsed":20609,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"0c41a3a9-f285-4ca0-8094-7ee488f69ff4"},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.5"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250129_200838-v9cm882r</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker/runs/v9cm882r' target=\"_blank\">rose-water-5</a></strong> to <a href='https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker' target=\"_blank\">https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker/runs/v9cm882r' target=\"_blank\">https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker/runs/v9cm882r</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 0 of 15\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"<ipython-input-45-da6d93805adf>\", line 79, in model_pipeline\n","    losses_dict = modelWrapper.train(num_epochs, train_dataset_loader, val_dataset_loader, optimizer)\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<ipython-input-12-87e4a927fe0b>\", line 167, in train\n","    batch_training_loss = self.__train_batch(batch, optimizer)\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<ipython-input-12-87e4a927fe0b>\", line 214, in __train_batch\n","    loss_dict = self.model(images, targets)\n","                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\", line 104, in forward\n","    proposals, proposal_losses = self.rpn(images, features, targets)\n","                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\", line 361, in forward\n","    objectness, pred_bbox_deltas = self.head(features)\n","                                   ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\", line 75, in forward\n","    t = self.conv(feature)\n","        ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n","    input = module(input)\n","            ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n","    input = module(input)\n","            ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n","    return F.conv2d(\n","           ^^^^^^^^^\n","RuntimeError: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 256]\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">rose-water-5</strong> at: <a href='https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker/runs/v9cm882r' target=\"_blank\">https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker/runs/v9cm882r</a><br> View project at: <a href='https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker' target=\"_blank\">https://wandb.ai/otterhian-student/Custom%20Backbone%20Faster%20R-CNN%20Tracker</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250129_200838-v9cm882r/logs</code>"]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 256]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-1fecaf1bb2e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model_pipeline(model , optimizer, num_epochs, train_dataset_loader,  \n\u001b[0m\u001b[1;32m      5\u001b[0m                \u001b[0mval_dataset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                True, \"Custom Backbone Faster R-CNN Tracker\")\n","\u001b[0;32m<ipython-input-45-da6d93805adf>\u001b[0m in \u001b[0;36mmodel_pipeline\u001b[0;34m(modelWrapper, optimizer, num_epochs, train_dataset_loader, val_dataset_loader, test_dataset_loader, device, isLogging, projectName)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;31m# train the model and measure training time to log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mlosses_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mexecution_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-87e4a927fe0b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, train_dataset, val_dataset, optimizer)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mepoch_loss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m           \u001b[0mbatch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m           \u001b[0mepoch_loss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_training_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-87e4a927fe0b>\u001b[0m in \u001b[0;36m__train_batch\u001b[0;34m(self, train_batch, optimizer)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;31m#IS THIS APPROACH COMPARABLE TO LOSS CALCULATION IN YOLO?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m#IN ORDER TO MAKE COMPARISONS BETWEEN ARCHITECTURES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0;31m# RPN uses all feature maps that are available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mbbox_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mbbox_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox_pred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [32, 256]"]}]},{"cell_type":"code","source":["stop\n","# save model state after training\n","torch.save(model.state_dict(), 'model_weights.pth')\n","\n","# load model weights into newly instantiated model\n","# model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model\n","model.load_state_dict(torch.load('model_weights.pth', weights_only=True))"],"metadata":{"id":"SZLw2F1aSa5-","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":5,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# inception module\n","# https://learning.oreilly.com/library/view/mastering-pytorch/9781801074308/Text/Chapter_02.xhtml#_idParaDest-40"],"metadata":{"id":"lMnMo8weSo7f","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# see output shape\n","# for layer in model.children():\n","#     if hasattr(layer, 'out_features'):\n","#         print(layer.out_features)"],"metadata":{"id":"49Ko2ABi-4O3","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0edS2ekEu-wY"},"source":["## YOLOv11 Implementation: <a class=\"anchor\" name=\"YOLOv11\"></a>"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0yQpNRJXTtlx","executionInfo":{"status":"ok","timestamp":1738237512125,"user_tz":0,"elapsed":69960,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"956b4ac3-6cf4-4dd2-d4e1-2192fc79ecd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.70-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.6)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.70-py3-none-any.whl (914 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.70 ultralytics-thop-2.0.14\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3zNmNytbi7rz","executionInfo":{"status":"ok","timestamp":1738237521619,"user_tz":0,"elapsed":9498,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bb7563b-65ec-4b18-d01e-d483c612a021"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"XO2YxE89XYiG","executionInfo":{"status":"ok","timestamp":1738237549653,"user_tz":0,"elapsed":28037,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"94bf1ab2-d939-4d03-ff54-7d26e58a3169"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import wandb\n","wandb.login()\n","# is there a way to pre-train on visDrone ?\n","# results = model.train(data=\"VisDrone.yaml\", epochs=100, imgsz=640)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QIe0Q0dXXQYK","executionInfo":{"status":"ok","timestamp":1738237671484,"user_tz":0,"elapsed":118473,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"93125570-8882-4f63-f55d-83bff0917867"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file  \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33motterhian\u001b[0m (\u001b[33motterhian-student\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.4"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250130_114558-yne90e8z</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/otterhian-student/YOLO%20Tracker/runs/yne90e8z' target=\"_blank\">clear-sun-44</a></strong> to <a href='https://wandb.ai/otterhian-student/YOLO%20Tracker' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/otterhian-student/YOLO%20Tracker' target=\"_blank\">https://wandb.ai/otterhian-student/YOLO%20Tracker</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/otterhian-student/YOLO%20Tracker/runs/yne90e8z' target=\"_blank\">https://wandb.ai/otterhian-student/YOLO%20Tracker/runs/yne90e8z</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 38.8M/38.8M [00:00<00:00, 195MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.70  Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11m.pt, data=/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/data.yaml, epochs=15, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 755k/755k [00:00<00:00, 9.90MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Overriding model.yaml nc=80 with nc=3\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n","  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n","  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n","  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n","  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n","  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n","  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n","  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n"," 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n"," 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n"," 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n","YOLO11m summary: 409 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n","\n","Transferred 643/649 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|| 5.35M/5.35M [00:00<00:00, 68.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/UoL Final Project/Project Data/official project data/train/labels.cache... 197 images, 17 backgrounds, 0 corrupt: 100%|| 197/197 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.2 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n","  check_for_updates()\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/valid/labels... 57 images, 16 backgrounds, 0 corrupt: 100%|| 57/57 [00:05<00:00, 10.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/valid/labels.cache\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Plotting labels to runs/detect/train/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n","Image sizes 640 train, 640 val\n","Using 8 dataloader workers\n","Logging results to \u001b[1mruns/detect/train\u001b[0m\n","Starting training for 15 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/15      8.64G      1.312      3.248      1.083         29        640: 100%|| 13/13 [00:03<00:00,  3.32it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:01<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578       0.33      0.463      0.362      0.227\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/15      8.62G      1.128      1.362      1.006        101        640: 100%|| 13/13 [00:02<00:00,  5.95it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  5.20it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578       0.34       0.47      0.385      0.233\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/15      8.66G      1.174      1.357       1.03         70        640: 100%|| 13/13 [00:02<00:00,  5.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  5.70it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.139      0.245      0.149     0.0967\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/15      8.65G      1.142      1.208     0.9987        199        640: 100%|| 13/13 [00:02<00:00,  6.03it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  3.38it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578    0.00524     0.0617    0.00249    0.00162\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/15      8.73G       1.15      1.159      1.012         93        640: 100%|| 13/13 [00:02<00:00,  6.17it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  5.74it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.803      0.186      0.158      0.101\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Closing dataloader mosaic\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       6/15      8.72G      1.151      1.385     0.9918         66        640: 100%|| 13/13 [00:03<00:00,  3.25it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.21it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.266      0.373      0.224      0.109\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       7/15      8.64G      1.087      1.087     0.9957         59        640: 100%|| 13/13 [00:02<00:00,  6.31it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.07it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.434      0.391      0.311      0.151\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       8/15       8.7G      1.133      1.089     0.9981        102        640: 100%|| 13/13 [00:02<00:00,  6.42it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.35it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.393      0.436      0.337      0.186\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       9/15      8.58G      1.126      1.113      0.989         79        640: 100%|| 13/13 [00:02<00:00,  6.50it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.39it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.361       0.51       0.37      0.186\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      10/15      8.71G       1.08     0.9763     0.9638         85        640: 100%|| 13/13 [00:02<00:00,  5.96it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.59it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.336      0.458      0.385      0.223\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      11/15      8.66G      1.055     0.9545     0.9703         34        640: 100%|| 13/13 [00:02<00:00,  6.34it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.18it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.423      0.471      0.402       0.23\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      12/15       8.7G      1.001     0.8785     0.9553         39        640: 100%|| 13/13 [00:02<00:00,  6.41it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.82it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.456      0.513      0.465      0.274\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      13/15      8.66G     0.9698     0.8704     0.9355         44        640: 100%|| 13/13 [00:01<00:00,  6.58it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  7.26it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.495      0.506      0.452       0.27\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      14/15      8.64G      0.944     0.8069     0.9329         61        640: 100%|| 13/13 [00:01<00:00,  6.63it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.84it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.519      0.518      0.475      0.288\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["      15/15      8.69G     0.9188     0.7856     0.9177         48        640: 100%|| 13/13 [00:01<00:00,  6.61it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  6.80it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.547      0.511      0.496      0.302\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","15 epochs completed in 0.018 hours.\n","Optimizer stripped from runs/detect/train/weights/last.pt, 40.5MB\n","Optimizer stripped from runs/detect/train/weights/best.pt, 40.5MB\n","\n","Validating runs/detect/train/weights/best.pt...\n","Ultralytics 8.3.70  Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","YOLO11m summary (fused): 303 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:00<00:00,  5.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all         57        578      0.546      0.512      0.497      0.302\n","      migrating_single         32        161      0.537      0.425      0.475      0.229\n","               plastic         32        240      0.584      0.233      0.257      0.111\n","        water_hyacinth         36        177      0.517      0.876      0.759      0.565\n","Speed: 0.1ms preprocess, 1.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training time</td><td></td></tr><tr><td>fitness</td><td></td></tr><tr><td>metrics/mAP50(B)</td><td></td></tr><tr><td>metrics/mAP50-95(B)</td><td></td></tr><tr><td>metrics/precision(B)</td><td></td></tr><tr><td>metrics/recall(B)</td><td></td></tr><tr><td>migrating_single_map50</td><td></td></tr><tr><td>migrating_single_map95</td><td></td></tr><tr><td>plastic_map50</td><td></td></tr><tr><td>plastic_map95</td><td></td></tr><tr><td>water_hyacinth_map50</td><td></td></tr><tr><td>water_hyacinth_map95</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training time</td><td>98.28418</td></tr><tr><td>fitness</td><td>0.32129</td></tr><tr><td>metrics/mAP50(B)</td><td>0.49733</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.30173</td></tr><tr><td>metrics/precision(B)</td><td>0.54629</td></tr><tr><td>metrics/recall(B)</td><td>0.51151</td></tr><tr><td>migrating_single_map50</td><td>0.47546</td></tr><tr><td>migrating_single_map95</td><td>0.22929</td></tr><tr><td>plastic_map50</td><td>0.25713</td></tr><tr><td>plastic_map95</td><td>0.1111</td></tr><tr><td>water_hyacinth_map50</td><td>0.7594</td></tr><tr><td>water_hyacinth_map95</td><td>0.56481</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">clear-sun-44</strong> at: <a href='https://wandb.ai/otterhian-student/YOLO%20Tracker/runs/yne90e8z' target=\"_blank\">https://wandb.ai/otterhian-student/YOLO%20Tracker/runs/yne90e8z</a><br> View project at: <a href='https://wandb.ai/otterhian-student/YOLO%20Tracker' target=\"_blank\">https://wandb.ai/otterhian-student/YOLO%20Tracker</a><br>Synced 5 W&B file(s), 2 media file(s), 3 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20250130_114558-yne90e8z/logs</code>"]},"metadata":{}}],"source":["from ultralytics import YOLO\n","import time\n","\n","# YOLO hyperparameter list: https://docs.ultralytics.com/usage/cfg/#train-settings\n","hyperparameters = {\n","      \"epochs\":15,\n","      # iterations=300,\n","      # Optimizer Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for automatic selection based on model configuration.\n","      \"optimizer\": \"AdamW\",\n","      \"img_size\": 640,\n","      \"batch_size\":16,\n","      \"cos_lr_scheduler\": True,\n","      \"weight_decay\":0.0005,\n","      \"warm_up_epochs\" : 3.0,\n","  }\n","\n","# code used to compare performance across YOLO versions\n","# YOLO_11_models = [\"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\", \"yolo11l.pt\", \"yolo11x.pt\"]\n","# YOLO_10_models = [\"yolov10n.pt\", \"yolov10s.pt\", \"yolov10m.pt\", \"yolov10l.pt\", \"yolov10x.pt\"]\n","# YOLO_9_models = [\"yolov9t.pt\", \"yolov9s.pt\", \"yolov9m.pt\", \"yolov9c.pt\", \"yolov9e.pt\"]\n","# YOLO_5_models = [\"yolov5n.pt\"] #try one and see if performance is better\n","\n","# for index, model in enumerate(YOLO_5_models):\n","#   with wandb.init(project=\"YOLO Tracker\", config=hyperparameters, name=f\"{model}\"):\n","\n","with wandb.init(project=\"YOLO Tracker\", config=hyperparameters):\n","    # Load a model\n","    model = \"yolo11m.pt\"\n","    model = YOLO(model)\n","\n","    # Train the model\n","    start_time = time.time()\n","\n","    train_results = model.train(\n","        data=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/data.yaml\", # path to dataset YAML\n","        epochs = hyperparameters[\"epochs\"], # number of training epochs\n","        imgsz = hyperparameters[\"img_size\"], # training image size\n","        batch = hyperparameters[\"batch_size\"],\n","        cos_lr = hyperparameters[\"cos_lr_scheduler\"],\n","        weight_decay = hyperparameters[\"weight_decay\"],\n","        warmup_epochs = hyperparameters[\"warm_up_epochs\"],\n","        plots=True, #save plots for analysis\n","        device=\"cuda:0\"\n","        )\n","\n","    end_time = time.time()\n","    execution_time = end_time - start_time\n","    # log training time\n","    wandb.log({\"Training time\": execution_time})\n","\n","    class_wise_metrics_map50 = {}\n","    class_wise_metrics_map95 = {}\n","    for class_index in train_results.ap_class_index:\n","      map50, map50_95 = train_results.class_result(class_index)[2], train_results.class_result(class_index)[3]\n","      class_wise_metrics_map50[f\"{train_results.names[class_index]}_map50\"] = map50\n","      class_wise_metrics_map95[f\"{train_results.names[class_index]}_map95\"] = map50_95\n","\n","\n","    wandb.log(class_wise_metrics_map50)\n","    wandb.log(class_wise_metrics_map95)\n","    wandb.log(train_results.results_dict)\n","\n","    # the following if block was used to log performance across multiple models\n","    # if index ==0:\n","    #   wandb.log({\"PR_curve\": wandb.Image(\"/content/runs/detect/train/PR_curve.png\")})\n","    #   wandb.log({\"Results\": wandb.Image(\"/content/runs/detect/train/results.png\")})\n","    # else:\n","    #   # second training folder is called 'train2' so add 1 to the index\n","    #   wandb.log({\"PR_curve\": wandb.Image(f\"/content/runs/detect/train{index+1}/PR_curve.png\")})\n","    #   wandb.log({\"Results\": wandb.Image(f\"/content/runs/detect/train{index+1}/results.png\")})\n","\n","    wandb.log({\"PR_curve\": wandb.Image(f\"/content/runs/detect/train/PR_curve.png\")})\n","    wandb.log({\"Results\": wandb.Image(f\"/content/runs/detect/train/results.png\")})\n","\n","    wandb.run.log_code(\"/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test\", include_fn=lambda path: path.endswith(\".ipynb\"))\n"]},{"cell_type":"code","source":["#save the custom trained model to be used with SAHI (slicing aided hyper inference)\n","yolo_model_path = './Custom_YOLO_v11.pt'\n","model.save(yolo_model_path)"],"metadata":{"id":"FVCE2mZD_ON5","executionInfo":{"status":"ok","timestamp":1738237858976,"user_tz":0,"elapsed":183,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## YOLO with Image Slicing (SAHI): <a class=\"anchor\" name=\"yolo_sahi\"></a>"],"metadata":{"id":"jitPiR2a-zRQ"}},{"cell_type":"code","source":["!pip install sahi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bul1msFLCtZp","executionInfo":{"status":"ok","timestamp":1738237841682,"user_tz":0,"elapsed":4195,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"d0f20540-35f6-465e-b2c1-38e5dd884a81"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sahi\n","  Downloading sahi-0.11.20-py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: opencv-python<=4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from sahi) (4.10.0.84)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from sahi) (2.0.6)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.11/dist-packages (from sahi) (4.67.1)\n","Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from sahi) (11.1.0)\n","Collecting pybboxes==0.1.6 (from sahi)\n","  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from sahi) (6.0.2)\n","Collecting fire (from sahi)\n","  Downloading fire-0.7.0.tar.gz (87 kB)\n","\u001b[2K     \u001b[90m\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting terminaltables (from sahi)\n","  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from sahi) (2.32.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sahi) (8.1.8)\n","Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from sahi) (1.26.4)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->sahi) (2.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->sahi) (2024.12.14)\n","Downloading sahi-0.11.20-py3-none-any.whl (112 kB)\n","\u001b[2K   \u001b[90m\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\n","Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=acae69fda1201f46ed175e0e0f17ca6c7f761f6335178bca2df0382ce593b639\n","  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n","Successfully built fire\n","Installing collected packages: terminaltables, pybboxes, fire, sahi\n","Successfully installed fire-0.7.0 pybboxes-0.1.6 sahi-0.11.20 terminaltables-3.1.10\n"]}]},{"cell_type":"code","source":["import os\n","# import required functions, classes\n","from sahi import AutoDetectionModel\n","from sahi.predict import get_sliced_prediction, predict, get_prediction\n","from sahi.utils.file import download_from_url\n","from sahi.utils.cv import read_image\n","from IPython.display import Image\n","\n","\n","\n","detection_model = AutoDetectionModel.from_pretrained(\n","    model_type=\"ultralytics\",\n","    model_path=yolo_model_path,\n","    confidence_threshold=0.3,\n","    device=\"cuda:0\",\n",")"],"metadata":{"id":"E81qKjV1-xtN","executionInfo":{"status":"ok","timestamp":1738237865197,"user_tz":0,"elapsed":969,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["source_image_dir = \"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/600 flipped/test/images\"\n","file_names = os.listdir(path=source_image_dir)\n","slice_height = 256\n","slice_width = 256\n","overlap_height_ratio = 0.2\n","overlap_width_ratio = 0.2\n","\n","for file in file_names[:1]:\n","  results = get_sliced_prediction(\n","      detection_model=detection_model,\n","      image = f\"{source_image_dir}/{file}\",\n","      slice_height=slice_height,\n","      slice_width=slice_width,\n","      overlap_height_ratio=overlap_height_ratio,\n","      overlap_width_ratio=overlap_width_ratio\n","  )\n","\n","  number_bboxes = len(results.to_coco_annotations())\n","  for prediction in results.to_coco_annotations():\n","    print(prediction)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdGy8s79-ygE","executionInfo":{"status":"ok","timestamp":1738237880641,"user_tz":0,"elapsed":4493,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"714b7a34-c8c5-4b6b-d6c3-3055c02fb66b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Performing prediction on 70 slices.\n","{'image_id': None, 'bbox': [1297.37744140625, 284.12310791015625, 37.1409912109375, 43.131439208984375], 'score': 0.870509922504425, 'category_id': 1, 'category_name': 'plastic', 'segmentation': [], 'iscrowd': 0, 'area': 1601}\n","{'image_id': None, 'bbox': [1474.1043701171875, 306.298828125, 28.6312255859375, 24.023162841796875], 'score': 0.7406089305877686, 'category_id': 1, 'category_name': 'plastic', 'segmentation': [], 'iscrowd': 0, 'area': 687}\n","{'image_id': None, 'bbox': [1386.2081756591797, 292.6988067626953, 12.646591186523438, 9.032203674316406], 'score': 0.6493463516235352, 'category_id': 1, 'category_name': 'plastic', 'segmentation': [], 'iscrowd': 0, 'area': 114}\n","{'image_id': None, 'bbox': [1419.0561828613281, 330.1237564086914, 5.8235321044921875, 6.362800598144531], 'score': 0.3012317717075348, 'category_id': 1, 'category_name': 'plastic', 'segmentation': [], 'iscrowd': 0, 'area': 37}\n","{'image_id': None, 'bbox': [1637.5033874511719, 930.2048110961914, 177.53811645507812, 167.0415267944336], 'score': 0.857839047908783, 'category_id': 2, 'category_name': 'water_hyacinth', 'segmentation': [], 'iscrowd': 0, 'area': 29656}\n","{'image_id': None, 'bbox': [1274.7518310546875, 670.9353637695312, 114.17529296875, 80.62908935546875], 'score': 0.7245948314666748, 'category_id': 2, 'category_name': 'water_hyacinth', 'segmentation': [], 'iscrowd': 0, 'area': 9205}\n","{'image_id': None, 'bbox': [1042.935546875, 322.1700134277344, 96.88671875, 93.10430908203125], 'score': 0.702242374420166, 'category_id': 2, 'category_name': 'water_hyacinth', 'segmentation': [], 'iscrowd': 0, 'area': 9020}\n","{'image_id': None, 'bbox': [1190.099609375, 407.98077392578125, 64.6077880859375, 55.22479248046875], 'score': 0.6860753893852234, 'category_id': 2, 'category_name': 'water_hyacinth', 'segmentation': [], 'iscrowd': 0, 'area': 3567}\n","{'image_id': None, 'bbox': [1251.5916748046875, 166.37351989746094, 268.01104736328125, 263.85694885253906], 'score': 0.5775325894355774, 'category_id': 2, 'category_name': 'water_hyacinth', 'segmentation': [], 'iscrowd': 0, 'area': 70716}\n","{'image_id': None, 'bbox': [1640.0, 1025.1254150420427, 238.4254913330078, 81.90919189155102], 'score': 0.5660204291343689, 'category_id': 2, 'category_name': 'water_hyacinth', 'segmentation': [], 'iscrowd': 0, 'area': 19529}\n"]}]},{"cell_type":"code","source":["# run this code to get visuals\n","# results = predict(\n","#     detection_model=detection_model,\n","#     source=source_image_dir,\n","#     slice_height=slice_height,\n","#     slice_width=slice_width,\n","#     overlap_height_ratio=overlap_height_ratio,\n","#     overlap_width_ratio=overlap_width_ratio\n","# )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39WcsKYtDatn","executionInfo":{"status":"ok","timestamp":1738238076211,"user_tz":0,"elapsed":121674,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"b8665652-f229-404c-e56b-db6624aa27ac"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 57 listed files in folder: images/\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   0%|          | 0/57 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 70 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   2%|         | 1/57 [00:01<01:26,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1341.44 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   2%|         | 1/57 [00:01<01:26,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   4%|         | 2/57 [00:03<01:41,  1.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1628.52 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   4%|         | 2/57 [00:04<01:41,  1.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   5%|         | 3/57 [00:05<01:50,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1531.76 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   5%|         | 3/57 [00:06<01:50,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   7%|         | 4/57 [00:07<01:46,  2.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1522.46 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   7%|         | 4/57 [00:08<01:46,  2.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   9%|         | 5/57 [00:10<01:47,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1548.85 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:   9%|         | 5/57 [00:10<01:47,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  11%|         | 6/57 [00:12<01:46,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1500.48 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  11%|         | 6/57 [00:12<01:46,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  12%|        | 7/57 [00:14<01:45,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1502.99 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  12%|        | 7/57 [00:14<01:45,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  14%|        | 8/57 [00:16<01:41,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1536.09 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  14%|        | 8/57 [00:16<01:41,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  16%|        | 9/57 [00:18<01:45,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1741.07 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  16%|        | 9/57 [00:19<01:45,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  18%|        | 10/57 [00:20<01:40,  2.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1543.64 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  18%|        | 10/57 [00:21<01:40,  2.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  19%|        | 11/57 [00:22<01:35,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1496.78 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  19%|        | 11/57 [00:22<01:35,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  21%|        | 12/57 [00:24<01:32,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1572.88 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  21%|        | 12/57 [00:25<01:32,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  23%|       | 13/57 [00:26<01:30,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1548.62 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  23%|       | 13/57 [00:27<01:30,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  25%|       | 14/57 [00:28<01:30,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1569.45 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  25%|       | 14/57 [00:29<01:30,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  26%|       | 15/57 [00:31<01:33,  2.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1517.14 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  26%|       | 15/57 [00:31<01:33,  2.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  28%|       | 16/57 [00:33<01:29,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1513.11 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  28%|       | 16/57 [00:33<01:29,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  30%|       | 17/57 [00:35<01:24,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1520.41 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  30%|       | 17/57 [00:35<01:24,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 70 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  32%|      | 18/57 [00:37<01:20,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1347.92 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  32%|      | 18/57 [00:37<01:20,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  33%|      | 19/57 [00:39<01:19,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1482.95 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  33%|      | 19/57 [00:39<01:19,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  35%|      | 20/57 [00:41<01:16,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1579.98 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  35%|      | 20/57 [00:41<01:16,  2.07s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 70 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  37%|      | 21/57 [00:43<01:12,  2.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1417.65 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  37%|      | 21/57 [00:43<01:12,  2.01s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  39%|      | 22/57 [00:45<01:12,  2.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1607.92 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  39%|      | 22/57 [00:46<01:12,  2.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  40%|      | 23/57 [00:47<01:11,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1677.36 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  40%|      | 23/57 [00:48<01:11,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  42%|     | 24/57 [00:50<01:11,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1536.33 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  42%|     | 24/57 [00:50<01:11,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  44%|     | 25/57 [00:52<01:06,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1534.63 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  44%|     | 25/57 [00:52<01:06,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  46%|     | 26/57 [00:54<01:04,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1516.38 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  46%|     | 26/57 [00:55<01:04,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  47%|     | 27/57 [00:56<01:07,  2.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1537.09 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  47%|     | 27/57 [00:57<01:07,  2.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  49%|     | 28/57 [00:59<01:05,  2.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1497.12 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  49%|     | 28/57 [00:59<01:05,  2.27s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  51%|     | 29/57 [01:01<01:03,  2.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1715.71 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  51%|     | 29/57 [01:01<01:03,  2.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  53%|    | 30/57 [01:03<00:59,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1511.17 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  53%|    | 30/57 [01:04<00:59,  2.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  54%|    | 31/57 [01:06<01:02,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1539.55 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  54%|    | 31/57 [01:06<01:02,  2.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  56%|    | 32/57 [01:08<00:56,  2.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1570.68 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  56%|    | 32/57 [01:08<00:56,  2.26s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  58%|    | 33/57 [01:10<00:54,  2.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1605.23 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  58%|    | 33/57 [01:10<00:54,  2.25s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  60%|    | 34/57 [01:12<00:51,  2.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1540.61 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  60%|    | 34/57 [01:13<00:51,  2.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  61%|   | 35/57 [01:14<00:47,  2.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1525.58 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  61%|   | 35/57 [01:14<00:47,  2.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  63%|   | 36/57 [01:16<00:43,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1534.63 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  63%|   | 36/57 [01:16<00:43,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  65%|   | 37/57 [01:18<00:42,  2.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1644.45 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  65%|   | 37/57 [01:19<00:42,  2.12s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  67%|   | 38/57 [01:20<00:40,  2.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1539.19 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  67%|   | 38/57 [01:21<00:40,  2.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  68%|   | 39/57 [01:23<00:37,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1527.74 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  68%|   | 39/57 [01:23<00:37,  2.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  70%|   | 40/57 [01:25<00:36,  2.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1502.94 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  70%|   | 40/57 [01:25<00:36,  2.18s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  72%|  | 41/57 [01:27<00:34,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1493.25 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  72%|  | 41/57 [01:27<00:34,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  74%|  | 42/57 [01:29<00:31,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1570.13 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  74%|  | 42/57 [01:29<00:31,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  75%|  | 43/57 [01:31<00:28,  2.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1510.34 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  75%|  | 43/57 [01:31<00:28,  2.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  77%|  | 44/57 [01:33<00:27,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1517.16 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  77%|  | 44/57 [01:33<00:27,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  79%|  | 45/57 [01:35<00:24,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1566.51 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  79%|  | 45/57 [01:35<00:24,  2.05s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  81%|  | 46/57 [01:37<00:22,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1539.10 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  81%|  | 46/57 [01:38<00:22,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  82%| | 47/57 [01:40<00:21,  2.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1514.43 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  82%| | 47/57 [01:40<00:21,  2.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  84%| | 48/57 [01:42<00:19,  2.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1519.36 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  84%| | 48/57 [01:42<00:19,  2.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  86%| | 49/57 [01:44<00:17,  2.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1548.92 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  86%| | 49/57 [01:44<00:17,  2.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  88%| | 50/57 [01:46<00:14,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1613.43 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  88%| | 50/57 [01:47<00:14,  2.09s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  89%| | 51/57 [01:48<00:13,  2.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1567.58 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  89%| | 51/57 [01:49<00:13,  2.23s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  91%| | 52/57 [01:51<00:11,  2.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1512.52 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  91%| | 52/57 [01:51<00:11,  2.24s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  93%|| 53/57 [01:53<00:08,  2.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1496.70 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  93%|| 53/57 [01:53<00:08,  2.22s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 70 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  95%|| 54/57 [01:54<00:06,  2.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1391.75 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  95%|| 54/57 [01:55<00:06,  2.08s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  96%|| 55/57 [01:57<00:04,  2.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1530.86 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  96%|| 55/57 [01:57<00:04,  2.17s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  98%|| 56/57 [01:59<00:02,  2.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1650.32 ms\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images:  98%|| 56/57 [01:59<00:02,  2.14s/it]"]},{"output_type":"stream","name":"stdout","text":["Performing prediction on 80 slices.\n"]},{"output_type":"stream","name":"stderr","text":["Performing inference on images: 100%|| 57/57 [02:01<00:00,  2.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Prediction time is: 1540.71 ms\n","Prediction results are successfully exported to runs/predict/exp\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWMFsh52XtXd","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["sys.exit()"]},{"cell_type":"markdown","metadata":{"id":"59FMkDBXnmpd"},"source":["## Appendix A: <a class=\"anchor\" name=\"AppenA\"></a>\n","Attempts to override FasterRCNN forward() method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vgv8c_3brIOb","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["##***************************************TEST\n","from torchvision.models.detection import FasterRCNN\n","\n","import warnings\n","\n","class ModifiedFasterRCNN(FasterRCNN):\n","  def forward(self, images, targets=None):\n","        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","        \"\"\"\n","        Args:\n","            images (list[Tensor]): images to be processed\n","            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","\n","        Returns:\n","            result (list[BoxList] or dict[Tensor]): the output from the model.\n","                During training, it returns a dict[Tensor] which contains the losses.\n","                During testing, it returns list[BoxList] contains additional fields\n","                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","\n","        \"\"\"\n","        if self.training and targets is None:\n","            raise ValueError(\"In training mode, targets should be passed\")\n","        if self.training:\n","            assert targets is not None\n","            for target in targets:\n","                boxes = target[\"boxes\"]\n","                if isinstance(boxes, torch.Tensor):\n","                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n","                        raise ValueError(f\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\")\n","                else:\n","                    raise ValueError(f\"Expected target boxes to be of type Tensor, got {type(boxes)}.\")\n","\n","        original_image_sizes: List[Tuple[int, int]] = []\n","        for img in images:\n","            val = img.shape[-2:]\n","            assert len(val) == 2\n","            original_image_sizes.append((val[0], val[1]))\n","\n","        images, targets = self.transform(images, targets)\n","\n","        # Check for degenerate boxes\n","        # TODO: Move this to a function\n","        if targets is not None:\n","            for target_idx, target in enumerate(targets):\n","                boxes = target[\"boxes\"]\n","                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n","                if degenerate_boxes.any():\n","                    # print the first degenerate box\n","                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                    degen_bb: List[float] = boxes[bb_idx].tolist()\n","                    raise ValueError(\n","                        \"All bounding boxes should have positive height and width.\"\n","                        f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n","                    )\n","\n","        features = self.backbone(images.tensors)\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","        proposals, proposal_losses = self.rpn(images, features, targets)\n","        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n","        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","        losses = {}\n","        losses.update(detector_losses)\n","        losses.update(proposal_losses)\n","\n","        if torch.jit.is_scripting():\n","            if not self._has_warned:\n","                warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n","                self._has_warned = True\n","            return losses, detections\n","        else:\n","            #MODIFIED THIS LINE\n","            return losses, detections\n","\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers, _mobilenet_extractor\n","from torchvision.ops import misc as misc_nn_ops\n","from torchvision.models.resnet import resnet50\n","from torchvision._internally_replaced_utils import load_state_dict_from_url\n","from torchvision.models.detection._utils import overwrite_eps\n","\n","model_urls = {\n","    \"fasterrcnn_resnet50_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\",\n","    \"fasterrcnn_mobilenet_v3_large_320_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\",\n","    \"fasterrcnn_mobilenet_v3_large_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\",\n","}\n","\n","def modified_fasterrcnn_resnet50_fpn(\n","    pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=None, **kwargs\n","):\n","    \"\"\"\n","    Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n","\n","    Reference: `\"Faster R-CNN: Towards Real-Time Object Detection with\n","    Region Proposal Networks\" <https://arxiv.org/abs/1506.01497>`_.\n","\n","    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each\n","    image, and should be in ``0-1`` range. Different images can have different sizes.\n","\n","    The behavior of the model changes depending if it is in training or evaluation mode.\n","\n","    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n","    containing:\n","\n","        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (``Int64Tensor[N]``): the class label for each ground-truth box\n","\n","    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression\n","    losses for both the RPN and the R-CNN.\n","\n","    During inference, the model requires only the input tensors, and returns the post-processed\n","    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as\n","    follows, where ``N`` is the number of detections:\n","\n","        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (``Int64Tensor[N]``): the predicted labels for each detection\n","        - scores (``Tensor[N]``): the scores of each detection\n","\n","    For more details on the output, you may refer to :ref:`instance_seg_output`.\n","\n","    Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.\n","\n","    Example::\n","\n","        >>> model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","        >>> # For training\n","        >>> images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n","        >>> boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n","        >>> labels = torch.randint(1, 91, (4, 11))\n","        >>> images = list(image for image in images)\n","        >>> targets = []\n","        >>> for i in range(len(images)):\n","        >>>     d = {}\n","        >>>     d['boxes'] = boxes[i]\n","        >>>     d['labels'] = labels[i]\n","        >>>     targets.append(d)\n","        >>> output = model(images, targets)\n","        >>> # For inference\n","        >>> model.eval()\n","        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","        >>> predictions = model(x)\n","        >>>\n","        >>> # optionally, if you want to export the model to ONNX:\n","        >>> torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","        num_classes (int): number of output classes of the model (including the background)\n","        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n","        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n","            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is\n","            passed (the default) this value is set to 3.\n","    \"\"\"\n","    trainable_backbone_layers = _validate_trainable_layers(\n","        pretrained or pretrained_backbone, trainable_backbone_layers, 5, 3\n","    )\n","\n","    if pretrained:\n","        # no need to download the backbone if pretrained is set\n","        pretrained_backbone = False\n","\n","    backbone = resnet50(pretrained=pretrained_backbone, progress=progress, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","    backbone = _resnet_fpn_extractor(backbone, trainable_backbone_layers)\n","    #MODIFIED THE LINE BELOW\n","    model = ModifiedFasterRCNN(backbone, num_classes, **kwargs)\n","\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls[\"fasterrcnn_resnet50_fpn_coco\"], progress=progress)\n","        model.load_state_dict(state_dict)\n","        overwrite_eps(model, 0.0)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHcdohwJsk8J","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["#TEST\n","model = modified_fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","num_classes = 4  #+1 for background - there are 3 classes\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","source":["# custom_model_state = custom_model.state_dict().__str__()\n","# model_state = model.state_dict().__str__()\n","# model_state == custom_model_state"],"metadata":{"id":"INtN-Rj0ItMS","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Appendix B: <a class=\"anchor\" name=\"AppenB\"></a>\n","Code used to check validity of Faster R-CNN implementation"],"metadata":{"id":"uI8xAC1p-jET"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4b0670af-ea6a-4b59-ae58-79767e4fad07","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #- 0.1568 mAP50\n","#model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True) - 0.1080 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) - 0.0434 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True) - 0.0161 mAP50\n","\n","#needs separate fine tuning process for this model\n","#model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n","\n","# BASELINE CHECK\n","# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","# from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n","\n","# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","# num_classes = 4  #+1 for background - there are 3 classes\n","# in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","source":["# optimizer = torch.optim.SGD(model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","# num_epochs = 15\n","# model_pipeline(model, optimizer, num_epochs, train_dataset_loader,  val_dataset_loader, test_dataset_loader, device, False)"],"metadata":{"id":"BTktjy3jvw-s","executionInfo":{"status":"aborted","timestamp":1738181340293,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"1JCtwPguP-eUqZEATKRbuAm_E21850bjI","authorship_tag":"ABX9TyMvS+NGsgGzkpzpCYWOYwt2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}