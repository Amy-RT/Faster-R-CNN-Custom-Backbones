{"cells":[{"cell_type":"markdown","metadata":{"id":"IEvqT7ibs91z"},"source":["# Table of Contents\n","\n","\n","[Definition of Utility Functions](#util_func) \u003cbr/\u003e\n","[Faster R-CNN Implementation](#fasterRCNN): \u003cbr/\u003e\n","- [Baseline Implementation](#fasterRCNN_baseline) \u003cbr/\u003e\n","- [Custom anchor sizes](#custom_anchor) \u003cbr/\u003e\n","\n","\u003cbr/\u003e\u003cbr/\u003e\n","[YOLOv11 Implementation](#YOLOv11): \u003cbr/\u003e\n","[Appendix A](#AppenA): \u003cbr/\u003e\n","\n","remember: First Bullet Header \u003ca class=\"anchor\" name=\"first-bullet\"\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1736519606276,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"PnBVLmZo69V_"},"outputs":[],"source":["#set up github"]},{"cell_type":"markdown","metadata":{"id":"2UDsTyiRurPL"},"source":["## Definition of Utility Functions: \u003ca class=\"anchor\" name=\"util_func\"\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":395,"status":"ok","timestamp":1736519607678,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"gY2GwNcpyJD0"},"outputs":[],"source":["import shutil\n","\n","# !git pull\n","# !git status\n","\n","#check remotes\n","#!git remote -v\n","\n","\n","def git_push(message, branch):\n","  #move to git repo before using git commands\n","  %cd '/content/Object-Detection-Neural-Networks'\n","\n","  #copy current notebook to cloned git repo\n","  file_to_copy = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/UoL Final Project based on CM3015 template \"deep learning on public dataset\".ipynb'\n","  git_repo = '/content/Object-Detection-Neural-Networks'\n","\n","  #overwrite the file in the local cloned github repo\n","  shutil.copy(file_to_copy, git_repo)\n","\n","  !git add 'UoL Final Project based on CM3015 template \"deep learning on public dataset\".ipynb'\n","\n","  !git commit -a -m \"{message}\"\n","  !git push origin \"{branch}\"\n","\n","def publish_branch(branch):\n","  %cd '/content/Object-Detection-Neural-Networks'\n","  !git push -u origin \"{branch}\"\n","\n","def create_branch(branch):\n","  %cd '/content/Object-Detection-Neural-Networks'\n","  !git checkout -b \"{branch}\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1736519607678,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"uLIWdd_hctyT","outputId":"10ba0fc6-b4c2-4b28-f69d-5352d3c28f3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 20] Not a directory: '/content/Object-Detection-Neural-Networks'\n","/content\n","fatal: not a git repository (or any of the parent directories): .git\n"]}],"source":["%cd '/content/Object-Detection-Neural-Networks'\n","!git branch"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1736519607678,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"mkwGtsS-Xm7F","outputId":"7835e24e-7fb8-454f-dad3-4df15e84b9a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 20] Not a directory: '/content/Object-Detection-Neural-Networks'\n","/content\n","fatal: not a git repository (or any of the parent directories): .git\n"]}],"source":["#CURRENT BRANCH\n","%cd '/content/Object-Detection-Neural-Networks'\n","!git switch \"training_func\""]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1409,"status":"ok","timestamp":1736519609085,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"RLFEC068ecPq","outputId":"8876085a-b631-440b-9755-566098181947"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 20] Not a directory: '/content/Object-Detection-Neural-Networks'\n","/content\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n","fatal: not a git repository (or any of the parent directories): .git\n"]}],"source":["git_push(\"Initial Prototype of validation loss calculator\", \"training_func\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1928,"status":"ok","timestamp":1736519611011,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"16yjq-CUqS3I","outputId":"e03d319d-15d3-4251-961e-9b3bb35e3f79"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["import random\n","import numpy as np\n","import torch\n","\n","#print(torch.__version__)\n","device = torch.device(\"cuda:0\")\n","print(device)\n","\n","#the google style guide suggest that a docstring is not necessary for the set_seeds() function\n","#as it does not meet any of the following critera\n","#A docstring is mandatory for every function that has one or more of the following properties:\n","  # - being part of the public API\n","  # - nontrivial size\n","  # - non-obvious logic\n","#https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings\n","\n","#THE SET_SEEDS FUNCTION IS SOURCED FROM:\n","#https://learnopencv.com/fine-tuning-faster-r-cnn/#aioseo-code-walkthrough\n","def set_seeds():\n","  # fix random seeds\n","  SEED_VALUE = 42\n","  random.seed(SEED_VALUE)\n","  np.random. seed(SEED_VALUE)\n","  torch.manual_seed(SEED_VALUE)\n","  if torch.cuda.is_available:\n","    torch.cuda.manual_seed(SEED_VALUE)\n","    torch.cuda.manual_seed_all(SEED_VALUE)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","set_seeds()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736519611011,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"L13RTJUKur1f"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import torch\n","\n","def visualisePredictions(img_id: int,\n","                         test_dataset: torch.utils.data.dataset.Subset,\n","                         predicted: list) -\u003e None:\n","\n","  \"\"\"Visualises model predictions (bounding boxes) on top of the corresponding image\n","\n","    Retrieves the original image from the test set and plots bouning boxes onto the image.\n","    Each classes is represented by a different colour bounding box.\n","\n","    Args:\n","        img_id: the number of the image within the batch,\n","\n","        test_dataset: Subset of PyTorch dataset ; each item in the array is a tuple consisting of\n","        a tensor representing the image and a dictionary containing 'boxes' and 'labels' keys,\n","\n","        precicted: array of model predictions for set of images ; the array consists of\n","        dictionaries each with 2 keys: boxes and labels\n","\n","    Returns:\n","        No value returned ; only plt.show()\n","\n","    Raises:\n","        IndexError: An error occurred accessing the image at the specified index.\n","    \"\"\"\n","\n","  try:\n","    img = test_dataset[img_id][0].permute(1,2,0)\n","    predictions = predicted[img_id]\n","\n","    fig, ax = plt.subplots()\n","    ax.imshow(img)\n","\n","    for (index, box) in enumerate(predictions['boxes']):\n","      #move data from GPU to CPU ready for visualisation\n","      cpu_box = box.cpu()\n","      predicted_label = predictions['labels'][index]\n","\n","      #class names: ['migrating_single', 'plastic', 'water_hyacinth']\n","      colour_mappings = ['y', 'white', 'orange']\n","      #draw bounding box\n","      try:\n","        bbox = patches.Rectangle((cpu_box[0], cpu_box[1]), cpu_box[2], cpu_box[3], linewidth=1, edgecolor=colour_mappings[predicted_label], facecolor='none')\n","      except IndexError:\n","        print(\"Bounding box does not have 4 coordinates\")\n","        print(cpu_box)\n","        raise\n","\n","      # Add the patch to the Axes\n","      ax.add_patch(bbox)\n","    plt.show()\n","\n","  except IndexError:\n","      print(f'Provided dataset is of length {len(test_dataset)} - image index {img_id} not within range')\n","      raise\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1358,"status":"ok","timestamp":1736519612365,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"bayugQaIFbNi"},"outputs":[],"source":["##THE CODE IN THIS CELL HAS BEEN MODIFIED FROM SOURCE:\n","#https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n","#the majority of this function is composed of PyTorch source code that has been slightly modififed so that both losses and predictions are returned during inference\n","#PyTorch source files (which part of code was sourced from) can be seen here\n","#https://github.com/pytorch/vision/blob/f40c8df02c197d1a9e194210e40dee0e6a6cb1c3/torchvision/models/detection/generalized_rcnn.py#L46\n","\n","#analyse using this reference\n","#https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn\n","from typing import Tuple, List, Dict, Optional\n","import torch\n","from torch import Tensor\n","from collections import OrderedDict\n","from torchvision.models.detection.roi_heads import fastrcnn_loss\n","from torchvision.models.detection.rpn import concat_box_prediction_layers\n","def eval_forward(model, images, targets):\n","    # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -\u003e Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","    \"\"\"\n","    Args:\n","        images (list[Tensor]): images to be processed\n","        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","    Returns:\n","        result (list[BoxList] or dict[Tensor]): the output from the model.\n","            It returns list[BoxList] contains additional fields\n","            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","    \"\"\"\n","    #Added (generalized_rcnn.py forward())\n","    model.eval()\n","\n","    #*******************************START:Sourced from generalized_rcnn.py forward()\n","    # lines 72-98 of generalized_rcnn.py\n","    original_image_sizes: List[Tuple[int, int]] = []\n","    for img in images:\n","        val = img.shape[-2:]\n","        assert len(val) == 2\n","        original_image_sizes.append((val[0], val[1]))\n","\n","    images, targets = model.transform(images, targets)\n","\n","    # Check for degenerate boxes\n","    # TODO: Move this to a function\n","    if targets is not None:\n","        for target_idx, target in enumerate(targets):\n","            boxes = target[\"boxes\"]\n","            degenerate_boxes = boxes[:, 2:] \u003c= boxes[:, :2]\n","            if degenerate_boxes.any():\n","                # print the first degenerate box\n","                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                degen_bb: List[float] = boxes[bb_idx].tolist()\n","                raise ValueError(\n","                    \"All bounding boxes should have positive height and width.\"\n","                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n","                )\n","\n","    features = model.backbone(images.tensors)\n","    if isinstance(features, torch.Tensor):\n","        features = OrderedDict([(\"0\", features)])\n","    #*******************************END:Sourced from generalized_rcnn.py forward()\n","\n","    # MODIFICATION: COMMENTED OUT THESE LINES OF CODE\n","    # AS RPN AND ROI DO NOT NEED TO BE TRAINING DURING INFERENCE ?\n","    # model.rpn.training=True\n","    # model.roi_heads.training=True\n","    # END OF MODIFICATION\n","\n","\n","    # ************Following section of code implements similar functionality to:\n","    # proposals, proposal_losses = model.rpn(images, features, targets)\n","    # from generalized_rcnn.py forward():\n","    # Code relating to the RPN (Region Proposal Network)\n","    #*******************************START:Sourced from rpn.py\n","    #lines 325-340 in rpn.py\n","    features_rpn = list(features.values())\n","    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n","    anchors = model.rpn.anchor_generator(images, features_rpn)\n","\n","    num_images = len(anchors)\n","    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n","    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n","    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n","    # Following 3 lines are comments from PyTorch source code\n","    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n","    # note that we detach the deltas because Faster R-CNN do not backprop through\n","    # the proposals\n","    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n","    proposals = proposals.view(num_images, -1, 4)\n","    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n","    proposal_losses = {}\n","\n","    #lines 342-351 in rpn.py\n","    assert targets is not None\n","    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n","    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n","    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n","        objectness, pred_bbox_deltas, labels, regression_targets\n","    )\n","    proposal_losses = {\n","        \"loss_objectness\": loss_objectness,\n","        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n","    }\n","    #*******************************END:Sourced from rpn.py\n","\n","    #image size required by ROI head\n","    image_shapes = images.image_sizes\n","\n","    # ************Following section of code implements similar functionality to:\n","    #detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n","    # from generalized_rcnn.py forward():\n","    # Code relating to the ROI heads (Region of Interest)\n","    #*******************************START:Sourced from roi_heads.py\n","    # lines 745-771 of roi_heads.py\n","    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n","\n","    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n","    box_features = model.roi_heads.box_head(box_features)\n","    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n","\n","    result: List[Dict[str, torch.Tensor]] = []\n","    detector_losses = {}\n","\n","    # The following 2 lines of code were original inside a 'if self.training' condition\n","    # they have been taken out of this if statment so that\n","    # losses can be returned when in inference mode (to get val loss)\n","    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n","    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n","\n","    # the code below was the corresponding 'else' statement (ie if not self.training)\n","    # it was taken out if the else statement\n","    # so that both predictions and losses could be returned when in inference mode\n","    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n","    num_images = len(boxes)\n","    for i in range(num_images):\n","        result.append(\n","            {\n","                \"boxes\": boxes[i],\n","                \"labels\": labels[i],\n","                \"scores\": scores[i],\n","            }\n","        )\n","    #*******************************END:Sourced from roi_heads.py\n","\n","    # rename results as detection so that it is compatible with the subsequent\n","    # line of code sourced from generalized_rcnn.py\n","    detections = result\n","    # Following line of code sourced from generalized_rcnn.py (line 100)\n","    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","    # MODIFICATION: REMOVED THE FOLLOWING 2 LINES OF CODE\n","    # model.rpn.training=False\n","    # model.roi_heads.training=False\n","\n","    #*******************************START:Sourced from generalized_rcnn.py forward()\n","    # lines 102-104 in generalized_rcnn.py\n","    losses = {}\n","    losses.update(detector_losses)\n","    losses.update(proposal_losses)\n","    #*******************************END:Sourced from generalized_rcnn.py forward()\n","\n","    #Added (generalized_rcnn.py forward())\n","    return losses, detections"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1736519612366,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"G6OoyOVXnZrF"},"outputs":[],"source":["#Function 'evaluate_loss' is modified from source:\n","#https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n","#A docstring has been added for the modified function\n","\n","def evaluate_loss(model, data_loader, device):\n","  #ballpark type of model\n","  #'torchvision.models.detection.faster_rcnn.FasterRCNN'\n","\n","    \"\"\"Calculates average validation loss across batches in a PyTorch DataLoader\n","\n","    Calls function eval_forward() to caluclate validation loss for each batch.\n","    Each batch validation loss is added to a running total which is divided by the\n","    number of batches to get average validation loss across batches.\n","\n","    Args:\n","        model: the PyTorch model which has been trained\n","\n","        data_loader: a Pytorch DataLoader\n","\n","        device: the device which the model is currently stored on (usually GPU)\n","\n","    Returns:\n","        The average validation loss across batches\n","\n","    Raises:\n","        AssertionError: If validation data is assigned a different device to model\n","    \"\"\"\n","\n","    #*************************************START OF MODIFICATION 1\n","    #model and validation data must be on same device in order to\n","    #calculate validation loss\n","    assert next(model.parameters()).device == device, \"Validation data and model must be assigned to the same device\"\n","    #*************************************START OF MODIFICATION 1\n","\n","    val_loss = 0\n","    with torch.no_grad():\n","      for batch in data_loader:\n","        #*************************************START OF MODIFICATIONS 2\n","        images = []\n","        targets = []\n","        #gather imgs and targets in separate arrays for each batch\n","        #ready to calculate validation loss for each batch\n","        for img, targets_dict in batch:\n","            images.append(img)\n","\n","            #following code only required if using MPS for Mac GPU\n","            # #format to tensor of dtype float 32 as supported by MPS\n","            # targets_dict['boxes'] = torch.tensor(targets_dict['boxes'])\n","            # targets_dict['boxes'] = targets_dict['boxes'].type(torch.float32)\n","            # #targets_dict['boxes'] =targets_dict['boxes'].to(device)\n","\n","            # targets_dict['labels'] = torch.tensor(targets_dict['labels'])\n","            # targets_dict['labels'] = targets_dict['labels'].type(torch.int64)\n","            # #targets_dict['labels'] =targets_dict['labels'].to(device)\n","\n","\n","            targets.append(targets_dict)\n","\n","        #move validation dataset to the same device that the model is on\n","        images = [ t.to(device) for t in images]\n","        targets = [ {'boxes':d['boxes'].to(device), 'labels':d['labels'].to(device)} for d in targets]\n","        #*************************************END OF MODIFICATIONS 2\n","        losses_dict, detections = eval_forward(model, images, targets)\n","        losses = sum(loss for loss in losses_dict.values())\n","        val_loss += losses\n","\n","    validation_loss = val_loss/ len(data_loader)\n","    return validation_loss"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736519612366,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"NDIbHuImB1ih"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_loss(epochs, losses):\n","  # cpu_epochs = Tensor.cpu(epochs)\n","  # cpu_losses = losses\n","  # plt.scatter([ t.cpu() for t in epochs], [ t.cpu() for t in losses])\n","  #plt.scatter(cpu_epochs, cpu_val_loss)\n","\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(\"Average loss across batches\")\n","  plt.plot(epochs, losses)\n","\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2LvEmShNuFJR"},"source":["## Faster R-CNN Implementation: \u003ca class=\"anchor\" name=\"fasterRCNN\"\u003e\u003c/a\u003e"]},{"cell_type":"markdown","metadata":{"id":"jqwB9h8IuqTs"},"source":["### Base line Implementation: \u003ca class=\"anchor\" name=\"fasterRCNN_baseline\"\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2219,"status":"ok","timestamp":1736519614581,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"OySnCC0JQueK","outputId":"367cf639-bb35-4edc-a83d-920e420e7dd9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736519614581,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"fRUC7DfhUSdO"},"outputs":[],"source":["root_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/train'\n","image_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/train/images'\n","label_folder_path = '/content/drive/MyDrive/UoL Final Project/Project Data/official project data/train/labels'"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736519614581,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"8c449728-d764-4f19-bd22-8e9ec845b222"},"outputs":[],"source":["import os\n","import torch\n","import torchvision\n","from torchvision import transforms\n","\n","from PIL import Image\n","import cv2\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1736519614581,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"ddbf01f3-2bb0-48c5-b673-e5ce40dcc72b"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","\n","class UAVImageDataset(Dataset):\n","    \"\"\"Overrides the PyTorch Dataset class\n","\n","      Defines how raw data should be read and transformed ready for model training\n","\n","      Attributes:\n","          data_dir_imgs: file string path to raw images\n","          data_dir_targets: file string path to raw target data\n","          imgs: the formatted images ready for training\n","          targets: the formatted targets ready for training\n","    \"\"\"\n","\n","    def __init__(self, data_dir_imgs: str, data_dir_targets: str, transform=None) -\u003e None:\n","        \"\"\"Formats the raw images and targets ready for model training\n","\n","          Images are converted to tensors as required by PyTorch for model training\n","          All tensor representations of images are stored in a List\n","          Targets (boxes and labels) are compiled into a dictionary for each image\n","          All targets are stored in a list\n","          An image and it's targets are stored at the same index in their respective arrays\n","\n","        Args:\n","          data_dir_imgs: a string representing the file path to the image folder of the dataset\n","          data_dir_targets: a string representing the file path to the 'targets' folder of the dataset\n","        \"\"\"\n","\n","        self.data_dir_imgs = data_dir_imgs\n","        self.data_dir_targets = data_dir_targets\n","        self.transforms = transforms\n","\n","        file_names = os.listdir(path=data_dir_imgs)\n","        number_of_images = len(file_names)\n","\n","        self.imgs = []\n","        self.targets = []\n","\n","        for i in range(number_of_images):\n","            img = data_dir_imgs + '/'+ file_names[i]\n","            img_read = cv2.imread(img)\n","            image_transform = transforms.ToTensor()  #cv library reads as a numpy array, needs to be a pytorch tensor to be compatible\n","            img = image_transform(img_read)\n","            self.imgs.append(img)\n","\n","            #remove .jpg or .png file extension and retrieve corresponding labels\n","            with open(data_dir_targets + '/' + file_names[i][:-4]+ '.txt','r') as f:\n","                file_lines = f.read().splitlines()\n","\n","                targets = {\n","                        \"boxes\": [],\n","                        \"labels\": []\n","                    }\n","\n","                #.txt label files with no annotations cause training error;\n","                if len(file_lines) ==0:\n","                    #follow 2 lines of code sourced from:\n","                    #https://discuss.pytorch.org/t/fasterrcnn-images-with-no-objects-present-cause-an-error/117974/13\n","                    #to address the issue of images with no ground truth bounding boxes\n","                    targets[\"boxes\"] = torch.as_tensor(np.array(np.zeros((0, 4)), dtype=float))\n","                    targets[\"labels\"] = torch.as_tensor(np.array([], dtype=int), dtype=torch.int64)\n","                else:\n","                    for file_line in file_lines:\n","                        indiv_items = file_line.split(\" \")\n","                        label, x1, y1, width, height = indiv_items[0], indiv_items[1], indiv_items[2], indiv_items[3], indiv_items[4]\n","                        label = int(label)\n","                        x1, width,  = float(x1)*img_read.shape[1], float(width)*img_read.shape[1],\n","                        y1, height = float(y1)*img_read.shape[0], float(height)*img_read.shape[0]\n","                        x2 = x1 + width\n","                        y2 = y1 + height\n","\n","                        #ADD TRY EXCEPT CLAUSE HERE\n","                        if x2 \u003c x1 or x1 == x2 or y2 \u003c y1 or y1 == y2:\n","                            print('Invalid Ground Truth Bounding Box')\n","                            print(x1, y1, x2, y2)\n","\n","                        #YOLO label format is x-centre, y-centre, width, height\n","                        targets['boxes'].append([x1-width/2, y1-height/2, x2-width/2, y2-height/2])\n","                        targets['labels'].append(label)\n","\n","                self.targets.append(targets)\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, idx):\n","        return self.imgs[idx], self.targets[idx]"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":7709,"status":"ok","timestamp":1736519622286,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"e74c039c-b215-4024-8f5b-88a6d12e61c3"},"outputs":[],"source":["dataset = UAVImageDataset(data_dir_imgs=image_folder_path, data_dir_targets=label_folder_path)"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1736519622286,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"5f4f4d3b-765d-4e32-a07f-bf9a6524111a"},"outputs":[],"source":["from torch.utils.data import random_split\n","\n","#ADD VALIDATION SET HERE - MULTIPLE VALIDATION SETS ?\n","train_dataset, test_dataset = random_split(dataset, [0.8, 0.2])"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1736519622287,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"GVze7O40DbBJ"},"outputs":[],"source":["#VISUALISE DATA AS SANITY CHECK"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1736519622287,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"8b7611c5-8f93-4d61-ba84-fb6d08e2d705"},"outputs":[],"source":["#set shuffle to True so that the training of the model is not dependent on the order of the data\n","#only have shuffle=False for validation/testing and inference\n","train_dataset = torch.utils.data.DataLoader(train_dataset, collate_fn=lambda x: x, batch_size=16, shuffle=True, pin_memory=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":459,"status":"ok","timestamp":1736519622744,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"4b0670af-ea6a-4b59-ae58-79767e4fad07"},"outputs":[],"source":["#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #- 0.1568 mAP50\n","#model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True) - 0.1080 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) - 0.0434 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True) - 0.0161 mAP50\n","\n","#needs separate fine tuning process for this model\n","#model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n","\n","\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","num_classes = 4  #+1 for background - there are 3 classes\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":1048,"status":"ok","timestamp":1736519623790,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"5019a818-edbb-4fa3-8b6a-d8214f94482c"},"outputs":[],"source":["#Must move model (and it's parameters) to the GPU before initialisating\n","#the optimizer - according to a video on the official PyTorch Youtube Channel\n","#https://www.youtube.com/watch?v=jF43_wj_DCQ\u0026t=213s\n","#otherwise the optimiser will update the wrong set of model parameters (on the CPU)\n","model.to(device)\n","optimizer = torch.optim.SGD(model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n","num_epochs = 15"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1736519623790,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"mKHXQJmZa2yE"},"outputs":[],"source":["import numpy as np\n","import time\n","\n","#as a utility function to ensure each iteration of Faster R-CNN is trained in the same way\n","#ie keep the training procedure as a control\n","\n","#CONTINUE WATCHING THIS VIDEO\n","#https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n","def train_model(num_epochs, train_dataset, model, optimizer):\n","  start_time = time.time()\n","  training_losses = []\n","  for epoch in range(num_epochs):\n","      epoch_loss = 0\n","      for batch in train_dataset:\n","          images = []\n","          targets = []\n","\n","          for img, targets_dict in batch:\n","              images.append(img)\n","\n","\n","              #target boxes expected to be of type Tensor\n","              targets_dict['boxes'] = torch.tensor(targets_dict['boxes'])\n","              targets_dict['labels'] = torch.tensor(targets_dict['labels'])\n","\n","              # Data type conversions required by Mac GPU 'MPS'\n","              #format to tensor of dtype float 32 as supported by MPS\n","              # targets_dict['boxes'] = targets_dict['boxes'].type(torch.float32)\n","              # targets_dict['labels'] = targets_dict['labels'].type(torch.int64)\n","\n","\n","\n","              targets.append(targets_dict)\n","\n","\n","          #move relevant data to GPU\n","          #otherwise error: RuntimeError: Mismatched Tensor types in NNPack convolutionOutput\n","          images = [ t.to(device) for t in images]\n","          targets = [ {'boxes':d['boxes'].to(device), 'labels':d['labels'].to(device)} for d in targets]\n","\n","          # print(model(images, targets))\n","          loss_dict = model(images, targets)\n","          #IS THIS APPROACH COMPARABLE TO LOSS CALCULATION IN YOLO?\n","          #IN ORDER TO MAKE COMPARISONS BETWEEN ARCHITECTURES\n","          loss = sum(v for v in loss_dict.values())\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","          print(loss)\n","          epoch_loss += loss.item()\n","          #TO DO: CALCULATE mAP FOR EACH EPOCH ?\n","          # is this avg calc correct ?\n","          # https://datascience.stackexchange.com/questions/92673/should-i-report-loss-on-the-last-batch-or-the-entire-training-dataset\n","      training_losses.append(epoch_loss/len(train_dataset))\n","\n","\n","  end_time = time.time()\n","  execution_time = end_time - start_time\n","  print(f'Training time: {execution_time} seconds')\n","  plot_loss([i+1 for i in range(num_epochs)], training_losses)\n","  # print(len(training_losses))\n","  # print([i for i in range(num_epochs)])\n","  # print(training_losses)\n","\n","          #memory management\n","          # images = [ t.cpu() for t in images]\n","          # targets = [ {'boxes':d['boxes'].cpu(), 'labels':d['labels'].cpu()} for d in targets]\n","          # del images\n","          # del targets"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":108374,"status":"ok","timestamp":1736519732162,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"dcfa1e1a-0cb0-4a51-974c-be78e5b9751e","outputId":"37233a72-265c-49a1-8049-770295fe9030"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-21-a734178ff619\u003e:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  targets_dict['boxes'] = torch.tensor(targets_dict['boxes'])\n","\u003cipython-input-21-a734178ff619\u003e:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  targets_dict['labels'] = torch.tensor(targets_dict['labels'])\n"]},{"name":"stdout","output_type":"stream","text":["tensor(3.5076, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(3.7921, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(2.2312, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.8527, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.7285, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.3854, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.1308, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.3506, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.0495, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8103, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.1533, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.4389, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.1306, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7694, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8276, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.9496, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.0910, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.2886, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.1000, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6521, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.1885, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.0185, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7328, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8551, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.1382, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5068, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.0540, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8402, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7456, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8080, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8137, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7759, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6281, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.0277, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7649, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.9031, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8190, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7567, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7505, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8289, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7278, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7812, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6488, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7532, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8193, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7559, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5890, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(1.0196, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5291, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7484, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5171, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6796, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5254, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6892, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7434, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6088, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7640, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7906, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7845, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6719, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6224, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7144, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5307, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7472, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6305, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7940, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6178, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4835, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5734, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5608, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6371, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.8414, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3941, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6811, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6553, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5087, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5387, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5155, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5165, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5001, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5984, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5888, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6914, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4107, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4843, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5146, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6824, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6041, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4677, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3490, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5155, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3722, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4408, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6200, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6221, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5828, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4736, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3403, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6995, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4962, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6351, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4790, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4298, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4656, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5591, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5311, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4941, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5568, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3223, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4372, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4339, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4051, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3366, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4987, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.7410, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4607, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3880, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5006, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4524, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5231, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4980, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4622, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5747, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3371, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6669, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.2921, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5661, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4530, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5026, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.2842, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5048, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5085, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4144, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5009, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.2883, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5229, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3969, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5986, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3363, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4316, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4150, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4992, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4238, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.6138, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.4788, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3340, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.3032, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5385, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.5503, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","tensor(0.2506, device='cuda:0', grad_fn=\u003cAddBackward0\u003e)\n","Training time: 108.79183745384216 seconds\n"]},{"data":{"image/png":"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\n","text/plain":["\u003cFigure size 640x480 with 1 Axes\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["train_model(num_epochs, train_dataset, model, optimizer)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1736519732162,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"a12f6ad7-d4ff-47cb-8166-b3621ceebc32","outputId":"fd3f8e36-9e9f-4479-9716-77a162d8f59e"},"outputs":[{"data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0-3): 4 x Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n","    )\n","  )\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["#evaluate on test set - only give image to model\n","model.eval()"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1736519732162,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"MLz74DVXeLgQ"},"outputs":[],"source":["test_dataset_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=lambda x: x, batch_size=16)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6226,"status":"ok","timestamp":1736519738384,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"469d0097-7a30-4e8d-9668-c7ca60dd655b","outputId":"87ce2e14-6af4-4396-b6e5-be2f368cb7e3"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-25-8e2a60fc8e83\u003e:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  targets_dict['boxes'] = torch.tensor(targets_dict['boxes'])\n","\u003cipython-input-25-8e2a60fc8e83\u003e:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  targets_dict['labels'] = torch.tensor(targets_dict['labels'])\n"]}],"source":["torch.cuda.empty_cache()\n","\n","images = []\n","targets = []\n","for batch in test_dataset_loader:\n","    for img, targets_dict in batch:\n","        #img = img.to(device)\n","        images.append(img)\n","\n","\n","        #format to tensor of dtype float 32 as supported by MPS\n","        targets_dict['boxes'] = torch.tensor(targets_dict['boxes'])\n","\n","        #targets_dict['boxes'] =targets_dict['boxes'].to(device)\n","\n","        targets_dict['labels'] = torch.tensor(targets_dict['labels'])\n","\n","        #targets_dict['labels'] =targets_dict['labels'].to(device)\n","\n","        # Data type conversions required by Mac GPU 'MPS'\n","        # targets_dict['boxes'] = targets_dict['boxes'].type(torch.float32)\n","        # targets_dict['labels'] = targets_dict['labels'].type(torch.int64)\n","\n","\n","        targets.append(targets_dict)\n","\n","\n","images = [ t.to(device) for t in images]\n","targets = [ {'boxes':d['boxes'].to(device), 'labels':d['labels'].to(device)} for d in targets]\n","\n","#reduce memory usage by not storing intermediate tensors needed to calculate gradients\n","#torch.no_grad reportedly speeds up computations\n","with torch.no_grad():\n","  predicted = model(images)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8945,"status":"ok","timestamp":1736519747325,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"DB-hZDZvibHs","outputId":"94ab9dd1-4aa1-4936-e6e2-bbfc4db99f59"},"outputs":[{"data":{"text/plain":["tensor(0.3531, device='cuda:0')"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["evaluate_loss(model, test_dataset_loader, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16de1e33-5bbe-4701-90a2-a8687c3ff60b","outputId":"70977e63-85da-4080-d2b7-3ce345dba5a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\n","Requirement already satisfied: numpy\u003e1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging\u003e17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n","Requirement already satisfied: lightning-utilities\u003e=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities\u003e=0.8.0-\u003etorchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities\u003e=0.8.0-\u003etorchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003etorchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003etorchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003etorchmetrics) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003etorchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=2.0.0-\u003etorchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1-\u003etorch\u003e=2.0.0-\u003etorchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=2.0.0-\u003etorchmetrics) (3.0.2)\n","Requirement already satisfied: faster-coco-eval in /usr/local/lib/python3.10/dist-packages (1.6.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faster-coco-eval) (1.26.4)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from faster-coco-eval) (5.24.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from faster-coco-eval) (2.2.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from faster-coco-eval) (11.1.0)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003efaster-coco-eval) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003efaster-coco-eval) (2024.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003efaster-coco-eval) (2024.2)\n","Requirement already satisfied: tenacity\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly-\u003efaster-coco-eval) (9.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly-\u003efaster-coco-eval) (24.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.8.2-\u003epandas-\u003efaster-coco-eval) (1.17.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (2.0.8)\n","Requirement already satisfied: matplotlib\u003e=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools) (1.26.4)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (1.3.1)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (4.55.3)\n","Requirement already satisfied: kiwisolver\u003e=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (1.4.8)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (24.2)\n","Requirement already satisfied: pillow\u003e=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (11.1.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (3.2.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib\u003e=2.1.0-\u003epycocotools) (2.8.2)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib\u003e=2.1.0-\u003epycocotools) (1.17.0)\n","{'map': tensor(0.3424), 'map_50': tensor(0.3784), 'map_75': tensor(0.3065), 'map_small': tensor(0.1176), 'map_medium': tensor(0.2584), 'map_large': tensor(0.3045), 'mar_1': tensor(0.0942), 'mar_10': tensor(0.3413), 'mar_100': tensor(0.4472), 'mar_small': tensor(0.1964), 'mar_medium': tensor(0.4741), 'mar_large': tensor(0.3778), 'map_per_class': tensor([0.0000, 0.2212, 0.8061]), 'mar_100_per_class': tensor([0.0000, 0.4167, 0.9250]), 'classes': tensor([0, 1, 2], dtype=torch.int32)}\n"]}],"source":["!pip install torchmetrics\n","!pip install faster-coco-eval\n","!pip install pycocotools\n","#https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html\n","\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5, 0.75], class_metrics=True)\n","\n","for pred in predicted:\n","    indices_to_keep = torchvision.ops.nms(pred['boxes'], pred['scores'], 0.8)\n","    #print(len(pred['boxes']))\n","    #print(indices_to_keep)\n","    pred = {\n","        'boxes': pred['boxes'][indices_to_keep],\n","        'labels': pred['labels'][indices_to_keep],\n","        'scores': pred['scores'][indices_to_keep],\n","    }\n","    #print(len(pred['boxes']))\n","\n","# Update metric with predictions and respective ground truth\n","metric.update(predicted, targets)\n","\n","# Compute the results\n","result = metric.compute()\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9Zc1BMitlp2"},"outputs":[],"source":["print('mAP50:')\n","print(result['map_50'])\n","\n","print('mAP75:')\n","print(result['map_75'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b786a51-d21b-4859-8ead-b567ca811e56"},"outputs":[],"source":["fig_, ax_ = metric.plot()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1s58It-OMuBQDvzBZR5EZBep2ACWceaNh"},"id":"FK5aLhnkct-e","outputId":"bb6b8489-49b2-45ec-a186-f7a7ff820113"},"outputs":[],"source":["for i in range(len(predicted)):\n","  visualisePredictions(i, test_dataset, predicted)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eYUvGLH5vdBW"},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (\u003cipython-input-31-e25be53e5bad\u003e, line 1)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"\u003cipython-input-31-e25be53e5bad\u003e\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    -stop execution-\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["-stop execution-"]},{"cell_type":"markdown","metadata":{"id":"fhJM1h1Mx40w"},"source":["### Custom Anchor sizes: \u003ca class=\"anchor\" name=\"custom_anchor\"\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-MCp3yqUzSHC"},"outputs":[],"source":["#**********************************NEW\n","\n","# frame of reference\n","# import torchvision\n","# from torchvision.models.detection import FasterRCNN\n","# from torchvision.models.detection.rpn import AnchorGenerator\n","\n","# # load a pre-trained model for classification and return\n","# # only the features\n","# backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# # ``FasterRCNN`` needs to know the number of\n","# # output channels in a backbone. For mobilenet_v2, it's 1280\n","# # so we need to add it here\n","# backbone.out_channels = 1280\n","\n","# # let's make the RPN generate 5 x 3 anchors per spatial\n","# # location, with 5 different sizes and 3 different aspect\n","# # ratios. We have a Tuple[Tuple[int]] because each feature\n","# # map could potentially have different sizes and\n","# # aspect ratios\n","# anchor_generator = AnchorGenerator(\n","#     sizes=((32, 64, 128, 256, 512),),\n","#     aspect_ratios=((0.5, 1.0, 2.0),)\n","# )\n","\n","# # let's define what are the feature maps that we will\n","# # use to perform the region of interest cropping, as well as\n","# # the size of the crop after rescaling.\n","# # if your backbone returns a Tensor, featmap_names is expected to\n","# # be [0]. More generally, the backbone should return an\n","# # ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n","# # feature maps to use.\n","# roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","#     featmap_names=['0'],\n","#     output_size=7,\n","#     sampling_ratio=2\n","# )\n","\n","# # put the pieces together inside a Faster-RCNN model\n","# model = FasterRCNN(\n","#     backbone,\n","#     num_classes=2,\n","#     rpn_anchor_generator=anchor_generator,\n","#     box_roi_pool=roi_pooler\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"woYKiPiV0QFX"},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","import torch.nn as nn\n","\n","#work around: AttributeError: 'ResNet' object has no attribute 'features'\n","# backbone = torchvision.models.resnet50(weights=\"DEFAULT\").features\n","# #https://discuss.pytorch.org/t/change-resnet50s-number-of-output-filters/146644\n","# backbone.out_channels = 2048\n","\n","# backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n","# # ``FasterRCNN`` needs to know the number of\n","# # output channels in a backbone. For mobilenet_v2, it's 1280\n","# # so we need to add it here\n","# backbone.out_channels = 1280\n","\n","resnet_net = torchvision.models.resnet50(pretrained=True)\n","modules = list(resnet_net.children())[:-2]\n","backbone = nn.Sequential(*modules)\n","backbone.out_channels = 2048\n","\n","#default\n","anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n","anchor_ars = ((0.5, 1.0, 2.0),) * len (anchor_sizes)\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=anchor_sizes,\n","    aspect_ratios=anchor_ars\n",")\n","# sizes=((16, 32, 64, 128, 256),),\n","#     aspect_ratios=((0.5, 1.0, 2.0),)\n","\n","\n","# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","#     output_size=7,\n","#     sampling_ratio=2)\n","\n","#keep default\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","\n","model = FasterRCNN(\n","    backbone,\n","    num_classes=4,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dq04dvm9x6NQ"},"outputs":[],"source":["optimizer = torch.optim.SGD(model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","#optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n","num_epochs = 20"]},{"cell_type":"markdown","metadata":{"id":"0edS2ekEu-wY"},"source":["## YOLOv11 Implementation: \u003ca class=\"anchor\" name=\"YOLOv11\"\u003e\u003c/a\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0yQpNRJXTtlx"},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3zNmNytbi7rz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XO2YxE89XYiG"},"outputs":[],"source":["from ultralytics import YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oWShE1KleZ0-"},"outputs":[],"source":["import numpy as np\n","import random\n","import torch\n","#THE SET_SEEDS FUNCTION IS SOURCED FROM:\n","#https://learnopencv.com/fine-tuning-faster-r-cnn/#aioseo-code-walkthrough\n","def set_seeds():\n","  # fix random seeds\n","  SEED_VALUE = 42\n","  random.seed(SEED_VALUE)\n","  np.random. seed(SEED_VALUE)\n","  torch.manual_seed(SEED_VALUE)\n","  if torch.cuda.is_available:\n","    torch.cuda.manual_seed(SEED_VALUE)\n","    torch.cuda.manual_seed_all(SEED_VALUE)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","set_seeds()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QIe0Q0dXXQYK"},"outputs":[],"source":["# Load a model\n","model = YOLO(\"yolo11n.pt\")\n","\n","# Train the model\n","train_results = model.train(\n","    data=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/YOLO_11_w_project_data_second/data.yaml\", # path to dataset YAML\n","    epochs=15, # number of training epochs\n","    imgsz=640, # training image size\n","    ) # device to run on, i.e. device=0 or device=0,1,2,3 or device=cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wWMFsh52XtXd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"59FMkDBXnmpd"},"source":["## Appendix A: \u003ca class=\"anchor\" name=\"AppenA\"\u003e\u003c/a\u003e\n","Attempts to override FasterRCNN forward() method"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vgv8c_3brIOb"},"outputs":[],"source":["##***************************************TEST\n","from torchvision.models.detection import FasterRCNN\n","\n","import warnings\n","\n","class ModifiedFasterRCNN(FasterRCNN):\n","  def forward(self, images, targets=None):\n","        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -\u003e Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","        \"\"\"\n","        Args:\n","            images (list[Tensor]): images to be processed\n","            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","\n","        Returns:\n","            result (list[BoxList] or dict[Tensor]): the output from the model.\n","                During training, it returns a dict[Tensor] which contains the losses.\n","                During testing, it returns list[BoxList] contains additional fields\n","                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","\n","        \"\"\"\n","        if self.training and targets is None:\n","            raise ValueError(\"In training mode, targets should be passed\")\n","        if self.training:\n","            assert targets is not None\n","            for target in targets:\n","                boxes = target[\"boxes\"]\n","                if isinstance(boxes, torch.Tensor):\n","                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n","                        raise ValueError(f\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\")\n","                else:\n","                    raise ValueError(f\"Expected target boxes to be of type Tensor, got {type(boxes)}.\")\n","\n","        original_image_sizes: List[Tuple[int, int]] = []\n","        for img in images:\n","            val = img.shape[-2:]\n","            assert len(val) == 2\n","            original_image_sizes.append((val[0], val[1]))\n","\n","        images, targets = self.transform(images, targets)\n","\n","        # Check for degenerate boxes\n","        # TODO: Move this to a function\n","        if targets is not None:\n","            for target_idx, target in enumerate(targets):\n","                boxes = target[\"boxes\"]\n","                degenerate_boxes = boxes[:, 2:] \u003c= boxes[:, :2]\n","                if degenerate_boxes.any():\n","                    # print the first degenerate box\n","                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                    degen_bb: List[float] = boxes[bb_idx].tolist()\n","                    raise ValueError(\n","                        \"All bounding boxes should have positive height and width.\"\n","                        f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n","                    )\n","\n","        features = self.backbone(images.tensors)\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","        proposals, proposal_losses = self.rpn(images, features, targets)\n","        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n","        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","        losses = {}\n","        losses.update(detector_losses)\n","        losses.update(proposal_losses)\n","\n","        if torch.jit.is_scripting():\n","            if not self._has_warned:\n","                warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n","                self._has_warned = True\n","            return losses, detections\n","        else:\n","            #MODIFIED THIS LINE\n","            if self.training:\n","              return losses\n","\n","            return detections\n","\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers, _mobilenet_extractor\n","from torchvision.ops import misc as misc_nn_ops\n","from torchvision.models.resnet import resnet50\n","from torchvision._internally_replaced_utils import load_state_dict_from_url\n","from torchvision.models.detection._utils import overwrite_eps\n","\n","model_urls = {\n","    \"fasterrcnn_resnet50_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\",\n","    \"fasterrcnn_mobilenet_v3_large_320_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\",\n","    \"fasterrcnn_mobilenet_v3_large_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\",\n","}\n","\n","def modified_fasterrcnn_resnet50_fpn(\n","    pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=None, **kwargs\n","):\n","    \"\"\"\n","    Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n","\n","    Reference: `\"Faster R-CNN: Towards Real-Time Object Detection with\n","    Region Proposal Networks\" \u003chttps://arxiv.org/abs/1506.01497\u003e`_.\n","\n","    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each\n","    image, and should be in ``0-1`` range. Different images can have different sizes.\n","\n","    The behavior of the model changes depending if it is in training or evaluation mode.\n","\n","    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n","    containing:\n","\n","        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 \u003c= x1 \u003c x2 \u003c= W`` and ``0 \u003c= y1 \u003c y2 \u003c= H``.\n","        - labels (``Int64Tensor[N]``): the class label for each ground-truth box\n","\n","    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression\n","    losses for both the RPN and the R-CNN.\n","\n","    During inference, the model requires only the input tensors, and returns the post-processed\n","    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as\n","    follows, where ``N`` is the number of detections:\n","\n","        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 \u003c= x1 \u003c x2 \u003c= W`` and ``0 \u003c= y1 \u003c y2 \u003c= H``.\n","        - labels (``Int64Tensor[N]``): the predicted labels for each detection\n","        - scores (``Tensor[N]``): the scores of each detection\n","\n","    For more details on the output, you may refer to :ref:`instance_seg_output`.\n","\n","    Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.\n","\n","    Example::\n","\n","        \u003e\u003e\u003e model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","        \u003e\u003e\u003e # For training\n","        \u003e\u003e\u003e images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n","        \u003e\u003e\u003e boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n","        \u003e\u003e\u003e labels = torch.randint(1, 91, (4, 11))\n","        \u003e\u003e\u003e images = list(image for image in images)\n","        \u003e\u003e\u003e targets = []\n","        \u003e\u003e\u003e for i in range(len(images)):\n","        \u003e\u003e\u003e     d = {}\n","        \u003e\u003e\u003e     d['boxes'] = boxes[i]\n","        \u003e\u003e\u003e     d['labels'] = labels[i]\n","        \u003e\u003e\u003e     targets.append(d)\n","        \u003e\u003e\u003e output = model(images, targets)\n","        \u003e\u003e\u003e # For inference\n","        \u003e\u003e\u003e model.eval()\n","        \u003e\u003e\u003e x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","        \u003e\u003e\u003e predictions = model(x)\n","        \u003e\u003e\u003e\n","        \u003e\u003e\u003e # optionally, if you want to export the model to ONNX:\n","        \u003e\u003e\u003e torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","        num_classes (int): number of output classes of the model (including the background)\n","        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n","        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n","            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is\n","            passed (the default) this value is set to 3.\n","    \"\"\"\n","    trainable_backbone_layers = _validate_trainable_layers(\n","        pretrained or pretrained_backbone, trainable_backbone_layers, 5, 3\n","    )\n","\n","    if pretrained:\n","        # no need to download the backbone if pretrained is set\n","        pretrained_backbone = False\n","\n","    backbone = resnet50(pretrained=pretrained_backbone, progress=progress, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","    backbone = _resnet_fpn_extractor(backbone, trainable_backbone_layers)\n","    #MODIFIED THE LINE BELOW\n","    # model = ModifiedFasterRCNN(backbone, num_classes, **kwargs)\n","    model = FasterRCNN(backbone, num_classes, **kwargs)\n","\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls[\"fasterrcnn_resnet50_fpn_coco\"], progress=progress)\n","        model.load_state_dict(state_dict)\n","        overwrite_eps(model, 0.0)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kHcdohwJsk8J"},"outputs":[],"source":["#TEST\n","model = modified_fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","num_classes = 4  #+1 for background - there are 3 classes\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOmisO2UpUE0oCLJvwHkqzy","gpuType":"A100","machine_shape":"hm","mount_file_id":"1JCtwPguP-eUqZEATKRbuAm_E21850bjI","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}