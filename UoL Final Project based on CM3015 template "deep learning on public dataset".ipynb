{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1743288931943,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"PnBVLmZo69V_"},"outputs":[],"source":["#set up github"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1743288934138,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"uLIWdd_hctyT","outputId":"8bf2c068-9de4-45de-d844-9e620f63cd81"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/Object-Detection-Neural-Networks\n","* \u001b[32mmain\u001b[m\n"]}],"source":["%cd '/content/Object-Detection-Neural-Networks'\n","!git branch"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":156,"status":"ok","timestamp":1743288934426,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"mkwGtsS-Xm7F","outputId":"0290f03d-cc2a-4286-884f-5d343a9e8bdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Branch 'hyperparameter_tuning' set up to track remote branch 'hyperparameter_tuning' from 'origin'.\n","Switched to a new branch 'hyperparameter_tuning'\n"]}],"source":["#CURRENT BRANCH\n","!git switch \"hyperparameter_tuning\""]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1743288982427,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"RLFEC068ecPq"},"outputs":[],"source":["git_push(\"Add more hyperparameters to search space\", \"hyperparameter_tuning\")"]},{"cell_type":"markdown","metadata":{"id":"2UDsTyiRurPL"},"source":["## Definition of Utility Functions: <a class=\"anchor\" name=\"util_func\"></a>"]},{"cell_type":"code","source":["!pip install sahi -q"],"metadata":{"id":"5hPzCIjUHPit","executionInfo":{"status":"ok","timestamp":1743283605479,"user_tz":0,"elapsed":8407,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"06447264-eaa3-4f58-e40f-8749c8abf3a4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1743288980097,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"gY2GwNcpyJD0"},"outputs":[],"source":["import shutil\n","\n","# !git pull\n","# !git status\n","\n","#check remotes\n","#!git remote -v\n","\n","\n","def git_push(message, branch):\n","  #move to git repo before using git commands\n","  %cd '/content/Object-Detection-Neural-Networks'\n","\n","  #copy current notebook to cloned git repo\n","  file_to_copy = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/UoL Final Project based on CM3015 template \"deep learning on public dataset\".ipynb'\n","  git_repo = '/content/Object-Detection-Neural-Networks'\n","\n","  #overwrite the file in the local cloned github repo\n","  shutil.copy(file_to_copy, git_repo)\n","\n","  !git add 'UoL Final Project based on CM3015 template \"deep learning on public dataset\".ipynb'\n","\n","  !git commit -a -m \"{message}\"\n","  !git push origin \"{branch}\"\n","\n","def publish_branch(branch):\n","  %cd '/content/Object-Detection-Neural-Networks'\n","  !git push -u origin \"{branch}\"\n","\n","def create_branch(branch):\n","  %cd '/content/Object-Detection-Neural-Networks'\n","  !git checkout -b \"{branch}\""]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3956,"status":"ok","timestamp":1743283609496,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"16yjq-CUqS3I","outputId":"cc4f884e-f158-4a2b-f9f3-772c58e91630"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import random\n","import numpy as np\n","import torch\n","\n","device = torch.device(\"cuda:0\")\n","print(device)\n","\n","#the google style guide suggest that a docstring is not necessary for the set_seeds() function\n","#as it does not meet any of the following critera\n","#A docstring is mandatory for every function that has one or more of the following properties:\n","  # - being part of the public API\n","  # - nontrivial size\n","  # - non-obvious logic\n","#https://github.com/google/styleguide/blob/gh-pages/pyguide.md#38-comments-and-docstrings\n","\n","#THE SET_SEEDS FUNCTION IS SOURCED FROM:\n","#https://learnopencv.com/fine-tuning-faster-r-cnn/#aioseo-code-walkthrough\n","def set_seeds():\n","  # fix random seeds\n","  SEED_VALUE = 42\n","  random.seed(SEED_VALUE)\n","  np.random. seed(SEED_VALUE)\n","  torch.manual_seed(SEED_VALUE)\n","  if torch.cuda.is_available:\n","    torch.cuda.manual_seed(SEED_VALUE)\n","    torch.cuda.manual_seed_all(SEED_VALUE)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch import nn\n","import numpy as np\n","import os\n","import cv2\n","from torchvision import transforms\n","import pandas as pd\n","import ast\n","\n","class UAVImageDataset(Dataset):\n","    \"\"\"Overrides the PyTorch Dataset class\n","\n","      Defines how raw data should be read and transformed ready for model training\n","\n","      Attributes:\n","          data_dir_imgs: file string path to raw images\n","          transforms: any Generalized R-CNN transforms to apply to the data\n","          all_imgs: list containing the filepath for each image\n","          all_targets: list of dictionaries - each dictionary contains a boxes and labels key and represents the\n","                        bounding boxes for the image at the corresponding index in all_imgs\n","    \"\"\"\n","\n","    def __init__(self,\n","                 data_dir_imgs: str,\n","                 df_path: str,\n","                 classes_start_at_zero: bool,\n","                 transform: nn.Module =None) -> None:\n","        \"\"\"Loads image file paths and targets into class (instance) attributes\n","\n","          Targets are loaded from .csv file (a pandas data frame that was converted to .csv)\n","          Img file paths and img targets are stored in separate arrays: 'self.all_imgs' and 'self.all_targets'\n","          Targets (boxes and labels) are compiled into a dictionary for each image\n","          An image file path and it's targets are stored at the same index in their respective arrays\n","\n","        Args:\n","          data_dir_imgs: a string representing the file path to the image folder of the dataset\n","          df_path: a string representing the file path to the .csv file (this .csv file should be a pandas dataframe that was converted to .csv)\n","          classes_start_at_zero: this faciliates support for both Faster R-CNN and YOLO ; as Faster R-CNN\n","                                  requires that classes start at 1, whilst YOLO requires classes at index 0.\n","          transforms: any img transforms to be applied\n","        \"\"\"\n","\n","        self.data_dir_imgs = data_dir_imgs\n","        self.transforms = transforms\n","\n","        file_names = os.listdir(path=data_dir_imgs)\n","        number_of_images = len(file_names)\n","\n","        self.all_imgs = []\n","        self.all_targets = []\n","\n","        df =  pd.read_csv(df_path)\n","        assert number_of_images == df.shape[0]\n","\n","        for index, row in df.iterrows():\n","            img = data_dir_imgs + '/'+ row['Img ID']\n","            self.all_imgs.append(img)\n","\n","            img_targets = {\n","                        \"boxes\": [],\n","                        \"labels\": []\n","                    }\n","\n","            if pd.isna(row['Boxes']) and pd.isna(row['Labels']):\n","                #follow 2 lines of code sourced from:\n","                #https://discuss.pytorch.org/t/fasterrcnn-images-with-no-objects-present-cause-an-error/117974/13\n","                #to address the issue of images with no ground truth bounding boxes\n","                img_targets[\"boxes\"] = torch.as_tensor(np.array(np.zeros((0, 4)), dtype=float))\n","                img_targets[\"labels\"] = torch.as_tensor(np.array([], dtype=int), dtype=torch.int64)\n","\n","            else:\n","              # append labels and boxes\n","                labels = row['Labels'][1:-1]\n","                labels = labels.split(\",\")\n","                try:\n","                  labels = [int(x) for x in labels]\n","                except:\n","                  print(labels)\n","\n","                if not classes_start_at_zero:\n","                  # Faster R-CNN requires that lebls start at 1\n","                  img_targets['labels'] = labels\n","                else:\n","                  # YOLO requires that labels start at 0\n","                  img_targets['labels'] = [x-1 for x in labels]\n","\n","\n","\n","                bboxes = ast.literal_eval(row['Boxes'])\n","                for bbox in bboxes:\n","                    x1, y1, width, height = bbox\n","                    img_targets['boxes'].append([x1, y1, width, height])\n","\n","            self.all_targets.append(img_targets)\n","\n","    def __len__(self):\n","        return len(self.all_imgs)\n","\n","    def __getitem__(self, idx: int) -> tuple:\n","      \"\"\"Returns image and target at the specified index\n","\n","          Images are loaded and then converted to tensors (as required by PyTorch for model training)\n","          Raw bbox coordinates are in YOLOv8 format (x,y,w,h) and are relative to img width and height\n","          __getitem__ converts these coordinates to x,y,x2,y2 ready for input to Faster R-CNN\n","\n","        Args:\n","          idx: the index of the img (and targets) to retrieve\n","\n","        Returns:\n","            tuple of the img and targets at the specified index\n","\n","        Raises: AssertionError if a bbox with negative height or width is found\n","        \"\"\"\n","      img = self.all_imgs[idx]\n","      img_read = cv2.imread(img)\n","      del img\n","      image_transform = transforms.ToTensor()  #cv library reads as a numpy array, needs to be a pytorch tensor to be compatible\n","      img = image_transform(img_read)\n","      del image_transform\n","\n","      # identify if the data point has been pre-processed\n","      # this accounts for validation set which is loaded multiple times\n","      if len(self.all_targets[idx]['labels'])!= 0:\n","        if isinstance(self.all_targets[idx]['boxes'][0][0], str):\n","\n","          list_tensors = []\n","          for i in range(len(self.all_targets[idx]['boxes'])):\n","            x1, y1, width, height = self.all_targets[idx]['boxes'][i]\n","            x1, width,  = float(x1)*img_read.shape[1], float(width)*img_read.shape[1],\n","            y1, height = float(y1)*img_read.shape[0], float(height)*img_read.shape[0]\n","            x2 = x1 + width\n","            y2 = y1 + height\n","\n","            # check for invalid bboxes\n","            if x2 < x1 or x1 == x2 or y2 < y1 or y1 == y2:\n","                raise AssertionError('Invalid Ground Truth Bounding Box')\n","                print(x1, y1, x2, y2)\n","\n","            list_tensors.append(torch.tensor([x1-width/2, y1-height/2, x2-width/2, y2-height/2]))\n","\n","          self.all_targets[idx]['boxes'] = torch.stack(list_tensors, dim=0)\n","          self.all_targets[idx]['labels'] = torch.as_tensor(self.all_targets[idx]['labels'])\n","\n","\n","      return img, self.all_targets[idx], self.all_imgs[idx]"],"metadata":{"id":"1eeORG-lXEUu","executionInfo":{"status":"ok","timestamp":1743283615761,"user_tz":0,"elapsed":6258,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import unittest\n","import torch\n","\n","class TestDatasetPreprocessing(unittest.TestCase):\n","    \"\"\"Checks that the integrity of the dataset is preserved after pre-processing\n","\n","        Checks that image data is preserved correctly, that no data is lost and that\n","        all data is in the correct format ready for model training\n","\n","        Attributes:\n","            dataset_classes_1_upwards: dataset that is formatted ready for Faster R-CNN (classes start at 1)\n","            dataset_classes_0_upwards: dataset that is formatted ready for YOLO (classes start at 0)\n","    \"\"\"\n","    def setUp(self):\n","      # run these unit tests using a much smaller dataset\n","      # otherwise loading large dataset will make unit tests uncessarily computationally expensive\n","      # as these tests are checking the integrity of pre-processing functions\n","      # they are not assessing the dataset itself\n","      self.dataset_classes_1_upwards = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","      self.dataset_classes_0_upwards = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train_pd.csv\",\n","                                classes_start_at_zero=True)\n","\n","    def test_img_conversion(self):\n","      \"\"\"Checks that colour channels, image height and width are preserved during conversion to torch.tensor\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","\n","        Raises:\n","            AssertionError if converted image does not match original image\n","        \"\"\"\n","      for i in range(self.dataset_classes_1_upwards.__len__()):\n","        img, targets, img_path = self.dataset_classes_1_upwards.__getitem__(i)\n","        num_colour_channels, img_height, img_width = img.size()\n","        # check colour channels have been preserved\n","        self.assertEqual(num_colour_channels, 3)\n","        # check image width and height have been preserved\n","        img_shape = cv2.imread(img_path).shape\n","        self.assertEqual(img_height, img_shape[0])\n","        self.assertEqual(img_width, img_shape[1])\n","\n","    def test_number_imgs_labels(self):\n","      \"\"\"Checks that no boxes, labels or images are lost during pre-processing\n","\n","         Checks that the number of images and bboxes tensors and labels tensors is the same\n","         Checks that the length of bboxes tensors and label tensors are the same\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","\n","        Raises:\n","            AssertionError if any image, bbox or label has been lost during pre-processing\n","        \"\"\"\n","      # check that there are the same number of img and target files\n","      self.assertEqual(len(self.dataset_classes_1_upwards.all_imgs), len(self.dataset_classes_1_upwards.all_targets))\n","      for i in range(self.dataset_classes_1_upwards.__len__()):\n","        img, targets, img_path = self.dataset_classes_1_upwards.__getitem__(i)\n","        boxes, labels = targets['boxes'], targets['labels']\n","        self.assertEqual(len(boxes), len(labels))\n","\n","    def test_label_img_type(self):\n","      \"\"\"Checks images, bboxes and labels are of type torch.Tensor as required for Faster R-CNN model training\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","\n","        Raises:\n","            AssertionError if data is not of correct type\n","        \"\"\"\n","      for i in range(self.dataset_classes_1_upwards.__len__()):\n","        img, targets, img_path = self.dataset_classes_1_upwards.__getitem__(i)\n","        self.assertIsInstance(img, torch.Tensor)\n","        boxes, labels = targets['boxes'], targets['labels']\n","        self.assertIsInstance(boxes, torch.Tensor)\n","        self.assertIsInstance(labels, torch.Tensor)\n","\n","\n","    def test_label_formatting(self):\n","      \"\"\"Checks that classes have been formatted correctly as specified by parameter 'classes_start_at_zero'\n","\n","        dataset class UAVImageDataset has a paramter classes_start_at_zero which allows a dataset to be formatted\n","        ready for either YOLO or Faster R-CNN ; YOLO requires that classes start at 0 whilst Faster R-CNN\n","        requires that classes start at 1 (0 is reserved for 'background')\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","\n","        Raises:\n","            AssertionError if classes do not start at the specified index\n","        \"\"\"\n","      for i in range(self.dataset_classes_1_upwards.__len__()):\n","        img, targets, img_path = self.dataset_classes_1_upwards.__getitem__(i)\n","        labels = targets['labels']\n","        self.assertNotIn(0, labels)\n","\n","      classes_start_at_zero = False\n","      for i in range(self.dataset_classes_1_upwards.__len__()):\n","        img, targets, img_path = self.dataset_classes_0_upwards.__getitem__(i)\n","        labels = targets['labels']\n","        if 0 in labels:\n","          classes_start_at_zero = True\n","      self.assertTrue(classes_start_at_zero)"],"metadata":{"id":"nMCL1GCLsAQw","executionInfo":{"status":"ok","timestamp":1743283615765,"user_tz":0,"elapsed":2,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import unittest\n","import torch\n","\n","class TestDataset(unittest.TestCase):\n","    \"\"\"Checks the quality of the datasets used to train, test and validate model performance\n","\n","        Checks for class imbalances that could lead to a biased model\n","\n","        Attributes:\n","            train_dataset: the pre-processed images, labels and bboxes used to train the model\n","            val_dataset: the pre-processed images, labels and bboxes used to validate model performance during training\n","            test_dataset: the pre-processed images, labels and bboxes used to test the model\n","    \"\"\"\n","    def setUp(self):\n","      self.train_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic - null images removed - original sizes/train/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic - null images removed - original sizes/train_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","      self.val_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic - null images removed - original sizes/valid/images\",\n","                                      df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic - null images removed - original sizes/val_pd.csv\",\n","                                    classes_start_at_zero=False)\n","\n","      self.test_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic - null images removed - original sizes/test/images\",\n","                                      df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic - null images removed - original sizes/test_pd.csv\",\n","                                    classes_start_at_zero=False)\n","\n","      self.class_labels = {'1': 'plastic', '2': 'water hyacinth'}\n","      # class imbalance thresholds sourced from developers.google\n","      # https://developers.google.com/machine-learning/crash-course/overfitting/imbalanced-datasets\n","      self.MILD_CLASS_IMBALANCE = {'lower_limit': 0.2, 'upper_limit':0.4}\n","      self.MODERATE_CLASS_IMBALANCE = {'lower_limit': 0.01, 'upper_limit':0.2}\n","      self.SEVERE_CLASS_IMBALANCE = 0.01\n","\n","      self.MAX_PERCENT_NULL_IMAGES = 0.1\n","\n","    def test_class_balance(self):\n","      \"\"\"Checks the quality of the dataset\n","\n","        Calculates class counts and determines if there is a mild, moderate or severe class imbalance.\n","        Checks each data subset (train/val/test) separately to help ensure good quality data at each stage in the pipeline\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","\n","        Raises:\n","            AssertionError with custom message specifiying whether a class imbalance is mild, moderate or severe\n","            (according to the thresholds established by developers.google.com)\n","        \"\"\"\n","      for idx, dataset_split in enumerate([self.train_dataset, self.val_dataset, self.test_dataset]):\n","        class_1_count = 0\n","        class_2_count = 0\n","        for i in range(dataset_split.__len__()):\n","          img, targets, img_path = dataset_split.__getitem__(i)\n","          bboxes, labels = targets['boxes'], targets['labels']\n","\n","          for label in labels:\n","            current_label = label.item() #use .item() to convert from tensor to int - for easy equality check\n","            if current_label == 1:\n","              class_1_count += 1\n","            else:\n","              class_2_count += 1\n","\n","        #calc percentage of data represented by minority class\n","        minority_class = min(class_1_count, class_2_count)\n","        total_num_labels = class_1_count + class_2_count\n","        percent_minority = minority_class/total_num_labels\n","        # check for imbalances\n","        self.assertGreater(percent_minority, self.SEVERE_CLASS_IMBALANCE, f\"Severe class imbalance detected in data split: {idx} [train,val,test]\")\n","        self.assertTrue(percent_minority > self.MODERATE_CLASS_IMBALANCE['lower_limit'] and percent_minority > self.MODERATE_CLASS_IMBALANCE['upper_limit'],\n","                        f\"Moderate class imabalce detected in data split: {idx} [train,val,test]\")\n","        self.assertTrue(percent_minority > self.MILD_CLASS_IMBALANCE['lower_limit'] and percent_minority > self.MILD_CLASS_IMBALANCE['upper_limit'],\n","                        f\"Mild class imabalce detected in data split: {idx} [train,val,test]\")\n","\n","    def test_proportion_null_images(self):\n","      \"\"\"Checks the number of null images as proportion of dataset\n","\n","        Checks percentages of null images is between 0-10% of dataset\n","        as recommended by Glenn Jocher, creator of YOLOv5, YOLOv8 and founder of Ultralytics\n","        https://github.com/ultralytics/ultralytics/issues/7981\n","\n","        Args:\n","            None\n","\n","        Returns:\n","            None\n","\n","        Raises:\n","            AssertionError with custom message if number of null images is greater than 10%\n","        \"\"\"\n","      for idx, dataset_split in enumerate([self.train_dataset, self.val_dataset, self.test_dataset]):\n","        null_img_count = 0\n","        for i in range(dataset_split.__len__()):\n","          img, targets, img_path = dataset_split.__getitem__(i)\n","          bboxes, labels = targets['boxes'], targets['labels']\n","          if len(labels) ==0:\n","            null_img_count += 1\n","\n","        percent_null_imgs = null_img_count / dataset_split.__len__()\n","        self.assertLess(percent_null_imgs, self.MAX_PERCENT_NULL_IMAGES, f\"More than 10% null images detected in data split: {idx} [train,val,test]\")"],"metadata":{"id":"z7p0QSFtEiip","executionInfo":{"status":"ok","timestamp":1743283615770,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# if __name__=='__main__':\n","#     unittest.main(argv=[''], verbosity=2, exit=False)"],"metadata":{"id":"Cum2ZXhysGJx","executionInfo":{"status":"ok","timestamp":1743283615788,"user_tz":0,"elapsed":2,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1743283615799,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"fqZ4zaUT7lG2"},"outputs":[],"source":["import torchvision\n","def format_batch(batch: torch.utils.data.dataset.Subset ,\n","                 model: torchvision.models.detection) -> dict:\n","  \"\"\"Formats batch for processing on GPU\n","\n","    Gathers all images and targets into separate lists ready to feed into the model\n","    Also moves all images and targets to the same device as the model ready for processing\n","\n","    Args:\n","        batch: a subset of the PyTorch dataset\n","\n","        model: used to determine which device the batch data should be sent to\n","\n","    Returns:\n","        dictionary of prepared batch data ; has 2 keys 'images and targets'\n","\n","    Raises:\n","        None\n","    \"\"\"\n","\n","  images = []\n","  device = next(model.parameters()).device\n","\n","  for img, targets_dict, img_file_path in batch:\n","      if img.get_device() != 0 and targets_dict[\"boxes\"].get_device() != 0:\n","        img = img.to(device)\n","        targets_dict[\"boxes\"] = targets_dict[\"boxes\"].to(device)\n","        targets_dict[\"labels\"] = targets_dict[\"labels\"].to(device)\n","        images.append(img)\n","\n","      # Data type conversions required by Mac GPU 'MPS'\n","      #format to tensor of dtype float 32 as supported by MPS\n","      # targets_dict['boxes'] = targets_dict['boxes'].type(torch.float32)\n","      # targets_dict['labels'] = targets_dict['labels'].type(torch.int64)\n","\n","  return {'images': images, 'targets': [item[1] for item in batch]}"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1743283615801,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"bJisk5MlApg6"},"outputs":[],"source":["# Function check_bboxes is sourced from the forward() method in PyTorch source code file generalized_rcnn.py\n","# https://github.com/pytorch/vision/blob/f40c8df02c197d1a9e194210e40dee0e6a6cb1c3/torchvision/models/detection/generalized_rcnn.py\n","# The source code for this code block had the following comment:\n","# \"TODO: Move this to a function\"\n","# So I have acted on this comment and added a docstring to the function and type hinting\n","def check_bboxes(targets:list) -> None:\n","  \"\"\"Checks that bounding boxes are of positive height and width\n","\n","    Args:\n","        targets: a list of dictionaries ; each dictionary has 2 keys 'boxes' and 'labels'\n","                (keys boxes and lables both have tensor values)\n","\n","    Returns:\n","        None\n","\n","    Raises:\n","        Value Error if bounidng boxes have negative height or width\n","    \"\"\"\n","  if targets is not None:\n","        for target_idx, target in enumerate(targets):\n","            boxes = target[\"boxes\"]\n","            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n","            if degenerate_boxes.any():\n","                # print the first degenerate box\n","                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                degen_bb: List[float] = boxes[bb_idx].tolist()\n","                raise ValueError(\n","                    \"All bounding boxes should have positive height and width.\"\n","                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n","                )\n","            else:\n","              return\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1743283615805,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"NDIbHuImB1ih"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_loss(epochs: list, training_losses: list, val_losses: list) -> None:\n","  \"\"\"Creates line graph of both training and validation loss after all epochs have been completed\n","\n","    Implemented using matplotlib plot()\n","\n","    Args:\n","        epochs: list containing values ranging from 1 to <num_epochs>,\n","\n","        training_losses: list containing training loss for each epoch,\n","\n","        val_losses: list containing validation loss for each epoch,\n","\n","    Returns:\n","        No value returned ; only plt.show()\n","\n","    Raises:\n","        AssertionError: Different number of training loss, validation loss and epochs\n","    \"\"\"\n","  assert len(epochs) == len(training_losses) == len(val_losses), \"Each argument must be the same length\"\n","\n","  fig, ax = plt.subplots()\n","  ax.set_xlabel(\"Epochs\")\n","  ax.set_ylabel(\"Epoch loss\")\n","  training_plot, = ax.plot(epochs, training_losses, label=\"Training Loss\")\n","  val_plot, = ax.plot(epochs, val_losses, label=\"Validation Loss\")\n","  ax.set_title(\"Training and Validation Loss\")\n","  handles, labels = ax.get_legend_handles_labels()\n","  ax.legend(handles, labels)\n","\n","  fig.show()"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1743283615831,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"IoSG5F2Iv9_m"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import torch\n","\n","def visualise_predictions(img_id: int,\n","                         test_dataset: torch.utils.data.dataset.Subset,\n","                         predicted: list) -> None:\n","\n","  \"\"\"Visualises model predictions (bounding boxes) on top of the corresponding image\n","\n","    Retrieves the original image from the test set and plots bouning boxes onto the image.\n","    Each classes is represented by a different colour bounding box.\n","\n","    Args:\n","        img_id: the number of the image within the batch,\n","\n","        test_dataset: Subset of PyTorch dataset ; each item in the array is a tuple consisting of\n","        a tensor representing the image and a dictionary containing 'boxes' and 'labels' keys,\n","\n","        precicted: array of model predictions for set of images ; the array consists of\n","        dictionaries each with 2 keys: boxes and labels\n","\n","    Returns:\n","        No value returned ; only plt.show()\n","\n","    Raises:\n","        IndexError: An error occurred accessing the image at the specified index.\n","    \"\"\"\n","\n","  try:\n","    img = test_dataset[img_id][0].permute(1,2,0)\n","    predictions = predicted[img_id]\n","\n","    fig, ax = plt.subplots()\n","    ax.imshow(img)\n","\n","    for (index, box) in enumerate(predictions['boxes']):\n","      #move data from GPU to CPU ready for visualisation\n","      cpu_box = box.cpu()\n","      predicted_label = predictions['labels'][index]\n","\n","      #class names: ['migrating_single', 'plastic', 'water_hyacinth']\n","      colour_mappings = ['y', 'white', 'orange']\n","      #draw bounding box\n","      try:\n","        bbox = patches.Rectangle((cpu_box[0], cpu_box[1]), cpu_box[2], cpu_box[3], linewidth=1, edgecolor=colour_mappings[predicted_label], facecolor='none')\n","      except IndexError:\n","        print(\"Bounding box does not have 4 coordinates\")\n","        print(cpu_box)\n","        raise\n","\n","      # Add the patch to the Axes\n","      ax.add_patch(bbox)\n","    plt.show()\n","\n","  except IndexError:\n","      print(f'Provided dataset is of length {len(test_dataset)} - image index {img_id} not within range')\n","      raise\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1743283615835,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"SDU7wbm1i-eN"},"outputs":[],"source":["def free_gpu_ram(dataloader: torch.utils.data.DataLoader) -> None:\n","  for batch in dataloader:\n","      for item in batch:\n","        del item\n","  torch.cuda.empty_cache()\n","  time.sleep(60) #allow time for GPU RAM to be cleared before training is 'finished'\n","  return"]},{"cell_type":"code","source":["#no deps flag required to not overwrite NVIDIA drivers\n","#overriding the driver pip packages causes model crash\n","!pip install torchmetrics==1.6.0 --no-deps\n","!pip install lightning-utilities==0.11.8 --no-deps"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_7d5wn3v1wP","executionInfo":{"status":"ok","timestamp":1743283619633,"user_tz":0,"elapsed":3798,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"ffd5529e-b5de-457c-e7e4-3401bbedddad"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics==1.6.0\n","  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n","Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchmetrics\n","Successfully installed torchmetrics-1.6.0\n","Collecting lightning-utilities==0.11.8\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Downloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Installing collected packages: lightning-utilities\n","Successfully installed lightning-utilities-0.11.8\n"]}]},{"cell_type":"code","source":["from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from sahi.prediction import visualize_object_predictions\n","import numpy\n","import sahi\n","from sahi import AutoDetectionModel\n","from typing import Optional\n","\n","def sahi_inference_eval(detection_model: AutoDetectionModel,\n","                        folder_path: str,\n","                        test_dataset_loader: torch.utils.data.dataloader,\n","                        img_export_path: str,\n","                        slice_h_w: Optional[tuple[int, int]] = None,\n","                        overlap_h_w: Optional[tuple[float, float]] = None,) -> dict[str, float]:\n","\n","  \"\"\"Executes Slicing Aided Hyper Inference (SAHI) for the given model\n","\n","    SAHI is implemented using the sahi pip library\n","    Computes and returns the mAP50 and mAP75 values\n","\n","  Args:\n","      detection_model: the model wrapped in a sahi AutoDetectionModel class\n","      folder_path: the path to the folder of images\n","      test_dataset_loader: the PyTorch dataset loader\n","      img_export_path: the path to the folder where resulting image predictions will be saved\n","      slice_h_w: the height and width of slices - this is optional as SAHI will call get_auto_slice_params if this param is not specified\n","      overlap_h_w: the height and width of overlap ratio - this is optional as SAHI will call get_auto_slice_params if this param is not specified\n","\n","  Returns:\n","      A dictionary of mAP values: mAP50, mAP50-95 and class wise mAP\n","\n","  Raises:\n","      None\n","  \"\"\"\n","\n","  img_file_names = os.listdir(path=folder_path)\n","\n","  map50_metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5], class_metrics=True, extended_summary=True)\n","  map50_95_metric = MeanAveragePrecision(iou_type=\"bbox\", class_metrics=True, extended_summary=True)\n","\n","  all_predictions = []\n","  all_ground_truths = []\n","  i=0\n","  for batch in test_dataset_loader:\n","    for img, targets, file_path in batch:\n","      # get sliced predictions\n","      if slice_h_w and overlap_h_w:\n","        img_predictions = get_sliced_prediction(\n","          detection_model=detection_model,\n","          image = file_path,\n","          slice_height=slice_h_w[0],\n","          slice_width=slice_h_w[1],\n","          overlap_height_ratio=overlap_h_w[0],\n","          overlap_width_ratio=overlap_h_w[1],\n","          verbose=0\n","        )\n","      else:\n","        img_predictions = get_sliced_prediction(\n","          detection_model=detection_model,\n","          image = file_path,\n","          verbose=0\n","        )\n","\n","      # visualise\n","      img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n","      img_converted = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","      numpy_img= numpy.asarray(img_converted)\n","      visualize_object_predictions(numpy_img,\n","                                   object_prediction_list=img_predictions.object_prediction_list,\n","                                   output_dir=img_export_path,\n","                                   file_name=f\"{i}\",\n","                                   hide_labels=1,\n","                                   export_format=\"jpg\"\n","                                   )\n","      i=i+1\n","\n","      # format predictions ready to calculate mAP metric\n","      img_predictions = img_predictions.to_coco_annotations()\n","      img_predictions = {\n","          \"boxes\": torch.tensor([pred['bbox'] for pred in img_predictions]),\n","          \"scores\": torch.tensor([pred['score'] for pred in img_predictions]),\n","          \"labels\": torch.tensor([pred['category_id'] for pred in img_predictions]) #needs to be int ?\n","      }\n","      # convert bboxes annotations to same format as ground truth\n","      for index, box in enumerate(img_predictions['boxes']):\n","        x,y,w,h = box\n","        img_predictions['boxes'][index][2] = x+w\n","        img_predictions['boxes'][index][3] = y+h\n","\n","      all_predictions.append(img_predictions)\n","\n","      # torchmetrics requires all bboxes in targets to be of type tensor\n","      for index, item in enumerate(targets['boxes']):\n","        targets['boxes'][index] = torch.tensor(list(map(lambda x: float(x), targets['boxes'][index])))\n","        targets['boxes'][index] = torch.tensor(targets['boxes'][index])\n","\n","      all_ground_truths.append({\"boxes\": targets['boxes'], \"labels\": targets['labels']})\n","\n","  # calculate mAP at two different thresholds\n","  map50_metric.update(all_predictions, all_ground_truths)\n","  map50_metric = map50_metric.compute()\n","\n","  map50_95_metric.update(all_predictions, all_ground_truths)\n","  map50_95_metric = map50_95_metric.compute()\n","\n","  return {'map50': map50_metric['map'], 'map50_95': map50_95_metric['map'], 'map_per_class': map50_95_metric['map_per_class']}"],"metadata":{"id":"WhC8GvkLNKo8","executionInfo":{"status":"ok","timestamp":1743283623078,"user_tz":0,"elapsed":3429,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["## Experiment with overrides to achieve custom loss"],"metadata":{"id":"SYNQUsZ43UhV"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def weighted_fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n","    # type: (Tensor, Tensor, List[Tensor], List[Tensor]) -> Tuple[Tensor, Tensor]\n","    \"\"\"\n","    Computes the loss for Faster R-CNN.\n","\n","    Args:\n","        class_logits (Tensor)\n","        box_regression (Tensor)\n","        labels (list[BoxList])\n","        regression_targets (Tensor)\n","\n","    Returns:\n","        classification_loss (Tensor)\n","        box_loss (Tensor)\n","    \"\"\"\n","\n","    labels = torch.cat(labels, dim=0)\n","    regression_targets = torch.cat(regression_targets, dim=0)\n","\n","    weights = torch.tensor([1. ,3. ,1. ]).cuda() #one weight for background class\n","    classification_loss = F.cross_entropy(class_logits, labels, weight=weights)\n","\n","    # get indices that correspond to the regression targets for\n","    # the corresponding ground truth labels, to be used with\n","    # advanced indexing\n","    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n","    labels_pos = labels[sampled_pos_inds_subset]\n","    N, num_classes = class_logits.shape\n","    box_regression = box_regression.reshape(N, box_regression.size(-1) // 4, 4)\n","\n","    box_loss = F.smooth_l1_loss(\n","        box_regression[sampled_pos_inds_subset, labels_pos],\n","        regression_targets[sampled_pos_inds_subset],\n","        beta=1 / 9,\n","        reduction=\"sum\",\n","    )\n","    box_loss = box_loss / labels.numel()\n","\n","    return classification_loss, box_loss"],"metadata":{"id":"4i07yGAtdsYh","executionInfo":{"status":"ok","timestamp":1743283623082,"user_tz":0,"elapsed":3,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from torchvision.models.detection.roi_heads import RoIHeads, maskrcnn_loss, maskrcnn_inference, keypointrcnn_loss, keypointrcnn_inference\n","\n","\n","class ModifiedROIHeads(RoIHeads):\n","  def __init__(\n","        self,\n","        box_roi_pool,\n","        box_head,\n","        box_predictor,\n","        # Faster R-CNN training\n","        fg_iou_thresh,\n","        bg_iou_thresh,\n","        batch_size_per_image,\n","        positive_fraction,\n","        bbox_reg_weights,\n","        # Faster R-CNN inference\n","        score_thresh,\n","        nms_thresh,\n","        detections_per_img,\n","        # Mask\n","        mask_roi_pool=None,\n","        mask_head=None,\n","        mask_predictor=None,\n","        keypoint_roi_pool=None,\n","        keypoint_head=None,\n","        keypoint_predictor=None,\n","    ):\n","        super().__init__(\n","        box_roi_pool,\n","        box_head,\n","        box_predictor,\n","        # Faster R-CNN training\n","        fg_iou_thresh,\n","        bg_iou_thresh,\n","        batch_size_per_image,\n","        positive_fraction,\n","        bbox_reg_weights,\n","        # Faster R-CNN inference\n","        score_thresh,\n","        nms_thresh,\n","        detections_per_img,\n","        # Mask\n","        mask_roi_pool,\n","        mask_head,\n","        mask_predictor,\n","        keypoint_roi_pool,\n","        keypoint_head,\n","        keypoint_predictor)\n","\n","  def forward(\n","        self,\n","        features,  # type: Dict[str, Tensor]\n","        proposals,  # type: List[Tensor]\n","        image_shapes,  # type: List[Tuple[int, int]]\n","        targets=None,  # type: Optional[List[Dict[str, Tensor]]]\n","    ):\n","        # type: (...) -> Tuple[List[Dict[str, Tensor]], Dict[str, Tensor]]\n","        \"\"\"\n","        Args:\n","            features (List[Tensor])\n","            proposals (List[Tensor[N, 4]])\n","            image_shapes (List[Tuple[H, W]])\n","            targets (List[Dict])\n","        \"\"\"\n","        if targets is not None:\n","            for t in targets:\n","                # TODO: https://github.com/pytorch/pytorch/issues/26731\n","                floating_point_types = (torch.float, torch.double, torch.half)\n","                if not t[\"boxes\"].dtype in floating_point_types:\n","                    raise TypeError(f\"target boxes must of float type, instead got {t['boxes'].dtype}\")\n","                if not t[\"labels\"].dtype == torch.int64:\n","                    raise TypeError(f\"target labels must of int64 type, instead got {t['labels'].dtype}\")\n","                if self.has_keypoint():\n","                    if not t[\"keypoints\"].dtype == torch.float32:\n","                        raise TypeError(f\"target keypoints must of float type, instead got {t['keypoints'].dtype}\")\n","\n","        if self.training:\n","            proposals, matched_idxs, labels, regression_targets = self.select_training_samples(proposals, targets)\n","        else:\n","            labels = None\n","            regression_targets = None\n","            matched_idxs = None\n","\n","        box_features = self.box_roi_pool(features, proposals, image_shapes)\n","        box_features = self.box_head(box_features)\n","        class_logits, box_regression = self.box_predictor(box_features)\n","\n","        result: List[Dict[str, torch.Tensor]] = []\n","        losses = {}\n","        if self.training:\n","            if labels is None:\n","                raise ValueError(\"labels cannot be None\")\n","            if regression_targets is None:\n","                raise ValueError(\"regression_targets cannot be None\")\n","                # USE MODIDIFIED LOSS FUNCTION HERE\n","            loss_classifier, loss_box_reg = weighted_fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n","            losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n","        else:\n","            boxes, scores, labels = self.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n","            num_images = len(boxes)\n","            for i in range(num_images):\n","                result.append(\n","                    {\n","                        \"boxes\": boxes[i],\n","                        \"labels\": labels[i],\n","                        \"scores\": scores[i],\n","                    }\n","                )\n","\n","        if self.has_mask():\n","            mask_proposals = [p[\"boxes\"] for p in result]\n","            if self.training:\n","                if matched_idxs is None:\n","                    raise ValueError(\"if in training, matched_idxs should not be None\")\n","\n","                # during training, only focus on positive boxes\n","                num_images = len(proposals)\n","                mask_proposals = []\n","                pos_matched_idxs = []\n","                for img_id in range(num_images):\n","                    pos = torch.where(labels[img_id] > 0)[0]\n","                    mask_proposals.append(proposals[img_id][pos])\n","                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n","            else:\n","                pos_matched_idxs = None\n","\n","            if self.mask_roi_pool is not None:\n","                mask_features = self.mask_roi_pool(features, mask_proposals, image_shapes)\n","                mask_features = self.mask_head(mask_features)\n","                mask_logits = self.mask_predictor(mask_features)\n","            else:\n","                raise Exception(\"Expected mask_roi_pool to be not None\")\n","\n","            loss_mask = {}\n","            if self.training:\n","                if targets is None or pos_matched_idxs is None or mask_logits is None:\n","                    raise ValueError(\"targets, pos_matched_idxs, mask_logits cannot be None when training\")\n","\n","                gt_masks = [t[\"masks\"] for t in targets]\n","                gt_labels = [t[\"labels\"] for t in targets]\n","                rcnn_loss_mask = maskrcnn_loss(mask_logits, mask_proposals, gt_masks, gt_labels, pos_matched_idxs)\n","                loss_mask = {\"loss_mask\": rcnn_loss_mask}\n","            else:\n","                labels = [r[\"labels\"] for r in result]\n","                masks_probs = maskrcnn_inference(mask_logits, labels)\n","                for mask_prob, r in zip(masks_probs, result):\n","                    r[\"masks\"] = mask_prob\n","\n","            losses.update(loss_mask)\n","\n","        # keep none checks in if conditional so torchscript will conditionally\n","        # compile each branch\n","        if (\n","            self.keypoint_roi_pool is not None\n","            and self.keypoint_head is not None\n","            and self.keypoint_predictor is not None\n","        ):\n","            keypoint_proposals = [p[\"boxes\"] for p in result]\n","            if self.training:\n","                # during training, only focus on positive boxes\n","                num_images = len(proposals)\n","                keypoint_proposals = []\n","                pos_matched_idxs = []\n","                if matched_idxs is None:\n","                    raise ValueError(\"if in trainning, matched_idxs should not be None\")\n","\n","                for img_id in range(num_images):\n","                    pos = torch.where(labels[img_id] > 0)[0]\n","                    keypoint_proposals.append(proposals[img_id][pos])\n","                    pos_matched_idxs.append(matched_idxs[img_id][pos])\n","            else:\n","                pos_matched_idxs = None\n","\n","            keypoint_features = self.keypoint_roi_pool(features, keypoint_proposals, image_shapes)\n","            keypoint_features = self.keypoint_head(keypoint_features)\n","            keypoint_logits = self.keypoint_predictor(keypoint_features)\n","\n","            loss_keypoint = {}\n","            if self.training:\n","                if targets is None or pos_matched_idxs is None:\n","                    raise ValueError(\"both targets and pos_matched_idxs should not be None when in training mode\")\n","\n","                gt_keypoints = [t[\"keypoints\"] for t in targets]\n","                rcnn_loss_keypoint = keypointrcnn_loss(\n","                    keypoint_logits, keypoint_proposals, gt_keypoints, pos_matched_idxs\n","                )\n","                loss_keypoint = {\"loss_keypoint\": rcnn_loss_keypoint}\n","            else:\n","                if keypoint_logits is None or keypoint_proposals is None:\n","                    raise ValueError(\n","                        \"both keypoint_logits and keypoint_proposals should not be None when not in training mode\"\n","                    )\n","\n","                keypoints_probs, kp_scores = keypointrcnn_inference(keypoint_logits, keypoint_proposals)\n","                for keypoint_prob, kps, r in zip(keypoints_probs, kp_scores, result):\n","                    r[\"keypoints\"] = keypoint_prob\n","                    r[\"keypoints_scores\"] = kps\n","            losses.update(loss_keypoint)\n","\n","        return result, losses\n"],"metadata":{"id":"qnYJgdcgdkmq","executionInfo":{"status":"ok","timestamp":1743283623083,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from torchvision.models.detection.generalized_rcnn import GeneralizedRCNN\n","from torchvision.ops import MultiScaleRoIAlign\n","from torchvision.models.detection.faster_rcnn import _default_anchorgen, TwoMLPHead\n","from torchvision.models.detection.rpn import RPNHead, RegionProposalNetwork\n","from torchvision.models.detection.transform import GeneralizedRCNNTransform\n","\n","class ModifiedFasterRCNN(GeneralizedRCNN):\n","    \"\"\"\n","    Implements Faster R-CNN.\n","\n","    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n","    image, and should be in 0-1 range. Different images can have different sizes.\n","\n","    The behavior of the model changes depending on if it is in training or evaluation mode.\n","\n","    During training, the model expects both the input tensors and targets (list of dictionary),\n","    containing:\n","        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (Int64Tensor[N]): the class label for each ground-truth box\n","\n","    The model returns a Dict[Tensor] during training, containing the classification and regression\n","    losses for both the RPN and the R-CNN.\n","\n","    During inference, the model requires only the input tensors, and returns the post-processed\n","    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n","    follows:\n","        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (Int64Tensor[N]): the predicted labels for each image\n","        - scores (Tensor[N]): the scores or each prediction\n","\n","    Args:\n","        backbone (nn.Module): the network used to compute the features for the model.\n","            It should contain an out_channels attribute, which indicates the number of output\n","            channels that each feature map has (and it should be the same for all feature maps).\n","            The backbone should return a single Tensor or and OrderedDict[Tensor].\n","        num_classes (int): number of output classes of the model (including the background).\n","            If box_predictor is specified, num_classes should be None.\n","        min_size (int): Images are rescaled before feeding them to the backbone:\n","            we attempt to preserve the aspect ratio and scale the shorter edge\n","            to ``min_size``. If the resulting longer edge exceeds ``max_size``,\n","            then downscale so that the longer edge does not exceed ``max_size``.\n","            This may result in the shorter edge beeing lower than ``min_size``.\n","        max_size (int): See ``min_size``.\n","        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n","            They are generally the mean values of the dataset on which the backbone has been trained\n","            on\n","        image_std (Tuple[float, float, float]): std values used for input normalization.\n","            They are generally the std values of the dataset on which the backbone has been trained on\n","        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n","            maps.\n","        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n","        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n","        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n","        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n","        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n","        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n","        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n","            considered as positive during training of the RPN.\n","        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n","            considered as negative during training of the RPN.\n","        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n","            for computing the loss\n","        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n","            of the RPN\n","        rpn_score_thresh (float): only return proposals with an objectness score greater than rpn_score_thresh\n","        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n","            the locations indicated by the bounding boxes\n","        box_head (nn.Module): module that takes the cropped feature maps as input\n","        box_predictor (nn.Module): module that takes the output of box_head and returns the\n","            classification logits and box regression deltas.\n","        box_score_thresh (float): during inference, only return proposals with a classification score\n","            greater than box_score_thresh\n","        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n","        box_detections_per_img (int): maximum number of detections per image, for all classes.\n","        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n","            considered as positive during training of the classification head\n","        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n","            considered as negative during training of the classification head\n","        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n","            classification head\n","        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n","            of the classification head\n","        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n","            bounding boxes\n","\n","    Example::\n","\n","        >>> import torch\n","        >>> import torchvision\n","        >>> from torchvision.models.detection import FasterRCNN\n","        >>> from torchvision.models.detection.rpn import AnchorGenerator\n","        >>> # load a pre-trained model for classification and return\n","        >>> # only the features\n","        >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n","        >>> # FasterRCNN needs to know the number of\n","        >>> # output channels in a backbone. For mobilenet_v2, it's 1280,\n","        >>> # so we need to add it here\n","        >>> backbone.out_channels = 1280\n","        >>>\n","        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n","        >>> # location, with 5 different sizes and 3 different aspect\n","        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n","        >>> # map could potentially have different sizes and\n","        >>> # aspect ratios\n","        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n","        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n","        >>>\n","        >>> # let's define what are the feature maps that we will\n","        >>> # use to perform the region of interest cropping, as well as\n","        >>> # the size of the crop after rescaling.\n","        >>> # if your backbone returns a Tensor, featmap_names is expected to\n","        >>> # be ['0']. More generally, the backbone should return an\n","        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n","        >>> # feature maps to use.\n","        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n","        >>>                                                 output_size=7,\n","        >>>                                                 sampling_ratio=2)\n","        >>>\n","        >>> # put the pieces together inside a FasterRCNN model\n","        >>> model = FasterRCNN(backbone,\n","        >>>                    num_classes=2,\n","        >>>                    rpn_anchor_generator=anchor_generator,\n","        >>>                    box_roi_pool=roi_pooler)\n","        >>> model.eval()\n","        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","        >>> predictions = model(x)\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        backbone,\n","        num_classes=None,\n","        # transform parameters\n","        min_size=800,\n","        max_size=1333,\n","        image_mean=None,\n","        image_std=None,\n","        # RPN parameters\n","        rpn_anchor_generator=None,\n","        rpn_head=None,\n","        rpn_pre_nms_top_n_train=2000,\n","        rpn_pre_nms_top_n_test=1000,\n","        rpn_post_nms_top_n_train=2000,\n","        rpn_post_nms_top_n_test=1000,\n","        rpn_nms_thresh=0.7,\n","        rpn_fg_iou_thresh=0.7,\n","        rpn_bg_iou_thresh=0.3,\n","        rpn_batch_size_per_image=256,\n","        rpn_positive_fraction=0.5,\n","        rpn_score_thresh=0.0,\n","        # Box parameters\n","        box_roi_pool=None,\n","        box_head=None,\n","        box_predictor=None,\n","        box_score_thresh=0.05,\n","        box_nms_thresh=0.5,\n","        box_detections_per_img=100,\n","        box_fg_iou_thresh=0.5,\n","        box_bg_iou_thresh=0.5,\n","        box_batch_size_per_image=512,\n","        box_positive_fraction=0.25,\n","        bbox_reg_weights=None,\n","        **kwargs,\n","    ):\n","\n","        if not hasattr(backbone, \"out_channels\"):\n","            raise ValueError(\n","                \"backbone should contain an attribute out_channels \"\n","                \"specifying the number of output channels (assumed to be the \"\n","                \"same for all the levels)\"\n","            )\n","\n","        if not isinstance(rpn_anchor_generator, (AnchorGenerator, type(None))):\n","            raise TypeError(\n","                f\"rpn_anchor_generator should be of type AnchorGenerator or None instead of {type(rpn_anchor_generator)}\"\n","            )\n","        if not isinstance(box_roi_pool, (MultiScaleRoIAlign, type(None))):\n","            raise TypeError(\n","                f\"box_roi_pool should be of type MultiScaleRoIAlign or None instead of {type(box_roi_pool)}\"\n","            )\n","\n","        if num_classes is not None:\n","            if box_predictor is not None:\n","                raise ValueError(\"num_classes should be None when box_predictor is specified\")\n","        else:\n","            if box_predictor is None:\n","                raise ValueError(\"num_classes should not be None when box_predictor is not specified\")\n","\n","        out_channels = backbone.out_channels\n","\n","        if rpn_anchor_generator is None:\n","            rpn_anchor_generator = _default_anchorgen()\n","        if rpn_head is None:\n","            rpn_head = RPNHead(out_channels, rpn_anchor_generator.num_anchors_per_location()[0])\n","\n","        rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n","        rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n","\n","        # MODIFICATION HERE\n","        rpn = RegionProposalNetwork(\n","            rpn_anchor_generator,\n","            rpn_head,\n","            rpn_fg_iou_thresh,\n","            rpn_bg_iou_thresh,\n","            rpn_batch_size_per_image,\n","            rpn_positive_fraction,\n","            rpn_pre_nms_top_n,\n","            rpn_post_nms_top_n,\n","            rpn_nms_thresh,\n","            score_thresh=rpn_score_thresh,\n","        )\n","\n","        if box_roi_pool is None:\n","            box_roi_pool = MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"], output_size=7, sampling_ratio=2)\n","\n","        if box_head is None:\n","            resolution = box_roi_pool.output_size[0]\n","            representation_size = 1024\n","            box_head = TwoMLPHead(out_channels * resolution**2, representation_size)\n","\n","        if box_predictor is None:\n","            representation_size = 1024\n","            box_predictor = FastRCNNPredictor(representation_size, num_classes)\n","\n","        # MODIFICATION HERE\n","        roi_heads = ModifiedROIHeads(\n","            # Box\n","            box_roi_pool,\n","            box_head,\n","            box_predictor,\n","            box_fg_iou_thresh,\n","            box_bg_iou_thresh,\n","            box_batch_size_per_image,\n","            box_positive_fraction,\n","            bbox_reg_weights,\n","            box_score_thresh,\n","            box_nms_thresh,\n","            box_detections_per_img,\n","        )\n","\n","        if image_mean is None:\n","            image_mean = [0.485, 0.456, 0.406]\n","        if image_std is None:\n","            image_std = [0.229, 0.224, 0.225]\n","        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std, **kwargs)\n","\n","        super().__init__(backbone, rpn, roi_heads, transform)"],"metadata":{"id":"6UiGb1no3USl","executionInfo":{"status":"ok","timestamp":1743283623086,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfF-WOVV6p9j"},"source":["## Definition of Faster R-CNN Wrapper Class: <a class=\"anchor\" name=\"faster_rcnn_class\"></a>"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1743283623113,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"rQOGEdDdADVe"},"outputs":[],"source":["# !pip install torchmetrics==1.6.1\n","# !pip install lightning-utilities==0.11.9\n","\n","#https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","import time\n","from torchvision.models.detection import FasterRCNN\n","from typing import Tuple, List, Dict, Optional\n","import torch\n","from torch import Tensor\n","from torch.utils.data import Dataset, DataLoader\n","from collections import OrderedDict\n","from torchvision.models.detection.roi_heads import fastrcnn_loss\n","from torchvision.models.detection.rpn import concat_box_prediction_layers\n","import numpy as np\n","import datetime\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","class FasterRCNNWrapper():\n","  \"\"\"A wrapper class that adds to the functionality of the PyTorch FasterRCNN class\n","\n","    The FasterRCNN model initialised by PyTorch FasterRCNN is stored in the 'model' attribute\n","    The methods of this class connect to the 'model' attribute to train and test the model\n","\n","    Attributes:\n","        model: the object initialiased by PyTorch FasterRCNN init()\n","    \"\"\"\n","  # Call the init method of FasterRCNN within the init() of this subclass\n","  # so that the model can be saved as an instance attribute\n","  # pass the default values of parameters to FasterRCNN init()\n","  # in case class is instantiate without specifying all values\n","  def __init__(self,\n","              backbone,\n","              num_classes=None,\n","              # transform parameters\n","              min_size=800,\n","              max_size=1333,\n","              image_mean=None,\n","              image_std=None,\n","              # RPN parameters\n","              rpn_anchor_generator=None,\n","              rpn_head=None,\n","              rpn_pre_nms_top_n_train=2000,\n","              rpn_pre_nms_top_n_test=1000,\n","              rpn_post_nms_top_n_train=2000,\n","              rpn_post_nms_top_n_test=1000,\n","              rpn_nms_thresh=0.7,\n","              rpn_fg_iou_thresh=0.7,\n","              rpn_bg_iou_thresh=0.3,\n","              rpn_batch_size_per_image=256,\n","              rpn_positive_fraction=0.5,\n","              rpn_score_thresh=0.0,\n","              # Box parameters\n","              box_roi_pool=None,\n","              box_head=None,\n","              box_predictor=None,\n","              box_score_thresh=0.05,\n","              box_nms_thresh=0.5,\n","              box_detections_per_img=100,\n","              box_fg_iou_thresh=0.5,\n","              box_bg_iou_thresh=0.5,\n","              box_batch_size_per_image=512,\n","              box_positive_fraction=0.25,\n","              bbox_reg_weights=None,\n","              **kwargs,) -> None:\n","    \"\"\"Initialises the 'model' attribute to be a FasterRCNN model\n","\n","    The 'model' attribute it utilised by all methods in this class\n","\n","    Args: All parameters required by the init() of PyTorch FasterRCNN\n","\n","    Returns: None\n","\n","    Raies: AssertionError is model is going to be initialised as NoneType\n","    \"\"\"\n","\n","    self.model = FasterRCNN( #ModifiedFasterRCNN(\n","        backbone,\n","        num_classes,\n","        # transform parameters\n","        min_size,\n","        max_size,\n","        image_mean,\n","        image_std,\n","        # RPN parameters\n","        rpn_anchor_generator,\n","        rpn_head,\n","        rpn_pre_nms_top_n_train,\n","        rpn_pre_nms_top_n_test,\n","        rpn_post_nms_top_n_train,\n","        rpn_post_nms_top_n_test,\n","        rpn_nms_thresh,\n","        rpn_fg_iou_thresh,\n","        rpn_bg_iou_thresh,\n","        rpn_batch_size_per_image,\n","        rpn_positive_fraction,\n","        rpn_score_thresh,\n","        # Box parameters\n","        box_roi_pool,\n","        box_head,\n","        box_predictor,\n","        box_score_thresh,\n","        box_nms_thresh,\n","        box_detections_per_img,\n","        box_fg_iou_thresh,\n","        box_bg_iou_thresh,\n","        box_batch_size_per_image,\n","        box_positive_fraction,\n","        bbox_reg_weights,\n","        **kwargs,)\n","\n","    assert self.model is not None, \"Model cannot be initialised as NoneType\"\n","\n","  def override_prediction_heads(self, num_classes: int) -> None:\n","    # update prediction head so that it's outputs are aligned with number of classes in dataset\n","    in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n","    self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","  def load_pretrained_weights(self, weights, weightsClass):\n","    # the .verify() method is inherited from the WeightsEnum class\n","    # verify checks that are the weights are not None and are the correct type\n","    weights = weightsClass.verify(weights)\n","    # load the weights into the model\n","    self.model.load_state_dict(weights.get_state_dict(progress=True, check_hash=True))\n","    # the following function call is required to address\n","    # the BC-breaking change introduced by the bug-fix at pytorch/vision#2933\n","    # overwrite_eps is only required when loading pre-trained weights\n","    overwrite_eps(self.model, 0.0)\n","\n","  def train(self, num_epochs: int,\n","                  optimizer: torch.optim,\n","                  train_dataset: torch.utils.data.DataLoader,\n","                  val_dataset: Optional[torch.utils.data.DataLoader]= None,\n","                  is_console_logging: bool = True,\n","                  calc_running_loss: Optional[bool] = True) -> Dict[str, list]:\n","    \"\"\"Trains the model for the specified number of epochs\n","\n","      Calls function train_batch to train the model on each batch in an epoch.\n","      Validation loss is calculated at the end of each epoch\n","      Training time is recorded for the purposes of evaluation.\n","      Epoch loss is calculates as the average training loss across batches\n","\n","      Args:\n","          num_epochs: the number of epochs to train for,\n","\n","          train_dataset: the PyTorch DataLoader for the training set,\n","\n","          val_dataset: the PyTorch DataLoader for the validation set, an optional parameter so that\n","                        this function can be used both in cross validation and hold out contexts\n","\n","          optimizer : any optimizer imported from torch.optim\n","\n","          is_console_logging: controls if print statements are sent to the console\n","\n","          calc_running_loss: a memory management variable - set to False to minimise RAM usage (ie tensor dependencies)\n","\n","      Returns:\n","          A dictionary containing 2 lists - one for training loss and another validation loss\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    # enforce correct model mode as model will be set to .eval mode to calculate\n","    # validation loss during training\n","    self.model.train()\n","    # track losses across each epoch ready to produce graph\n","    if calc_running_loss:\n","      training_losses = []\n","      val_losses = []\n","\n","    for epoch in range(num_epochs):\n","        if is_console_logging:\n","          # simple progress monitor\n","          print(f'Epoch {epoch} of {num_epochs}')\n","        # Keep running total of batch losses ready to take average across epoch\n","        if calc_running_loss:\n","          epoch_loss_train = 0\n","\n","        for batch in train_dataset:\n","          batch_dict = format_batch(batch, self.model)\n","          # memory management\n","          for item in batch:\n","            del item\n","          time.sleep(30) #allow time for GPU RAM to be cleared before proceeding\n","\n","          batch_training_loss = self.__train_batch(batch_dict, optimizer)\n","          del batch_dict['images'], batch_dict['targets'] # memory management\n","          time.sleep(30) #allow time for GPU RAM to be cleared before proceeding\n","\n","          if calc_running_loss:\n","            epoch_loss_train += batch_training_loss\n","\n","          del batch_training_loss # memory management\n","          torch.cuda.empty_cache()\n","          print('batch complete')\n","\n","        if calc_running_loss:\n","          # divide total epoch loss by the total number of batches per epoch\n","          epoch_training_loss = epoch_loss_train/len(train_dataset)\n","          del epoch_loss_train\n","          training_losses.append(epoch_training_loss)\n","\n","\n","        if is_console_logging and calc_running_loss:\n","          print(f'Epoch Training loss: {epoch_training_loss}')\n","        del epoch_training_loss\n","\n","        if val_dataset is not None:\n","          val_loss = self.__calc_val_loss(val_dataset_loader)\n","          val_losses.append(val_loss)\n","\n","          if is_console_logging:\n","            print(f'Epoch Validation loss: {val_loss}')\n","          del val_loss\n","\n","        # free_gpu_ram ?\n","\n","    if is_console_logging and calc_running_loss and val_dataset is not None:\n","      plot_loss([i+1 for i in range(num_epochs)], training_losses, val_losses)\n","\n","    del training_losses, val_losses\n","    if val_dataset is not None:\n","      # this frees up the GPU ready to assign training data\n","      return {'training_losses': training_losses, 'validation_losses': val_losses}\n","    else:\n","      return {'training_losses': training_losses}\n","\n","  def __train_batch(self, prepped_batch: torch.utils.data.dataset.Subset,\n","                  optimizer: torch.optim) -> float:\n","    \"\"\"Trains the model on a single batch and returns the batch loss\n","\n","      Calls function format_batch() before training the model on the batch data.\n","      Total loss is calculated across each of the 4 losses returned by Faster R-CNN\n","      A backward pass is then completed\n","\n","      Args:\n","          train_batch: a subset of the PyTorch dataset\n","\n","          optimizer : any optimizer imported from torch.optim\n","\n","      Returns:\n","          batch loss\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    # Calculate training loss\n","    images, targets = prepped_batch['images'], prepped_batch['targets']\n","\n","    # use mixed precision to address CUDA out of memory error\n","    with torch.autocast(device_type=\"cuda\"):\n","      loss_dict = self.model(images, targets)\n","      training_loss = sum(v for v in loss_dict.values())\n","      del loss_dict\n","    # loss_dict = self.model(images, targets)\n","    # training_loss = sum(v for v in loss_dict.values())\n","    del images, targets, prepped_batch\n","\n","    # backward pass\n","    optimizer.zero_grad()\n","    training_loss.backward()\n","\n","    optimizer.step()\n","\n","    # detach from computational graph to free up GPU memory\n","    # use .item() to convert loss to a Python Number ready for plotting\n","    return training_loss.detach().item()\n","\n","  ##THE CODE IN FUNCTION EVAL_FORWARD HAS BEEN MODIFIED FROM SOURCE:\n","  #https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n","  #the majority of this function is composed of PyTorch source code that has been slightly modififed so that both losses and predictions are returned during inference\n","  #PyTorch source files (which part of code was sourced from) can be seen here\n","  #https://github.com/pytorch/vision/blob/f40c8df02c197d1a9e194210e40dee0e6a6cb1c3/torchvision/models/detection/generalized_rcnn.py#L46\n","\n","  #analyse using this reference\n","  #https://stackoverflow.com/questions/60339336/validation-loss-for-pytorch-faster-rcnn\n","\n","  def __eval_forward(self, images, targets):\n","      # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","      \"\"\"\n","      Args:\n","          images (list[Tensor]): images to be processed\n","          targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","      Returns:\n","          result (list[BoxList] or dict[Tensor]): the output from the model.\n","              It returns list[BoxList] contains additional fields\n","              like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","      \"\"\"\n","      #Added (generalized_rcnn.py forward())\n","      self.model.eval()\n","\n","      #*******************************START:Sourced from generalized_rcnn.py forward()\n","      # lines 72-98 of generalized_rcnn.py\n","      original_image_sizes: List[Tuple[int, int]] = []\n","      for img in images:\n","          val = img.shape[-2:]\n","          assert len(val) == 2\n","          original_image_sizes.append((val[0], val[1]))\n","\n","      images, targets = self.model.transform(images, targets)\n","\n","      # MODIFICATION: Refactored a block of code into the function check_bboxes\n","      # Check for degenerate boxes\n","      check_bboxes(targets)\n","\n","      features = self.model.backbone(images.tensors)\n","      if isinstance(features, torch.Tensor):\n","          features = OrderedDict([(\"0\", features)])\n","      #*******************************END:Sourced from generalized_rcnn.py forward()\n","\n","      # MODIFICATION: COMMENTED OUT THESE LINES OF CODE\n","      # AS RPN AND ROI DO NOT NEED TO BE TRAINING DURING INFERENCE ?\n","      # model.rpn.training=True\n","      # model.roi_heads.training=True\n","      # END OF MODIFICATION\n","\n","\n","      # ************Following section of code implements similar functionality to:\n","      # proposals, proposal_losses = model.rpn(images, features, targets)\n","      # from generalized_rcnn.py forward():\n","      # Code relating to the RPN (Region Proposal Network)\n","      #*******************************START:Sourced from rpn.py\n","      #lines 325-340 in rpn.py\n","      features_rpn = list(features.values())\n","      objectness, pred_bbox_deltas = self.model.rpn.head(features_rpn)\n","      anchors = self.model.rpn.anchor_generator(images, features_rpn)\n","\n","      num_images = len(anchors)\n","      num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n","      num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n","      objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n","      # Following 3 lines are comments from PyTorch source code\n","      # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n","      # note that we detach the deltas because Faster R-CNN do not backprop through\n","      # the proposals\n","      proposals = self.model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n","      proposals = proposals.view(num_images, -1, 4)\n","      proposals, scores = self.model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n","      proposal_losses = {}\n","\n","      #lines 342-351 in rpn.py\n","      assert targets is not None\n","      labels, matched_gt_boxes = self.model.rpn.assign_targets_to_anchors(anchors, targets)\n","      regression_targets = self.model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n","      loss_objectness, loss_rpn_box_reg = self.model.rpn.compute_loss(\n","          objectness, pred_bbox_deltas, labels, regression_targets\n","      )\n","      proposal_losses = {\n","          \"loss_objectness\": loss_objectness,\n","          \"loss_rpn_box_reg\": loss_rpn_box_reg,\n","      }\n","      #*******************************END:Sourced from rpn.py\n","\n","      #image size required by ROI head\n","      image_shapes = images.image_sizes\n","\n","      # ************Following section of code implements similar functionality to:\n","      #detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n","      # from generalized_rcnn.py forward():\n","      # Code relating to the ROI heads (Region of Interest)\n","      #*******************************START:Sourced from roi_heads.py\n","      # lines 745-771 of roi_heads.py\n","      proposals, matched_idxs, labels, regression_targets = self.model.roi_heads.select_training_samples(proposals, targets)\n","\n","      box_features = self.model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n","      box_features = self.model.roi_heads.box_head(box_features)\n","      class_logits, box_regression = self.model.roi_heads.box_predictor(box_features)\n","\n","      result: List[Dict[str, torch.Tensor]] = []\n","      detector_losses = {}\n","\n","      # The following 2 lines of code were original inside a 'if self.training' condition\n","      # they have been taken out of this if statment so that\n","      # losses can be returned when in inference mode (to get val loss)\n","      loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n","      detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n","\n","      # the code below was the corresponding 'else' statement (ie if not self.training)\n","      # it was taken out if the else statement\n","      # so that both predictions and losses could be returned when in inference mode\n","      boxes, scores, labels = self.model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n","      num_images = len(boxes)\n","      for i in range(num_images):\n","          result.append(\n","              {\n","                  \"boxes\": boxes[i],\n","                  \"labels\": labels[i],\n","                  \"scores\": scores[i],\n","              }\n","          )\n","      # TEST HERE\n","      # conf_scores = []\n","      # for pred in result:\n","      #   for index, item in enumerate(pred['labels']):\n","      #     if item ==2:\n","      #       conf_scores.append(pred[\"scores\"][index].item())\n","      # print(conf_scores)\n","\n","      #*******************************END:Sourced from roi_heads.py\n","\n","      # rename results as detection so that it is compatible with the subsequent\n","      # line of code sourced from generalized_rcnn.py\n","      detections = result\n","      # Following line of code sourced from generalized_rcnn.py (line 100)\n","      detections = self.model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","      # MODIFICATION: REMOVED THE FOLLOWING 2 LINES OF CODE\n","      # model.rpn.training=False\n","      # model.roi_heads.training=False\n","\n","      #*******************************START:Sourced from generalized_rcnn.py forward()\n","      # lines 102-104 in generalized_rcnn.py\n","      losses = {}\n","      losses.update(detector_losses)\n","      losses.update(proposal_losses)\n","      #*******************************END:Sourced from generalized_rcnn.py forward()\n","\n","      #Added (generalized_rcnn.py forward())\n","      return losses, detections\n","\n","  #Function 'evaluate_loss' is modified from source:\n","  #https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n","  # this function has been modified to become a private class method\n","  # meaining the arg list and some of the internal code has changed\n","  #A docstring has been added for the modified function\n","  def __calc_val_loss(self, data_loader: torch.utils.data.DataLoader) -> float:\n","      \"\"\"Calculates average validation loss across batches in a PyTorch DataLoader\n","\n","      Calls function eval_forward() to caluclate validation loss for each batch.\n","      Each batch validation loss is added to a running total which is divided by the\n","      number of batches to get average validation loss across batches.\n","      Calls format_batch to prep input data and move it to the same device as the model (usually GPU)\n","\n","      Args:\n","          data_loader: a Pytorch DataLoader\n","\n","      Returns:\n","          The average validation loss across batches\n","\n","      Raises:\n","          None\n","      \"\"\"\n","      val_loss = 0\n","      with torch.no_grad():\n","        for batch in data_loader:\n","          #*************************************START OF MODIFICATIONS 1\n","          formatted_batch = format_batch(batch, self.model)\n","          images, targets = formatted_batch['images'], formatted_batch['targets']\n","          losses_dict, detections = self.__eval_forward(images, targets)\n","          #*************************************END OF MODIFICATIONS 1\n","          losses = sum(loss for loss in losses_dict.values())\n","          val_loss += losses\n","\n","      validation_loss = val_loss/ len(data_loader)\n","      #*************************************START OF MODIFICATIONS 2\n","      # detach from computational graph to free up GPU memory\n","      # use .item() to convert loss to a Python Number ready for plotting\n","      return validation_loss.detach().item()\n","\n","  def test(self,\n","           test_dataset_loader: torch.utils.data.DataLoader,\n","           is_console_logging: bool = True) -> dict:\n","    \"\"\"Calculates model mAP at different IoU thresholds on the given test set\n","\n","      Sets the model to eval mode and runs the model to get inferences.\n","      Non-maximum suppression is applied before mAPs are calculated.\n","      mAP50 and mAP50-95 are printed for convenience\n","      Model predictions are returned so that they can be visualised by\n","      a follow up call to visualise_predictions()\n","\n","      Args:\n","          test_dataset_loader: the Pytorch DataLoader for the test set\n","          is_console_logging: controls if print statements are sent to the console\n","\n","      Returns:\n","          Dictionary with 3 keys ; model predictions, model evaluation metrics (mAP) and image paths\n","          (to support visualisations of predictions on original imgs)\n","\n","      Raises:\n","          None\n","      \"\"\"\n","\n","    # according to the PyTorch docs:\n","    # \"be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode.\n","    # Failing to do this will yield inconsistent inference results.\"\n","    # https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html#:~:text=Saving%20and%20Loading%20Model%20Weights&text=Using%20weights_only%3DTrue%20is%20considered%20a%20best%20practice%20when%20loading%20weights.&text=be%20sure%20to%20call%20model,will%20yield%20inconsistent%20inference%20results.\n","    self.model.eval()\n","\n","    # memory management\n","    torch.cuda.empty_cache()\n","\n","    inferences_times = []\n","    all_preds = []\n","    all_targets = []\n","    test_imgs = []\n","    metric_map50 = MeanAveragePrecision(box_format=\"xyxy\",iou_type=\"bbox\", iou_thresholds=[0.5], class_metrics=True, extended_summary=True)\n","    #no iou_threshold leads to 50-95 coco mAP\n","    metric_map50_95 = MeanAveragePrecision(box_format=\"xyxy\",iou_type=\"bbox\", class_metrics=True, extended_summary=True)\n","\n","    #reduce memory usage by not storing intermediate tensors needed to calculate gradients\n","    #torch.no_grad reportedly speeds up computations\n","    with torch.no_grad():\n","      for batch in test_dataset_loader:\n","          # gather all images into a single array\n","          formatted_data = format_batch(batch, self.model)\n","          batch_images, batch_targets = formatted_data['images'], formatted_data['targets']\n","\n","          start_time = time.time()\n","          predicted = self.model(batch_images)\n","          # calculate average inference time for an image\n","          end_time = time.time()\n","          duration = execution_time = end_time - start_time\n","          test_size = len(test_dataset_loader)*test_dataset_loader.batch_size\n","          avg_time = duration / test_size\n","          inferences_times.append(avg_time)\n","          all_preds.extend(predicted)\n","          all_targets.extend(batch_targets)\n","          test_imgs.extend(batch_images)\n","\n","      # Update metric with predictions and respective ground truth\n","      metric_map50.update(all_preds, all_targets)\n","      metric_map50_95.update(all_preds, all_targets)\n","      # Compute the results\n","      map50_result = metric_map50.compute()\n","      map_50_95_result = metric_map50_95.compute()\n","\n","      if is_console_logging:\n","        print('mAP50:')\n","        print(map50_result['map'])\n","\n","        print('Class wise metrics: mAP50:')\n","        print(map50_result['map_per_class'])\n","        print(\"\\n\")\n","        print('mAP 50-95:')\n","        print(map_50_95_result['map'])\n","\n","        print('Class wise metrics: mAP50-95:')\n","        print(map_50_95_result['map_per_class'])\n","\n","        print('Average Inference time:')\n","        print(sum(inferences_times)/len(inferences_times))\n","\n","      # fig_, ax_ = metric.plot()\n","      return {\"predictions\": predicted, \"metrics\": {\"mAP50\": map50_result, \"mAP50-95\": map_50_95_result}, \"test_imgs\": test_imgs}"]},{"cell_type":"markdown","source":["### Cross validation code:"],"metadata":{"id":"cbPrMrteDsvw"}},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","\n","# add wandB logging here ?\n","def k_fold_training(train_dataset: UAVImageDataset,\n","                    n_splits: int,\n","                    random_state: int,\n","                    batch_size: int,\n","                    # model parameters\n","                    backbone: nn.Module,\n","                    anchor_generator: torchvision.models.detection.rpn.AnchorGenerator,\n","                    roi_pooler: torchvision.ops.MultiScaleRoIAlign,\n","                    num_epochs: 15) -> dict[str, float]:\n","    \"\"\"Performs K-fold cross validation and returns a dictionary of average mAP scores across folds\n","\n","      A new Faster R-CNN model is instantiated for each fold - based on the input parameters\n","\n","      Args:\n","          train_dataset: instance of the UAVImageDataset class,\n","          n_splits: number of splits for k-fold cross validation\n","          random_state: ensures splits are reproducible\n","          batch_size: batch size for both training and testing\n","          backbone: a custom neural network backbone for the Faster R-CNN model\n","          anchor_generator: the custom anchor generator to go with the custom backbone\n","          roi_pooler: the custom roi_pooler to go with the custom backbone,\n","          num_epochs: number of epochs to train the model for\n","\n","      Returns:\n","          Dictionary with 6 keys ; metrics for average, plastic and water hyacinth - at two IoU\n","          thresholds mAP50 and mAP50-95\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    mAP50_scores = []\n","    plastic_map50_scores = []\n","    water_hyacinth_map50_scores = []\n","\n","    mAP50_95_scores = []\n","    plastic_map50_95_scores = []\n","    water_hyacinth_map50_95_scores = []\n","\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n","\n","    for fold, (train_idx, test_idx) in enumerate(kf.split(train_dataset)):\n","      print(f\"Fold {fold + 1} of {n_splits}\")\n","\n","      # Define the data loaders for the current fold\n","      subsampled_train_dataset = torch.utils.data.Subset(train_dataset, train_idx)\n","      train_dataset_loader = torch.utils.data.DataLoader(subsampled_train_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=batch_size,\n","                                                   shuffle=True,\n","                                                   pin_memory=True,\n","                                                  num_workers=12,\n","                                                   prefetch_factor=10\n","                                                   )\n","\n","      subsampled_test_dataset = torch.utils.data.Subset(train_dataset, test_idx)\n","      test_dataset_loader = torch.utils.data.DataLoader(subsampled_test_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=batch_size,\n","                                                   shuffle=True,\n","                                                   pin_memory=True,\n","                                                  num_workers=12,\n","                                                   prefetch_factor=10\n","                                                   )\n","\n","      # initialise a model based on the parameters and move to device\n","      model_wrapper = FasterRCNNWrapper(\n","          backbone=backbone,\n","          num_classes=3,\n","          rpn_anchor_generator=anchor_generator,\n","          box_roi_pool=roi_pooler\n","      )\n","      num_classes=3 #2 classes +1 to account for background\n","      model_wrapper.override_prediction_heads(num_classes)\n","      model_wrapper.model.to(device)\n","\n","      # initialise optimizer\n","      optimizer = torch.optim.Adam(model_wrapper.model.parameters(), lr=0.00025)\n","      num_epochs = 15\n","\n","      # train\n","      model_wrapper.train(num_epochs=num_epochs,\n","                          train_dataset=train_dataset_loader,\n","                          optimizer=optimizer,\n","                          is_console_logging=False)\n","\n","      # test\n","      model_wrapper.model.eval()\n","      test_results = model_wrapper.test(test_dataset_loader=test_dataset_loader,\n","                         is_console_logging=False)\n","\n","      # push mAP to test_scores ready to calculate average acrosss folds\n","      # average results across classes\n","      mAP50_scores.append(test_results['metrics']['mAP50']['map'])\n","      mAP50_95_scores.append(test_results['metrics']['mAP50-95']['map'])\n","\n","      # class wise results\n","      plastic_map50_scores.append(test_results['metrics']['mAP50']['map_per_class'][0])\n","      plastic_map50_95_scores.append(test_results['metrics']['mAP50-95']['map_per_class'][0])\n","      water_hyacinth_map50_scores.append(test_results['metrics']['mAP50']['map_per_class'][1])\n","      water_hyacinth_map50_95_scores.append(test_results['metrics']['mAP50-95']['map_per_class'][1])\n","\n","      free_gpu_ram(train_dataset_loader)\n","      free_gpu_ram(test_dataset_loader)\n","\n","    # calculate average test scores across folds\n","    return {'avgMAP50': sum(mAP50_scores) / len(mAP50_scores),\n","            'avgMAP50-95': sum(mAP50_95_scores) / len(mAP50_95_scores),\n","            'avgPlastic_mAP50': sum(plastic_map50_scores) / len(plastic_map50_scores),\n","            'avgWaterHyacinth_mAP50': sum(water_hyacinth_map50_scores) / len(water_hyacinth_map50_scores),\n","            'avgPlastic_mAP50-95': sum(plastic_map50_95_scores) / len(plastic_map50_95_scores),\n","            'avgWaterHyacinth_mAP50-95': sum(water_hyacinth_map50_95_scores) / len(water_hyacinth_map50_95_scores)}"],"metadata":{"id":"bmzy3mfADsBe","executionInfo":{"status":"ok","timestamp":1743283623152,"user_tz":0,"elapsed":38,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Function to Test the robustness of the model"],"metadata":{"id":"6LPcvDJior8h"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","\n","\n","def model_robustness_curve(anchor_generator: torchvision.models.detection.rpn.AnchorGenerator,\n","                           roi_pooler: torchvision.ops.MultiScaleRoIAlign,\n","                           backbone: torch.nn.Module,\n","                          train_dataset: UAVImageDataset,\n","                           list_dataset_subsamples: List[float],\n","                           num_epochs: int,\n","                           val_dataset_loader: torch.utils.data.DataLoader,\n","                           test_dataset_loader: torch.utils.data.DataLoader,) -> Dict[str, float]:\n","    \"\"\"Analyses model performance across different sizes of training datasets\n","\n","      Trains the model on the specified subsets and creates a 'robsutness curve' showing mAP50-95 of each model\n","      The model is re-initialised for each train/test on dataset subsample ; so that the test set effectively\n","      remains unseen by the model\n","\n","      Args:\n","          anchor_generator: the anchor generator required for model initialisation\n","          roi_pooler: the ROI pooler required for model initialisation\n","          backbone: the custom backbone required for model initialisation\n","          train_dataset: an instance of the UAVImageDataset which has been loaded with data\n","                          required to be torch.utils.Dataset type so that torch.utils.data.Subset can be used to subsample,\n","            list_dataset_subsamples: the sizes of each subsample represented as decimals - ie 0.1 == 10%,\n","            num_epochs: number of epochs to train the model for,\n","            val_dataset_loader: validation dataset loader,\n","            test_dataset_loader: test dataset loader\n","\n","      Returns:\n","          Dictionary\n","\n","      Raises:\n","          None\n","      \"\"\"\n","\n","    training_set_sizes = []\n","    results = []\n","    for decimal in list_dataset_subsamples:\n","      size_training_set = round(len(train_dataset)*decimal)\n","      training_set_sizes.append(size_training_set)\n","      indices_to_extract = [i for i in range(size_training_set)]\n","      subsampled_dataset = torch.utils.data.Subset(train_dataset, indices_to_extract)\n","      subset_train_dataset_loader = torch.utils.data.DataLoader(subsampled_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=16,\n","                                                   shuffle=True,\n","                                                   pin_memory=True,\n","                                                  num_workers=12,\n","                                                   prefetch_factor=10\n","                                                   )\n","\n","      # re-initialise the model each time so that the test dataset remains unseen by the model\n","      model_wrapper = FasterRCNNWrapper(\n","          backbone=backbone,\n","          num_classes=3,\n","          rpn_anchor_generator=anchor_generator,\n","          box_roi_pool=roi_pooler\n","      )\n","      num_classes=3 #3 classes +1 to account for background\n","      model_wrapper.override_prediction_heads(num_classes)\n","\n","      model_wrapper.model.to(device)\n","      optimizer = torch.optim.SGD(model_wrapper.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","      test_results = model_pipeline(model_wrapper, optimizer, num_epochs, subset_train_dataset_loader,  val_dataset_loader, test_dataset_loader, device, False, \"Faster R-CNN Tracker\", is_console_logging=False)\n","      results.append(test_results['metrics']['mAP50-95']['map'].item())\n","\n","    # draw graph\n","    plt.plot(training_set_sizes, results)\n","    plt.title(\"Robustness Curve: Different sizes of dataset\")\n","    plt.xlabel(\"Number of images in training dataset\")\n","    plt.ylabel(\"mAP50-95 (average across the 2 classes)\")\n","    plt.show()"],"metadata":{"id":"pgARPlmMovAK","executionInfo":{"status":"ok","timestamp":1743283623166,"user_tz":0,"elapsed":9,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksp4GJNuAXY1"},"source":["## Definition of Model Pipeline: <a class=\"anchor\" name=\"model_pipeline\"></a>"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8129,"status":"ok","timestamp":1743283631296,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"ZBUsIr_1QSoq","outputId":"0a629737-48b1-4499-b1fb-71cec3071204"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n","Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: onnx\n","Successfully installed onnx-1.17.0\n"]}],"source":["!pip install wandb -qU\n","!pip install onnx"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"executionInfo":{"elapsed":5381,"status":"ok","timestamp":1743283636679,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"W2k2bLF8_BCM","outputId":"541a2094-d936-457c-b684-3fb20e4c1599"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33motterhian\u001b[0m (\u001b[33motterhian-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}],"source":["import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":3195,"status":"ok","timestamp":1743283639878,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"g3Nxq1HOAX2O"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter # to print to tensorboard\n","import onnx\n","\n","def model_pipeline(model_wrapper: FasterRCNNWrapper,\n","                   optimizer: torch.optim,\n","                   num_epochs: int,\n","\n","                   train_dataset_loader,\n","                   val_dataset_loader,\n","                   test_dataset_loader,\n","\n","                   device: torch.device,\n","                   is_logging: bool,\n","                   project_name: str,\n","                   is_console_logging: bool = True) -> Dict:\n","\n","  # Set up TensorBoard and Wandb logging\n","  if is_logging:\n","    log_dir = \"/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/tensorboard_runs/FasterRCNN/\"\n","\n","    # Prep identifiers for the run\n","    train_size = len(train_dataset_loader) * train_dataset_loader.batch_size\n","    val_size = len(val_dataset_loader) * val_dataset_loader.batch_size\n","    test_size = len(test_dataset_loader) * test_dataset_loader.batch_size\n","    time_stamp = datetime.datetime.now().strftime(\"%m_%d-%H_%M\")\n","\n","    writer = SummaryWriter(f\"{log_dir}/{train_size}_{val_size}_{test_size}____{time_stamp}\")\n","\n","    # gather hyperparameters used in this run\n","    optimizer_hyperparams = optimizer.state_dict()['param_groups'][0] #dict is first item in array\n","\n","    model = model_wrapper.model\n","    model_anchor_generator = model.rpn.anchor_generator\n","    train_set_size = train_dataset_loader.batch_size*len(train_dataset_loader)\n","    val_set_size = val_dataset_loader.batch_size*len(val_dataset_loader)\n","    test_set_size = test_dataset_loader.batch_size*len(test_dataset_loader)\n","\n","    if optimizer_hyperparams.get('momentum') == None: #ADAM does not have property momentum\n","      learning_rate, weight_decay = optimizer_hyperparams['lr'], optimizer_hyperparams['weight_decay']\n","      hyperparameters = {\n","          \"num_epochs\": num_epochs,\n","          \"train_val_test_split\": f\"{train_set_size}/{val_set_size}/{test_set_size}\",\n","          \"optimizer_lr\": learning_rate,\n","          \"optimizer_weight_decay\": weight_decay,\n","          # TensorBoard only accepts datatypes int, float, str, bool, or torch.Tensor\n","          # for the method add_hparams()\n","          # convert the following tuples and lists to strings\n","          \"anchor_aspect_ratios\": str(model_anchor_generator.aspect_ratios),\n","          \"num_anchors_per_loc\": str(model_anchor_generator.num_anchors_per_location()),\n","          \"anchor_sizes\": str(model_anchor_generator.sizes)\n","\n","      }\n","    else: #SGD does have property momentum\n","      learning_rate, momentum, weight_decay = optimizer_hyperparams['lr'], optimizer_hyperparams['momentum'], optimizer_hyperparams['weight_decay']\n","      hyperparameters = {\n","          \"num_epochs\": num_epochs,\n","          \"train_val_test_split\": f\"{train_set_size}/{val_set_size}/{test_set_size}\",\n","          \"optimizer_lr\": learning_rate,\n","          \"optimizer_momentum\": momentum,\n","          \"optimizer_weight_decay\": weight_decay,\n","          # TensorBoard only accepts datatypes int, float, str, bool, or torch.Tensor\n","          # for the method add_hparams()\n","          # convert the following tuples and lists to strings\n","          \"anchor_aspect_ratios\": str(model_anchor_generator.aspect_ratios),\n","          \"num_anchors_per_loc\": str(model_anchor_generator.num_anchors_per_location()),\n","          \"anchor_sizes\": str(model_anchor_generator.sizes)\n","\n","      }\n","\n","    # initialise wandb before training so that it monitors CPU usage\n","    # use 'with wandb.init' context manager so that wandb.finish() is automatically\n","    # called at the end of the block\n","    with wandb.init(project=project_name, config=hyperparameters):\n","      # track gradients and parameters\n","      # wandb.watch(modelWrapper.model, log=\"all\") #NOT WORKING WITH CUSTOM BACKBONE\n","\n","      # train the model and measure training time to log\n","      start_time = time.time()\n","      losses_dict = model_wrapper.train(num_epochs, optimizer, train_dataset_loader, val_dataset_loader, is_console_logging)\n","      end_time = time.time()\n","      execution_time = end_time - start_time\n","\n","      # delete training data from GPU RAM ready for testing\n","      free_gpu_ram(train_dataset_loader)\n","      free_gpu_ram(val_dataset_loader)\n","\n","      # log training time\n","      wandb.log({\"Training time\": execution_time})\n","      writer.add_scalar(\"Training time\", execution_time)\n","\n","      training_losses, val_losses = losses_dict['training_losses'], losses_dict['validation_losses']\n","      # log training data\n","      for i in range(len(training_losses)):\n","            # tensorboard log\n","            writer.add_scalar('Training Loss', training_losses[i], i)\n","            writer.add_scalar('Validation Loss', val_losses[i], i)\n","            writer.add_scalars( 'Training vs. Validation Loss',\n","                                {'Training' : training_losses[i], 'Validation' : val_losses[i] },\n","                                i)\n","      # Wandb log\n","      # log train vs val loss separately so they they can be visually compared with different runs\n","      wandb.log({\"train_loss\" : wandb.plot.line_series(\n","                  xs=list(range(num_epochs)),\n","                  ys=[training_losses],\n","                  title=\"Training loss\",\n","                  xname=\"Epochs\")})\n","\n","      wandb.log({\"val_loss\" : wandb.plot.line_series(\n","                  xs=list(range(num_epochs)),\n","                  ys=[val_losses],\n","                  title=\"Validation loss\",\n","                  xname=\"Epochs\")})\n","\n","      # plot together on same graph for direct comparison of train and val loss for a run\n","      wandb.log({\"train_vs_val_loss\" : wandb.plot.line_series(\n","                  xs=list(range(num_epochs)),\n","                  ys=[training_losses, val_losses],\n","                  keys=[\"Training\", \"Validation\"],\n","                  title=\"Train vs val loss\",\n","                  xname=\"Epochs\")})\n","\n","      # test the model and measure execution time so avg inference time can be calculated\n","      start_time = time.time()\n","      test_results = model_wrapper.test(test_dataset_loader, is_console_logging)\n","      end_time = time.time()\n","      execution_time = end_time - start_time\n","      # log testing data\n","\n","      # Save the model in the exchangeable ONNX format\n","      free_gpu_ram(test_dataset_loader)\n","      # GPU RAM not capable of tracing pre-trained model through input\n","      # test_images = [item[0].to(device=next(model.parameters()).device) for batch in test_dataset_loader for item in batch]\n","      # torch.onnx.export(modelWrapper.model, test_images, \"model.onnx\") #test_results[\"test_imgs\"]\n","      # wandb.save(\"model.onnx\")\n","\n","      # memory management\n","      # for img in test_images:\n","      #   del img\n","      # torch.cuda.empty_cache()\n","      # writer.add_graph(modelWrapper.model, test_images)\n","\n","      # calculate average inference time and log\n","      wandb.log({\"Inference time\": execution_time/test_set_size})\n","      writer.add_scalar(\"Inference\", execution_time/test_set_size)\n","\n","      # TensorBoard Log\n","      # UPDATE HERE ********************** FOR NEW RETURN STATEMENT FROM TEST FUNC\n","      # info\n","      # print('mAP50:')\n","      # print(map50_result['map'])\n","\n","      # print('Class wise metrics: mAP50:')\n","      # print(map50_result['map_per_class'])\n","\n","      # print('mAP 50-95:')\n","      # print(map_50_95_result['map'])\n","\n","      # print('Class wise metrics: mAP50-95:')\n","      # print(map_50_95_result['map_per_class'])\n","\n","      # # fig_, ax_ = metric.plot()\n","      # return {\"predictions\": predicted, \"metrics\": {\"mAP50\": map50_result, \"mAP50-95\": map_50_95_result},\n","\n","\n","\n","      avg_map50 = test_results['metrics']['mAP50']['map']\n","      plastic_mAP50 = test_results['metrics']['mAP50']['map_per_class'][0]\n","      water_hyacinth_mAP50 = test_results['metrics']['mAP50']['map_per_class'][1]\n","\n","      avg_mAP50_95 = test_results['metrics']['mAP50-95']['map']\n","      plastic_mAP50_95 = test_results['metrics']['mAP50-95']['map_per_class'][0]\n","      water_hyacinth_mAP50_95 = test_results['metrics']['mAP50-95']['map_per_class'][1]\n","      test_metrics = {\"avg mAP50\": avg_map50,\n","                 \"plastic mAP50\": plastic_mAP50,\n","                 \"water hyacinth mAP50\": water_hyacinth_mAP50,\n","\n","                 \"avg mAP50-95\": avg_mAP50_95,\n","                 \"plastic mAP50-95\": plastic_mAP50_95,\n","                 \"water hyacinth mAP50-95\": water_hyacinth_mAP50_95\n","                 }\n","\n","      writer.add_hparams(hyperparameters, test_metrics)\n","\n","      # Wandb log\n","      wandb.log(test_metrics)\n","      # log code files in case extra info is needed for analysis\n","      wandb.run.log_code(\"/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test\", include_fn=lambda path: path.endswith(\".ipynb\"))\n","\n","        # Write image data to TensorBoard log dir\n","        # writer.add_image( 'Four Fashion-MNIST Images', img_grid)\n","\n","      # free up memory as writer no longer needed\n","      writer.close()\n","      if not is_console_logging:\n","        return test_results\n","\n","  else:\n","    model_wrapper.train(num_epochs, optimizer, train_dataset_loader, val_dataset_loader, is_console_logging)\n","    # memory management\n","    # clear training data off GPU reading for testing data\n","    free_gpu_ram(train_dataset_loader)\n","\n","    test_results = model_wrapper.test(test_dataset_loader, is_console_logging)\n","    free_gpu_ram(test_dataset_loader)\n","\n","    if not is_console_logging:\n","      return test_results\n","\n","  # visualise samples of predictions\n","  # model_predictions = test_results[\"predictions\"]\n","  # for i in range(len(model_predictions)):\n","  #   visualisePredictions(i, test_dataset, model_predictions)"]},{"cell_type":"code","source":["stop"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"WJNxqUBt7iC7","executionInfo":{"status":"error","timestamp":1743283640058,"user_tz":0,"elapsed":177,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"3fd7673c-bc69-4d19-b792-132fe12de2db"},"execution_count":28,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'stop' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-4f76a9dad686>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"]}]},{"cell_type":"markdown","source":["## Load Data:"],"metadata":{"id":"kQZWkyOk4IIT"}},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8554,"status":"ok","timestamp":1743283653756,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"OySnCC0JQueK","outputId":"b8f595ea-e85b-44c7-ef02-6def48eebe2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1743283653824,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"bfTNhPgXP-xY"},"outputs":[],"source":["# set deterministic behaviour\n","set_seeds()"]},{"cell_type":"code","source":["# faster loading times\n","train_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","val_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/valid/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/valid_pd.csv\",\n","                              classes_start_at_zero=False)\n","\n","test_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/test/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/test_pd.csv\",\n","                               classes_start_at_zero=False)"],"metadata":{"id":"AT3PjUz-UlGx","executionInfo":{"status":"aborted","timestamp":1743283640129,"user_tz":0,"elapsed":43414,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43415,"status":"aborted","timestamp":1743283640131,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"nrRB0qqh-HxI"},"outputs":[],"source":["train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=8,\n","                                                   shuffle=True,\n","                                                   pin_memory=True,\n","                                                  num_workers=12,\n","                                                   prefetch_factor=10\n","                                                   )\n","\n","\n","val_dataset_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=lambda x: x, batch_size=8,\n","                                                  num_workers=6)\n","\n","test_dataset_loader = torch.utils.data.DataLoader(test_dataset, collate_fn=lambda x: x, batch_size=8,\n","                                                  num_workers=6)"]},{"cell_type":"markdown","metadata":{"id":"jqwB9h8IuqTs"},"source":["## Establishing a meaningful baseline of performance: <a class=\"anchor\" name=\"fasterRCNN_baseline\"></a>"]},{"cell_type":"markdown","source":["### Pre-trained Faster R-CNN baseline performance:"],"metadata":{"id":"lRJfIm0h4A8d"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1743283640159,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"UcNClpum_Rij"},"outputs":[],"source":["from torchvision.models import resnet50\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models._utils import _ovewrite_value_param\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights, FastRCNNPredictor\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers\n","from torchvision.models.detection._utils import overwrite_eps\n","from torchvision.ops import misc as misc_nn_ops\n","import torch.nn as nn\n","\n","num_trainable_backbone_layers = 3 # valid range for resnet50 is 0-5 according to PyTorch docs\n","backbone = resnet50(weights=None, progress=True, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","backbone = _resnet_fpn_extractor(backbone, num_trainable_backbone_layers)\n","\n","#use default values for a baseline implementation\n","anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n","\n","# anchor_sizes = ((16,), (32,), (64,), (128,), (512,))\n","\n","# default values\n","anchor_aspect_ratios = ((0.5, 1.0, 2.0),) * len (anchor_sizes)\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=anchor_sizes,\n","    aspect_ratios=anchor_aspect_ratios\n",")\n","#use default values for a baseline implementation\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[\"0\", \"1\", \"2\", \"3\"],\n","                                                output_size=7,\n","                                                sampling_ratio=2)\n","\n","from torchvision.models.detection.roi_heads import RoIHeads\n","custom_roi_heads = RoIHeads(\n","            box_roi_pool = None,\n","            box_head = None,\n","            box_predictor = None,\n","            # Faster R-CNN training\n","            fg_iou_thresh=0.5,\n","            bg_iou_thresh=0.5,\n","            batch_size_per_image=512,\n","            positive_fraction=0.25,\n","            bbox_reg_weights=None,\n","            # Faster R-CNN inference\n","            score_thresh=0.001, #default is 0.05,\n","            nms_thresh=0,  #default is 0.5,\n","            detections_per_img=100\n","        )\n","\n","# # Box parameters\n","#         box_roi_pool=None,\n","#         box_head=None,\n","#         box_predictor=None,\n","#         box_score_thresh=0.05,\n","#         box_nms_thresh=0.5,\n","#         box_detections_per_img=100,\n","#         box_fg_iou_thresh=0.5,\n","#         box_bg_iou_thresh=0.5,\n","#         box_batch_size_per_image=512,\n","#         box_positive_fraction=0.25,\n","#         bbox_reg_weights=None,\n","\n","\n","\n","\n","custom_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=91,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler,\n","    # TESTING\n","    # box_predictor=FastRCNNPredictor(1024, 3) #seems like this could replace the override_prediction_heads method?\n","    # roi_heads=custom_roi_heads\n","\n","\n","    # box_score_thresh=0.001,\n","    # box_nms_thresh=0,\n","\n",")\n","# use default weights trained on COCO image dataset\n","weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n","custom_model_wrapper.load_pretrained_weights(weights, FasterRCNN_ResNet50_FPN_Weights)\n","#override the prediction heads to have the correct number of classes\n","num_classes=3 #3 classes +1 to account for background\n","custom_model_wrapper.override_prediction_heads(num_classes)\n","\n","# print(custom_model_wrapper.model.roi_heads.nms_thresh)\n","# print(custom_model_wrapper.model.roi_heads.score_thresh)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1743283640161,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"vvGMgIbP_UbT"},"outputs":[],"source":["# #Must move model (and it's parameters) to the GPU before initialisating the optimizer\n","# #according to a video on the official PyTorch Youtube Channel\n","# #https://www.youtube.com/watch?v=jF43_wj_DCQ&t=213s\n","# #otherwise the optimiser will update the wrong set of model parameters (on the CPU)\n","custom_model_wrapper.model.to(device)\n","# # try this for optimizer\n","# # filter(lambda p: p.requires_grad, model.parameters())\n","# # https://medium.com/we-talk-data/guide-to-freezing-layers-in-pytorch-best-practices-and-practical-examples-8e644e7a9598#:~:text=To%20avoid%20this%2C%20verify%20the%20requires_grad%20status%20of%20each%20layer.&text=By%20running%20this%20code%2C%20you,ve%20frozen%20the%20correct%20ones.&text=If%20you%20don't%20filter,parameters%20that%20won't%20change.\n","optimizer = torch.optim.SGD(custom_model_wrapper.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","num_epochs = 15\n","model_pipeline(custom_model_wrapper, optimizer, num_epochs, train_dataset_loader,  val_dataset_loader, test_dataset_loader, device, False, \"Faster R-CNN Tracker\")"]},{"cell_type":"code","source":["resnet_50_path = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/saved_models/resnet_50_weights.pth'\n","torch.save(custom_model_wrapper.model.state_dict(), resnet_50_path)"],"metadata":{"id":"JD3tXRqAo22G","executionInfo":{"status":"aborted","timestamp":1743283640162,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43444,"status":"aborted","timestamp":1743283640163,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"HYxxkChrR-6e"},"outputs":[],"source":["#code sourced from\n","#https://saturncloud.io/blog/check-the-total-number-of-parameters-in-a-pytorch-model/#:~:text=To%20check%20the%20number%20of%20parameters%20in%20a%20PyTorch%20model,its%20total%20number%20of%20elements.\n","total_params = sum(p.numel() for p in custom_model_wrapper.model.parameters())\n","print(f\"Number of parameters: {total_params}\")"]},{"cell_type":"markdown","source":["### Pre-trained YOLO baseline performance:"],"metadata":{"id":"KjpmwmZo4Spp"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43444,"status":"aborted","timestamp":1743283640164,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"0yQpNRJXTtlx"},"outputs":[],"source":["!pip install ultralytics --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43445,"status":"aborted","timestamp":1743283640165,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"VFcHjpunGQ2T"},"outputs":[],"source":["!pip install thop --no-deps"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43444,"status":"aborted","timestamp":1743283640165,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"3zNmNytbi7rz"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# use the default parameters specified by docs: https://docs.ultralytics.com/usage/cfg/#train-settings\n","# this gives an idea of how YOLOv12 would perform 'out of the box'\n","hyperparameters = {\n","      \"epochs\":15,\n","      # Optimizer Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for automatic selection based on model configuration.\n","      \"optimizer\": \"auto\",\n","      \"img_size\": 640, #preferred images size according to github\n","      \"batch_size\":16,\n","      \"weight_decay\":0.0005,\n","      \"warm_up_epochs\" : 3.0,\n","  }"],"metadata":{"id":"IJNyZgXeG0-O","executionInfo":{"status":"aborted","timestamp":1743283640166,"user_tz":0,"elapsed":43444,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43444,"status":"aborted","timestamp":1743283640167,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"QIe0Q0dXXQYK"},"outputs":[],"source":["from ultralytics import YOLO\n","import time\n","\n","with wandb.init(project=\"Updated YOLO Tracker\", config=hyperparameters):\n","    # Load a model\n","    model = YOLO(\"yolo12s.pt\") #use the latest version of YOLO\n","\n","    # Train the model\n","    start_time = time.time()\n","\n","    train_results = model.train(\n","        data=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/UAV riverine plastic 2 - null images removed - resized-to-2048x1535/data.yaml\",\n","        epochs = hyperparameters[\"epochs\"], # number of training epochs\n","        imgsz = hyperparameters[\"img_size\"], # training image size\n","        batch = hyperparameters[\"batch_size\"],\n","        weight_decay = hyperparameters[\"weight_decay\"],\n","        warmup_epochs = hyperparameters[\"warm_up_epochs\"],\n","        plots=True, #save plots for analysis\n","        device= \"cuda:0\"\n","        )\n","\n","    end_time = time.time()\n","    execution_time = end_time - start_time\n","    # log training time\n","    wandb.log({\"Training time\": execution_time})\n","\n","    class_wise_metrics_map50 = {}\n","    class_wise_metrics_map95 = {}\n","    for class_index in train_results.ap_class_index:\n","      map50, map50_95 = train_results.class_result(class_index)[2], train_results.class_result(class_index)[3]\n","      class_wise_metrics_map50[f\"{train_results.names[class_index]}_map50\"] = map50\n","      class_wise_metrics_map95[f\"{train_results.names[class_index]}_map95\"] = map50_95\n","\n","\n","    wandb.log(class_wise_metrics_map50)\n","    wandb.log(class_wise_metrics_map95)\n","    wandb.log(train_results.results_dict)\n","\n","    wandb.log({\"PR_curve\": wandb.Image(f\"/content/runs/detect/train/PR_curve.png\")})\n","    wandb.log({\"Results\": wandb.Image(f\"/content/runs/detect/train/results.png\")})"]},{"cell_type":"markdown","metadata":{"id":"lhnkHUQVBDfy"},"source":["## Custom Backbone 1: Residual connections + CBAM + FPN:"]},{"cell_type":"markdown","source":["### Definition of Residual Blocks:"],"metadata":{"id":"soxHzc876PnE"}},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1743283671899,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"N83VwbvzNb_4"},"outputs":[],"source":["from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models._utils import _ovewrite_value_param\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights, FastRCNNPredictor\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers\n","from torchvision.models.detection._utils import overwrite_eps\n","from torchvision.ops import misc as misc_nn_ops\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.nn import Conv2d, BatchNorm2d, LeakyReLU, ReLU, LazyConv2d\n","\n","#  output size of conv layer = [(input size + 2(padding) - kernel size)/stride]  +1\n","\n","# the ResNet block proposed in the original paper https://arxiv.org/pdf/1512.03385\n","# 3x3 convolutional layers with stride of 1 and padding of 1\n","# followed by batch normalisation and activation function\n","# then 3x3 convolutional layer\n","# batch normalisation\n","# residual connection (add block inputs to current set of features)\n","# final activation function\n","\n","# CODE IN THE FOLLOWING CELLS IS INFORMED BY\n","# https://debuggercafe.com/traffic-sign-detection-using-pytorch-faster-rcnn-with-custom-backbone/\n","# https://debuggercafe.com/traffic-sign-recognition-using-custom-image-classification-model-in-pytorch/\n","\n","# try ReLU or ELU ?\n","    # ReLU not great for intermediate layers in a network as\n","    # it suffers from 'dying ReLUs'\n","    # https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/\n","    # x = F.relu(self.bn1(self.conv1(x)))\n","\n","class IdentityBlock(nn.Module):\n","  \"\"\"\n","  An identity conv block with a residual connection ;\n","  based on the original paper ResNet https://arxiv.org/pdf/1512.03385\n","\n","  This block processes an image without changing dimensions;\n","  The block has 2 convolutional layers + batch normalisation\n","  Kernel size is consistent across the 2 convolutional layers\n","  (kernel size must remain the same otherwise input dimensions would not match output dimensions)\n","  stride is 1 for each conv layer - otherwise iput shape will change and raw input dimension will\n","  not be compatible with latest layer output. This means residual connection will fail.\n","\n","  Attributes:\n","  - 2 conv layers\n","  - 2 batch normalisation layers\n","  \"\"\"\n","  def __init__(self,\n","               num_filters : int,\n","               kernel : int,\n","               input_padding : int\n","               ) -> None:\n","\n","    \"\"\"Initalises layers required for forward pass\n","\n","    Args:\n","      num_filters: the number of filters (out_channels) for every conv layer\n","      kernel: kernel size for each conv layer\n","      input_padding: padding for every conv layer\n","\n","    Returns: None\n","    \"\"\"\n","    super().__init__()\n","\n","    #formula for output shape of conv2d:\n","    # o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n","    # i= input dim, p=padding, k=kernel, d=dilation , s=stride\n","    # source: https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338\n","    # in order for input dimensions to remain the same:\n","    # if stride is 1:  o = [i + 2*p - k - (k-1)*(d-1)] + 1\n","    # for input.dim to equal output.dim, then 2*p - k - (k-1)*(d-1) = -1 ; so that o =  i-1+1\n","    # dilation is 1 by default: 2*p - k = -1\n","    # so 2p = k-1  in order for input shape not to change ?\n","    assert 2*input_padding == kernel-1, \"Padding and kernel size not suitable for identity block, these p and k values will change input dimensions\"\n","\n","    self.conv1 = LazyConv2d(\n","                        out_channels = num_filters,\n","                        kernel_size = kernel,\n","                        stride = 1,\n","                        padding = input_padding\n","                        )\n","\n","    self.conv2 = LazyConv2d(\n","                        out_channels = num_filters,\n","                        kernel_size = kernel,\n","                        stride = 1,\n","                        padding = input_padding\n","                        )\n","    # batch normalisation has learnable parameters ('affine parameters') so create\n","    # separate batch normalisation layers for each conv layer\n","    self.bn1 = BatchNorm2d(num_filters)\n","    self.bn2 = BatchNorm2d(num_filters)\n","\n","  def forward(self, x: torch.tensor) -> torch.tensor:\n","    # save original input ready to add at residual connection\n","    identity = x\n","\n","    # LAYER 1\n","    x = self.conv1(x)\n","    x = self.bn1(x)\n","    x = F.relu(x)\n","\n","    # LAYER 2\n","    x = self.conv2(x)\n","    x = self.bn2(x)\n","\n","    x += identity #residual connection\n","\n","    x = F.relu(x)\n","    return x"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1743283672584,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"UPRgJcltRP-u"},"outputs":[],"source":["class ConvBlock(nn.Module):\n","  \"\"\"\n","  A conv block with a residual connection;\n","  based on the original paper ResNet https://arxiv.org/pdf/1512.03385\n","\n","  The block has 2 convolutional layers + batch normalisation\n","  The third conv layer is to resize the original input so it can be added as a residual connection\n","\n","  Attributes:\n","  - 3 conv layers\n","  - 2 batch normalisation layers\n","  \"\"\"\n","  def __init__(self,\n","               num_filters : int,\n","               kernel : int,\n","               stride_val : int ,\n","               input_padding : int\n","               ) -> None:\n","    \"\"\"\n","    Initialises the layers require by forward()\n","\n","    Args:\n","    num_filters: the number of filters for every conv layer\n","    kernel: kernel size for every conv layer\n","    stride_val: kernel size for the first and last conv layer\n","    input_padding: padding size for the first 2 conv layers\n","\n","    Returns: None\n","    \"\"\"\n","    super().__init__()\n","    self.conv1 = LazyConv2d(\n","                        out_channels = num_filters,\n","                        kernel_size = kernel,\n","                        stride = stride_val,\n","                        padding = input_padding\n","                        )\n","\n","    self.conv2 = LazyConv2d(out_channels = num_filters,\n","                            kernel_size = kernel,\n","                            #use defualt stride value,\n","                            padding = input_padding\n","                            )\n","\n","    # use a 1x1 conv layer for 'cross channel downsampling'\n","    # the 1x1 conv will be used to effectively reshape the inputs ('identity') so that\n","    # it can be added to the output of the layer preceding the residual connection\n","    self.conv_input_transformer = LazyConv2d(\n","                                        out_channels = num_filters, #match output shape of last layer,\n","                                        kernel_size = 1,\n","                                        stride = stride_val,\n","                                        padding = 0\n","                                        )\n","    # batch normalisation has learnable parameters ('affine parameters') so create\n","    # separate batch normalisation layers for each conv layer\n","    self.bn1 = BatchNorm2d(num_filters)\n","    self.bn2 = BatchNorm2d(num_filters)\n","\n","\n","  def forward(self, x: torch.tensor) -> torch.tensor:\n","    # save original input ready to add at residual connection\n","    identity = x\n","\n","    # LAYER 1\n","    x = self.conv1(x)\n","    x = self.bn1(x)\n","    x = F.relu(x)\n","\n","    # LAYER 2\n","    x = self.conv2(x)\n","    x = self.bn2(x)\n","\n","    x += self.conv_input_transformer(identity) #residual connection\n","    x = F.relu(x)\n","\n","    return x\n"]},{"cell_type":"markdown","source":["### Definition of Spatial and Channel Attention:"],"metadata":{"id":"hf-U1MTr6LvO"}},{"cell_type":"code","source":["#SAM\n","# 1. channel pool: input tensor (cxhxw) is decomposed to (2xhxw)\n","# the two channels represent max pooling and average pooling across all channels\n","\n","# 2. conv layer which outputs a 1 channel feature map (1xhxw)\n","# 3. batch normalisation layer to normalise and scale output of conv layer\n","\n","# 4. option to use relu activation function after conv layer\n","\n","# 5. output is passed to sigmoid activation fn\n","\n","# 6. element-wise multiplication of spatial attention mask and all feature maps in input tensor\n","\n","# https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py\n","class SpatialAttention(nn.Module):\n","  def __init__(self, kernel_size:int = 7,\n","               conv_bias: bool = False,\n","               has_batch_norm: bool = True,\n","               has_relu: bool = True) -> None:\n","    super().__init__()\n","\n","    self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, stride=1, padding=(kernel_size-1) // 2, bias=conv_bias)\n","    self.bn = nn.BatchNorm2d(1) if has_batch_norm else None\n","    self.relu = nn.ReLU() if has_relu else None\n","\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    raw_input = x\n","\n","    # compress input\n","    x = torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )\n","\n","    # conv layer\n","    x = self.conv(x)\n","    if self.bn is not None:\n","      x = self.bn(x)\n","    if self.relu is not None:\n","      x = self.relu(x)\n","\n","    x = self.sigmoid(x)\n","\n","    output = raw_input * x\n","    return output\n","\n","class ChannelAttention(nn.Module):\n","  def __init__(self, in_channels: int, reduction_ratio: int =16, ):\n","    super().__init__()\n","\n","    # according to the CBAM research paper\n","    # use a shared MLP for attention inference to save parameters, as both of aggregated channel\n","    # features lie in the same semantic embedding space.\n","    self.mlp = nn.Sequential(\n","        # input layer\n","        nn.Linear(in_channels, in_channels // reduction_ratio),\n","        nn.ReLU(),\n","        # hidden layer\n","        nn.LazyLinear(in_channels // reduction_ratio),\n","        nn.ReLU(), #use relu within the hidden layers to mitigate risk of vanishing gradient\n","        nn.LazyLinear(in_channels // reduction_ratio),\n","        nn.ReLU(),\n","        # output layer\n","        nn.Linear(in_channels // reduction_ratio, in_channels)\n","    )\n","    self.sigmoid = nn.Sigmoid()\n","\n","  def forward(self, x):\n","    raw_input = x\n","    # Maxpool input\n","    max_pool = F.max_pool2d( x,\n","                            kernel_size=(x.size(2), x.size(3)),\n","                            stride=(x.size(2), x.size(3)))\n","    # feed through MLP\n","    max_pool = max_pool.view(max_pool.size(0), -1) #flatten ready for linear layers\n","    channel_att_max = self.mlp( max_pool )\n","\n","    # AvgPool input\n","    avg_pool = F.avg_pool2d( x,\n","                            kernel_size=(x.size(2), x.size(3)),\n","                             stride=(x.size(2), x.size(3)))\n","    # feed through MLP\n","    avg_pool = avg_pool.view(avg_pool.size(0), -1) #flatten ready for linear layers\n","    channel_att_avg = self.mlp( avg_pool )\n","\n","    # add MLP results\n","    attention_mask = channel_att_max + channel_att_avg\n","\n","    # feed through sigmoid\n","    attention_mask = self.sigmoid(attention_mask)\n","    # unsqueeze to make dimension of attention mask compatible with raw input\n","    # fixes RuntimeError: The size of tensor a (136) must match the size of tensor b (32) at non-singleton dimension 3\n","    attention_mask = attention_mask.unsqueeze(-1).unsqueeze(-1)\n","\n","    output = raw_input * attention_mask\n","    return output"],"metadata":{"id":"qiXa1_-gaf7B","executionInfo":{"status":"ok","timestamp":1743283673415,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["class CBAM(nn.Module):\n","  def __init__(self,\n","               in_channels:int,\n","               reduction_ratio: int = 16,\n","               is_channel_att: bool = True,\n","               is_spatial_att: bool = True) -> None:\n","    super().__init__()\n","    self.channel_attention = ChannelAttention(in_channels, reduction_ratio) if is_channel_att else None\n","    self.spatial_attention = SpatialAttention() if is_spatial_att else None\n","\n","  def forward(self, x):\n","    if self.channel_attention is not None:\n","      x = self.channel_attention(x)\n","    if self.spatial_attention is not None:\n","      x = self.spatial_attention(x)\n","\n","    return x"],"metadata":{"id":"imOq5Hy5axYW","executionInfo":{"status":"ok","timestamp":1743283673922,"user_tz":0,"elapsed":17,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["class ScaledCBAM(nn.Module):\n","  def __init__(self, in_channels, scaling_parameter):\n","    super().__init__()\n","    self.cbam = CBAM(in_channels)\n","    self.scale = scaling_parameter\n","\n","  def forward(self, x):\n","    return self.scale * self.cbam(x)\n"],"metadata":{"id":"4L-4fYicav4U","executionInfo":{"status":"ok","timestamp":1743283674580,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1743283675484,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"X8lzhS2rGJqV"},"outputs":[],"source":["def initialise_resnet_block(num_filters: int,\n","                     kernel_size: int,\n","                     stride: int,\n","                     padding: int,\n","                     cbam_module: CBAM | ScaledCBAM = None\n","                     ) -> torch.nn.Sequential:\n","  \"\"\"\n","  Initialises a ResNet block (consisting of one 'Conv block\" and one \"Identity block\")\n","\n","  Args:\n","  num_filters: the number of filters for both blocks\n","  kernel_size: the kernel size for both blocks\n","  stride: the stride value for the first block only (conv block)\n","          (identity block must have stride of 1 so that dimensions remain unchanged)\n","  padding: padding size for both conv blocks\n","  cbam_module (optional parameter): add CBAM attention module to the block - this parameter is an instantiation one of the 2 class types ^\n","\n","  Returns: a PyTorch sequential container (effectively a model)\n","\n","  \"\"\"\n","\n","  if cbam_module:\n","    return nn.Sequential(ConvBlock(num_filters=num_filters,\n","                                        kernel =kernel_size,\n","                                        stride_val=stride,\n","                                        input_padding =padding ),\n","\n","                          IdentityBlock(num_filters=num_filters,\n","                                        kernel=kernel_size,\n","                                        input_padding=padding),\n","                         cbam_module)\n","  else:\n","    return nn.Sequential(ConvBlock(num_filters=num_filters,\n","                                    kernel =kernel_size,\n","                                    stride_val=stride,\n","                                    input_padding =padding ),\n","\n","                          IdentityBlock(num_filters=num_filters,\n","                                        kernel=kernel_size,\n","                                        input_padding=padding))"]},{"cell_type":"markdown","source":["### Definition of Feature Pyramid Network:"],"metadata":{"id":"D7EFhOZ-5kV0"}},{"cell_type":"code","source":["# the code in this cell is adapted from Kapil Sachdeva:\n","# https://www.youtube.com/watch?v=FKsgO0U7CUw&list=PLjRC2jqp3e7QNio7NJXRh_gvAdLV6_aZC&index=132\n","# the code has been modified so that it accomodates all 4 layers (feature maps) of the ResNetCBAM backbone (defined above)\n","from functools import partial\n","from torchvision.ops.misc import Conv2dNormActivation\n","\n","# TOP DOWN FPN\n","class FPN(nn.Module):\n","  def __init__(self, in_channels: list[int], out_channels: int) -> list:\n","    \"\"\"Implements top down feature pyramid network\n","\n","      Sets the model to eval mode and runs the model to get inferences.\n","      Non-maximum suppression is applied before mAPs are calculated.\n","      mAP50 and mAP50-95 are printed for convenience\n","      Model predictions are returned so that they can be visualised by\n","      a follow up call to visualise_predictions()\n","\n","      Args:\n","          in_channels: list of integers representing the number of channels for each feature map:\n","                        list should follow the order low level -> high level features\n","          out_channels: the number of channels each output feature map will have\n","\n","      Returns:\n","          A list containing the resulting feature maps (ordered high level -> low level)\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","    self.output_channels = out_channels\n","    if in_channels[3] != out_channels:\n","      self.hl_channel_reducer = ConvBlock(\n","          in_channels=in_channels[3],\n","          out_channels=out_channels,\n","          kernel_size=1,\n","      )\n","\n","    self.ml_1_channel_reducer = ConvBlock(\n","        in_channels=in_channels[2],\n","        out_channels=out_channels,\n","        kernel_size=1,\n","    )\n","\n","    self.ml_2_channel_reducer = ConvBlock(\n","        in_channels=in_channels[1],\n","        out_channels=out_channels,\n","        kernel_size=1,\n","    )\n","\n","    self.ll_channel_reducer = ConvBlock(\n","        in_channels=in_channels[0],\n","        out_channels=out_channels,\n","        kernel_size=1,\n","    )\n","\n","    self.ml_1_smoother = ConvBlock(\n","        in_channels=out_channels,\n","        out_channels=out_channels,\n","    )\n","\n","    self.ml_2_smoother = ConvBlock(\n","        in_channels=out_channels,\n","        out_channels=out_channels,\n","    )\n","\n","    self.ll_smoother = ConvBlock(\n","        in_channels=out_channels,\n","        out_channels=out_channels,\n","    )\n","\n","  def forward(self,\n","              hl_features:torch.Tensor,\n","              ml_1_features:torch.Tensor,\n","              ml_2_features:torch.Tensor,\n","              ll_features:torch.Tensor):\n","\n","    if hl_features.size()[1] != self.output_channels:\n","      hl_channel_r = self.hl_channel_reducer(hl_features)\n","      del hl_features\n","\n","    ml_channel_1_r = self.ml_1_channel_reducer(ml_1_features) #Given groups=1, weight of size [256, 128, 1, 1], expected input[16, 64, 50, 68] to have 128 channels, but got 64 channels instead\n","    del ml_1_features\n","    ml_channel_2_r = self.ml_2_channel_reducer(ml_2_features)\n","    del ml_2_features\n","    ll_channel_r = self.ll_channel_reducer(ll_features)\n","    del ll_features\n","\n","    # addition 1\n","    if hl_features.size()[1] != self.output_channels:\n","      upsample_hl = F.interpolate(\n","          hl_channel_r,\n","          size=[ml_channel_1_r.size(2), ml_channel_1_r.size(3)],\n","          mode=\"nearest\",\n","      )\n","    else:\n","      upsample_hl = F.interpolate(\n","          hl_features,\n","          size=[ml_channel_1_r.size(2), ml_channel_1_r.size(3)],\n","          mode=\"nearest\",\n","      )\n","\n","    fused_hl_ml_1 = upsample_hl + ml_channel_1_r\n","    smoothed_ml_1_features = self.ml_1_smoother(fused_hl_ml_1)\n","    # memory management\n","    del ml_channel_1_r, fused_hl_ml_1, upsample_hl\n","\n","    # addition 2\n","    upsample_ml_1 = F.interpolate(\n","        smoothed_ml_1_features,\n","        size=[ml_channel_2_r.size(2), ml_channel_2_r.size(3)],\n","        mode=\"nearest\",\n","    )\n","    fused_ml_2_ml_1 = upsample_ml_1 + ml_channel_2_r\n","    smoothed_ml_2_features = self.ml_2_smoother(fused_ml_2_ml_1)\n","    # memory management\n","    del upsample_ml_1, ml_channel_2_r, fused_ml_2_ml_1\n","\n","    # addition 3\n","    upsample_ml_2 = F.interpolate(\n","        smoothed_ml_2_features,\n","        size=[ll_channel_r.size(2), ll_channel_r.size(3)],\n","        mode=\"nearest\",\n","    )\n","    fused_ml_ll = upsample_ml_2 + ll_channel_r\n","    smoothed_ll_features = self.ll_smoother(fused_ml_ll)\n","    # memory management\n","    del upsample_ml_2, ll_channel_r, fused_ml_ll\n","\n","    out = [smoothed_ll_features,\n","           smoothed_ml_1_features,\n","           smoothed_ml_2_features,\n","           hl_features if hl_features.size()[1] == self.output_channels else hl_channel_r,\n","           ]\n","    return out"],"metadata":{"id":"M99cUP8a5VBU","executionInfo":{"status":"ok","timestamp":1743283677718,"user_tz":0,"elapsed":1,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["class ResNetCBAMCustomFPN(nn.Module):\n","    \"\"\"\"\n","    Initialises a custom network that is a blend of ResNet and CBAM Attention\n","\n","    Attributes:\n","    - block 1 is created in line with definition of original ResNet research paper\n","    - blocks 2-4 are composed of: ResNet \"conv block\" + ResNet \"Identity Block\" + CBAM attention module\n","    - block 5 is composed of: ResNet \"conv block\" + ResNet \"Identity Block\"\n","\n","    \"\"\"\n","    def __init__(self,\n","                 input_channels: int =3, # datasets imgs are in colour so default input channels is 3\n","                 num_classes: int =4) -> None:\n","\n","        \"\"\"\n","        Initalises 5 blocks ready for the forward pass\n","\n","        Args:\n","        input_channels: number of channles in the input image (3 for color imgs)\n","        num_classes: the number of classes in the dataset\n","\n","        Returns: None\n","        \"\"\"\n","        super().__init__()\n","        # a conv layer with kernel=3, stride=1, padding=1 is known as 'same convolution' meaning the output size won't change\n","        # first block of resnet is defined as follows\n","        self.block1 = nn.Sequential(\n","            Conv2d(in_channels=input_channels,\n","                      out_channels=16, #output channels = number of filters\n","                      kernel_size=7,\n","                      stride=2,\n","                      padding=3),\n","\n","            nn.BatchNorm2d(16),\n","            LeakyReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        )\n","\n","        self.block2 = initialise_resnet_block(num_filters=32,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1,\n","                                              cbam_module=ScaledCBAM(in_channels=32, scaling_parameter=nn.Parameter(torch.zeros(1))))\n","\n","        self.block3 = initialise_resnet_block(num_filters=64,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1,\n","                                              cbam_module=CBAM(in_channels=64))\n","\n","        self.block4 = initialise_resnet_block(num_filters=128,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1,\n","                                              cbam_module=CBAM(in_channels=128))\n","\n","        self.block5 = initialise_resnet_block(num_filters=256,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1)\n","\n","        self.fpn = FPN(in_channels= [32,64,128,256], out_channels= 256)\n","\n","        # TEST\n","        # self.dropout = nn.Dropout(p=0.25)\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor :\n","        x = self.block1(x)\n","\n","        ll_feats = self.block2(x)\n","        ml_2_feats = self.block3(ll_feats)\n","        ml_1_feats = self.block4(ml_2_feats)\n","        hl_feats = self.block5(ml_1_feats)\n","\n","        x = self.fpn(hl_features = hl_feats,\n","              ml_1_features= ml_1_feats,\n","              ml_2_features= ml_2_feats,\n","              ll_features= ll_feats)\n","\n","        return OrderedDict([(k, v) for k, v in zip([\"ll_feats\",\n","                                                    \"ml_1_feats\",\n","                                                    \"ml_2_feats\",\n","                                                    \"hl_feats\"], x)])"],"metadata":{"id":"_aUGl81ozcXp","executionInfo":{"status":"aborted","timestamp":1743283640173,"user_tz":0,"elapsed":43444,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43443,"status":"aborted","timestamp":1743283640173,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"nO511zVYe5S9"},"outputs":[],"source":["backbone_fpn = ResNetCBAMCustomFPN(num_classes=3)\n","backbone_fpn.out_channels = 256\n","\n","# anchor_generator = AnchorGenerator(\n","#     sizes=((32, 64, 128, 256),),\n","#     aspect_ratios=((0.5, 1.0, 2.0),)\n","# )\n","\n","anchor_sizes = ((16,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=[\"ll_feats\",\n","                  \"ml_1_feats\",\n","                  \"ml_2_feats\",\n","                  \"hl_feats\"],\n","    output_size=7, #output size for pooled region\n","    sampling_ratio=2 #sampling ratio for ROIAlign\n",")\n","\n","fpn_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone_fpn,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=3 #3 classes +1 to account for background\n","fpn_model_wrapper.override_prediction_heads(num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43486,"status":"aborted","timestamp":1743283640217,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"8Ffysvine5S-"},"outputs":[],"source":["fpn_model_wrapper.model.to(device)\n","# optimizer = torch.optim.Adam(cbam_model_wrapper.model.parameters(), lr=0.0005)\n","optimizer = torch.optim.Adam(fpn_model_wrapper.model.parameters(), lr=0.00025)\n","num_epochs = 15\n","model_pipeline(fpn_model_wrapper , optimizer, num_epochs, train_dataset_loader,\n","               val_dataset_loader, test_dataset_loader, device,\n","               True, \"Custom Backbone Faster R-CNN Tracker\")"]},{"cell_type":"markdown","source":["### FPN With Dynamic texture attention"],"metadata":{"id":"TLA1SU2zOC3w"}},{"cell_type":"code","source":["import torch.nn as nn\n","from functools import partial\n","from torchvision.ops.misc import Conv2dNormActivation\n","\n","class FPNDynamicTextureAttention(nn.Module):\n","  def __init__(self,\n","               ll_feat_input_size: int,\n","               hl_feat_input_size: int):\n","\n","    super().__init__()\n","\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","\n","    # conv blocks to upsample and downsample as needed\n","    # tried using conv blocks here for up/downsamplers but made performance worse\n","    self.upsampler = nn.Conv2d(in_channels= 48,\n","                               out_channels=256,\n","                               kernel_size=3,\n","                               )\n","\n","    self.downsampler = nn.Conv2d(in_channels= 256,\n","                               out_channels=48,\n","                               kernel_size=3\n","\n","                             )\n","\n","    self.intermediate_upsampler = nn.Conv2d(in_channels=32, out_channels=48, kernel_size=3)\n","    self.ll_final_upsampler = nn.Conv2d(in_channels=48, out_channels=256, kernel_size=3)\n","\n","    # feature refiner for lowerl level features\n","    self.feature_refiner = nn.Sequential(\n","        nn.AvgPool2d(kernel_size=1),\n","        nn.LazyConv2d(out_channels=96, kernel_size=1),\n","        nn.ReLU(),\n","        nn.LazyConv2d(out_channels=96, kernel_size=1), #out_channels used to be ll_feat_input_size\n","        nn.Sigmoid()\n","    )\n","\n","    self.dilated_conv_1 = nn.Conv2d(in_channels= ll_feat_input_size,\n","                                    out_channels=ll_feat_input_size,\n","                                    kernel_size=3,\n","                                    dilation=1,\n","                                    stride=1\n","                                  )\n","\n","    self.dilated_conv_2 = nn.Conv2d(in_channels= ll_feat_input_size,\n","                                    out_channels=ll_feat_input_size,\n","                                    kernel_size=3,\n","                                    dilation=2,\n","                                    stride=1,\n","                                  )\n","    self.dilated_conv_3 = nn.Conv2d(in_channels= ll_feat_input_size,\n","                                    out_channels=ll_feat_input_size,\n","                                    kernel_size=3,\n","                                    dilation=3,\n","                                    stride=1,\n","                                  )\n","\n","    self.sigmoid = nn.Sigmoid()\n","\n","    # smoothers\n","    self.ll_smoother = ConvBlock(\n","        in_channels=96,\n","        out_channels=96,\n","    )\n","\n","    self.ll_multiplied_smoother = ConvBlock(\n","        in_channels=96,\n","        out_channels=96,\n","    )\n","\n","    self.ll_added_smoother = ConvBlock(\n","        in_channels=96,\n","        out_channels=96,\n","    )\n","\n","    self.refined_ll_smoother = ConvBlock(\n","        in_channels=48,\n","        out_channels=48,\n","    )\n","\n","    self.intermediate_feats_smoother = ConvBlock(\n","        in_channels=256,\n","        out_channels=256,\n","    )\n","\n","    self.refined_hl_smoother = ConvBlock(\n","        in_channels=256,\n","        out_channels=256,\n","    )\n","\n","    self.hl_ll_multiplied_smoother = ConvBlock(\n","        in_channels=48,\n","        out_channels=48,\n","    )\n","\n","    self.hl_ll_subtracted_smoother = ConvBlock(\n","        in_channels=48,\n","        out_channels=48,\n","    )\n","\n","  def forward(self, ll_feat_map, hl_feat_map):\n","\n","    ll_dilated_1 = self.dilated_conv_1(ll_feat_map)\n","    ll_dilated_2 = self.dilated_conv_2(ll_feat_map)\n","    ll_dilated_3 = self.dilated_conv_3(ll_feat_map)\n","    # increase height and width of ll_dilated_2 and 3 so that they can be concatenated with ll_dilated_1\n","    upsampled_ll_dilated_2 = F.interpolate(\n","        ll_dilated_2,\n","        size=[ll_dilated_1.size(2), ll_dilated_1.size(3)],\n","        mode=\"nearest\",\n","    )\n","    upsampled_ll_dilated_3 = F.interpolate(\n","        ll_dilated_3,\n","        size=[ll_dilated_1.size(2), ll_dilated_1.size(3)],\n","        mode=\"nearest\",\n","    )\n","\n","    #first dimension is batch size\n","    ll_x = torch.cat((ll_dilated_1, upsampled_ll_dilated_2, upsampled_ll_dilated_3), dim=1) #is dim 1 ?\n","    # smoothes ll_x\n","    ll_x = self.ll_smoother(ll_x)\n","\n","    ll_x_refined = self.feature_refiner(ll_x)\n","    ll_x_combined = ll_x * ll_x_refined\n","    #smoothing out artificats introduced by F.interpolate\n","    ll_x_combined = self.ll_multiplied_smoother(ll_x_combined)\n","\n","    #split & sum\n","    half_1, half_2 = torch.split(ll_x_combined, ll_x_combined.size(dim=1)//2, dim=1)\n","    ll_combined_feats = half_1 + half_2\n","    #smoothing out artificats introduced by F.interpolate\n","    ll_x_combined = self.ll_added_smoother(ll_x_combined)\n","\n","    upsampled_ll_combined = F.interpolate(\n","        ll_combined_feats,\n","        size=[ll_feat_map.size(2), ll_feat_map.size(3)],\n","        mode=\"nearest\",\n","    )\n","\n","    upsampled_ll_feat_map = self.intermediate_upsampler(ll_feat_map)\n","    upsampled_ll_feat_map = F.interpolate(\n","        upsampled_ll_feat_map,\n","        size=[upsampled_ll_combined.size(2), upsampled_ll_combined.size(3)],\n","        mode=\"nearest\",\n","    )\n","    refined_ll_feat_map = upsampled_ll_feat_map * upsampled_ll_combined\n","    refined_ll_feat_map = self.refined_ll_smoother(refined_ll_feat_map)\n","\n","    upsampled_hl = F.interpolate(\n","        hl_feat_map,\n","        size=[refined_ll_feat_map.size(2), refined_ll_feat_map.size(3)],\n","        mode=\"nearest\",\n","    )\n","\n","    usampled_hl = self.sigmoid(upsampled_hl)\n","    upsampled_refined_ll = self.upsampler(refined_ll_feat_map)\n","\n","    # upsample height and width of conv output\n","    upsampled_refined_ll = F.interpolate(\n","        upsampled_refined_ll,\n","        size=[upsampled_hl.size(2), upsampled_hl.size(3)],\n","        mode=\"nearest\",\n","    )\n","    intermediate_feat_map = upsampled_refined_ll * upsampled_hl\n","    intermediate_feat_map = self.intermediate_feats_smoother(intermediate_feat_map)\n","\n","    downsampled_intermediate_feats = F.interpolate(\n","        intermediate_feat_map,\n","        size=[hl_feat_map.size(2), hl_feat_map.size(3)],\n","        mode=\"nearest\",\n","    )\n","    refined_hl_feat_map = downsampled_intermediate_feats + hl_feat_map\n","    refined_hl_feat_map = self.refined_hl_smoother(refined_hl_feat_map)\n","\n","    # downsample ready for element wise multiplication\n","    downsampled_intermediate_feats = self.downsampler(intermediate_feat_map)\n","\n","    downsampled_intermediate_feats = F.interpolate(\n","        downsampled_intermediate_feats,\n","        size=[refined_ll_feat_map.size(2), refined_ll_feat_map.size(3)],\n","        mode=\"nearest\",\n","    )\n","    ll_hl_combined_feat_map = refined_ll_feat_map * downsampled_intermediate_feats\n","    ll_hl_combined_feat_map = self.hl_ll_multiplied_smoother(ll_hl_combined_feat_map)\n","\n","    subtracted_ll_feat_map = refined_ll_feat_map - ll_hl_combined_feat_map\n","    subtracted_ll_feat_map = self.hl_ll_subtracted_smoother(subtracted_ll_feat_map)\n","\n","    #upsamople the subtracted_ll_feat_map so that it can be processed by the RPN\n","    # RPN requires all feature maps to have the same number of channels\n","    final_ll_feat_map = self.ll_final_upsampler(subtracted_ll_feat_map)\n","    return [final_ll_feat_map, refined_hl_feat_map]"],"metadata":{"id":"lhWa5mVr4wQo","executionInfo":{"status":"aborted","timestamp":1743283640218,"user_tz":0,"elapsed":43487,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResNetCustomFPNWithAtt(nn.Module):\n","    \"\"\"\"\n","    Initialises a custom network that is a blend of ResNet and CBAM Attention\n","\n","    Attributes:\n","    - block 1 is created in line with definition of original ResNet research paper\n","    - blocks 2-4 are composed of: ResNet \"conv block\" + ResNet \"Identity Block\" + CBAM attention module\n","    - block 5 is composed of: ResNet \"conv block\" + ResNet \"Identity Block\"\n","\n","    \"\"\"\n","    def __init__(self,\n","                 input_channels: int =3, # datasets imgs are in colour so default input channels is 3\n","                 num_classes: int =4) -> None:\n","\n","        \"\"\"\n","        Initalises 5 blocks ready for the forward pass\n","\n","        Args:\n","        input_channels: number of channles in the input image (3 for color imgs)\n","        num_classes: the number of classes in the dataset\n","\n","        Returns: None\n","        \"\"\"\n","        super().__init__()\n","        # a conv layer with kernel=3, stride=1, padding=1 is known as 'same convolution' meaning the output size won't change\n","        # first block of resnet is defined as follows\n","        self.block1 = nn.Sequential(\n","            Conv2d(in_channels=input_channels,\n","                      out_channels=16, #output channels = number of filters\n","                      kernel_size=7,\n","                      stride=2,\n","                      padding=3),\n","\n","            nn.BatchNorm2d(16),\n","            LeakyReLU(),\n","            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        )\n","\n","        self.block2 = initialise_resnet_block(num_filters=32,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1,\n","                                              cbam_module=ScaledCBAM(in_channels=32, scaling_parameter=nn.Parameter(torch.zeros(1))))\n","\n","        self.block3 = initialise_resnet_block(num_filters=64,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1,\n","                                              cbam_module=CBAM(in_channels=64))\n","\n","        self.block4 = initialise_resnet_block(num_filters=128,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1,\n","                                              cbam_module=CBAM(in_channels=128))\n","\n","        self.block5 = initialise_resnet_block(num_filters=256,\n","                                              kernel_size=3,\n","                                              stride=2,\n","                                              padding=1)\n","\n","        self.fpn = FPNDynamicTextureAttention(ll_feat_input_size = 32, hl_feat_input_size=256) #should probably have output_channels =256\n","\n","        self.ml_1_upsampler = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3)\n","        self.ml_2_upsampler = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3)\n","\n","        # TEST\n","        # self.dropout = nn.Dropout(p=0.25)\n","\n","    def forward(self, x: torch.tensor) -> torch.tensor :\n","\n","        # test = x.view(x.size(0), -1)\n","        # print('block 1 input shape')\n","        # print(test.shape)\n","\n","        x = self.block1(x)\n","\n","\n","        # x = self.block2(x)\n","        ll_feats = self.block2(x)\n","\n","\n","        # x = self.block3(x)\n","        ml_1_feats = self.block3(ll_feats)\n","\n","\n","        # x = self.block4(x)\n","        ml_2_feats = self.block4(ml_1_feats)\n","\n","        # x = self.block5(x)\n","        hl_feats = self.block5(ml_2_feats)\n","\n","        x = self.fpn(ll_feat_map = ll_feats, hl_feat_map = hl_feats)\n","\n","        ml_1_upsampled = self.ml_1_upsampler(ml_1_feats)\n","        ml_2_upsampled = self.ml_2_upsampler(ml_2_feats)\n","        feat_maps = [x[0], ml_1_upsampled, ml_2_upsampled, x[1]]\n","\n","\n","        # for item in feat_maps:\n","        #   print(item.size())\n","\n","        # torch.Size([16, 256, 98, 134])\n","        # torch.Size([16, 256, 48, 66])\n","        # torch.Size([16, 256, 23, 32])\n","        # torch.Size([16, 256, 13, 17])\n","\n","        # torch.Size([16, 256, 98, 134])\n","        # torch.Size([16, 64, 50, 68])\n","        # torch.Size([16, 128, 25, 34])\n","        # torch.Size([16, 256, 100, 136])\n","\n","        return OrderedDict([(k, v) for k, v in zip([\"ll_feats\",\n","                                                    \"ml_1_feats\",\n","                                                    \"ml_2_feats\",\n","                                                    \"hl_feats\"\n","                                                    ], feat_maps)])"],"metadata":{"id":"4Kv_R6GQgNqq","executionInfo":{"status":"aborted","timestamp":1743283640218,"user_tz":0,"elapsed":43486,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJvyxkQigNqr","executionInfo":{"status":"aborted","timestamp":1743283640218,"user_tz":0,"elapsed":43485,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["backbone_fpn = ResNetCustomFPNWithAtt(num_classes=3)\n","#specify outchannels as it used as inchannels for the RPNHeads\n","backbone_fpn.out_channels = 256\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","\n","anchor_sizes = ((16,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","# debugging\n","anchor_sizes = ((16,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=[\"ll_feats\",\n","                  \"ml_1_feats\",\n","                  \"ml_2_feats\",\n","                  \"hl_feats\"],\n","    output_size=7, #output size for pooled region\n","    sampling_ratio=2 #sampling ratio for ROIAlign\n",")\n","\n","fpn_att_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone_fpn,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=3 #3 classes +1 to account for background\n","fpn_att_model_wrapper.override_prediction_heads(num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43484,"status":"aborted","timestamp":1743283640218,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"yWASfDn0gNqr"},"outputs":[],"source":["fpn_att_model_wrapper.model.to(device)\n","# optimizer = torch.optim.Adam(cbam_model_wrapper.model.parameters(), lr=0.0005)\n","optimizer = torch.optim.Adam(fpn_att_model_wrapper.model.parameters(), lr=0.00025)\n","num_epochs = 15\n","model_pipeline(fpn_att_model_wrapper , optimizer, num_epochs, train_dataset_loader,\n","               val_dataset_loader, test_dataset_loader, device,\n","               False, \"Custom Backbone Faster R-CNN Tracker\")"]},{"cell_type":"code","source":["# fpn_DTA_weights_path = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/saved_models/custom_fpn_with_DTA_model_weights.pth'\n","# torch.save(fpn_att_model_wrapper.model.state_dict(), fpn_DTA_weights_path)"],"metadata":{"id":"deTZtA4-_2pE","executionInfo":{"status":"aborted","timestamp":1743283640220,"user_tz":0,"elapsed":43485,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model_robustness_curve(anchor_generator=anchor_generator,\n","#                        roi_pooler=roi_pooler,\n","#                        backbone=backbone_fpn,\n","#                        train_dataset=train_dataset,\n","#                        list_dataset_subsamples=[0.5,0.8,1],\n","#                        num_epochs=5,\n","#                        val_dataset_loader=val_dataset_loader,\n","#                        test_dataset_loader=test_dataset_loader)"],"metadata":{"id":"QAbrGYoyn31S","executionInfo":{"status":"aborted","timestamp":1743283640220,"user_tz":0,"elapsed":43485,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Custom Backbone 2: Biomimetic Blocks + CBAM + FPN:"],"metadata":{"id":"_Ip0WsZUOU2x"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ShallowFovea(nn.Module):\n","  def __init__(self, num_channels: int, out_channels: int):\n","    \"\"\"Implements the Shallow Fovea Module as proposed by Chen et Lin\n","\n","      Implements the biomimetic convolutional block introduced by the research paper:\n","      https://www.sciencedirect.com/science/article/abs/pii/S1051200424005815\n","      This block mimics the visual pathways in eagle eyes in order to detect small\n","      objects in aerial imagery. The large kernel sizes of this module simulate the\n","      wide field of view of an eagle.\n","      This module utilises branches (similar to the Inception model)\n","      Batch normalisation and activation functions are added to each conv layer\n","      using a combination of functools.parital() and torch.Conv2dNormActivation.\n","\n","      Args:\n","          num_channels: number of channels for the first layer\n","          out_channels: the number of channels each output feature map will have\n","\n","      Returns:\n","          a torch tensor\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","    self.branch_1 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=7),\n","    )\n","\n","    self.branch_2 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(1,7)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(7,1)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=7, dilation=3),\n","    )\n","\n","    self.branch_3 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,1)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(1,3)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=7, dilation=3),\n","    )\n","\n","    self.branch_4 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=out_channels, kernel_size=1),\n","    )\n","\n","    self.downsampler = nn.LazyConv2d(out_channels=out_channels, kernel_size=3)\n","\n","\n","  def forward(self, x):\n","    branch_1 = self.branch_1(x)\n","    branch_2 = self.branch_2(x)\n","    branch_3 = self.branch_3(x)\n","    branch_4 = self.branch_4(x)\n","\n","    upsampled_branch_1 = F.interpolate(\n","        branch_1,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_1 #remove intermediate calculation from GPU RAM clear\n","    upsampled_branch_2 = F.interpolate(\n","        branch_2,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_2 #remove intermediate calculation from GPU RAM clear\n","\n","    branch_concat = torch.cat( (upsampled_branch_1,\n","                                upsampled_branch_2,\n","                                branch_3), dim=1 ) #concat on first dimension to allow for different number of channels\n","    del upsampled_branch_1, upsampled_branch_2, branch_3\n","\n","\n","    # prep ready for element wise multiplication\n","    downsampled_branch_concat = self.downsampler(branch_concat)\n","    del branch_concat #remove intermediate calculation from GPU RAM clear\n","\n","    resized_branch_concat = F.interpolate(\n","        downsampled_branch_concat,\n","        size=[branch_4.size(2), branch_4.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    result = branch_4 * resized_branch_concat\n","    del branch_4, resized_branch_concat\n","    return result"],"metadata":{"id":"Rx7QRmlTG1qb","executionInfo":{"status":"ok","timestamp":1743283753076,"user_tz":0,"elapsed":26,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class DeepFovea(nn.Module):\n","  def __init__(self, num_channels: int, out_channels: int):\n","    \"\"\"Implements the Deep Fovea Module as proposed by Chen et Lin\n","\n","      Implements the biomimetic convolutional block introduced by the research paper:\n","      https://www.sciencedirect.com/science/article/abs/pii/S1051200424005815\n","      This block mimics the visual pathways in eagle eyes in order to detect small\n","      objects in aerial imagery. The small kernel sizes of this module preserve the\n","      pixel data of small objects - effectively simulating the magnification mechanism\n","      in eagle eyes which allows the to spot small prey during flight.\n","      This module utilises branches (similar to the Inception model)\n","      Batch normalisation and activation functions are added to each conv layer\n","      using a combination of functools.parital() and torch.Conv2dNormActivation.\n","\n","      Args:\n","          num_channels: number of channels for the first layer\n","          out_channels: the number of channels each output feature map will have\n","\n","      Returns:\n","          a torch tensor\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","    self.branch_1 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=3),\n","    )\n","\n","    self.branch_2 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(1,3)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,1)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=3, dilation=3),\n","    )\n","\n","    self.branch_3 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=1),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(3,1)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=(1,3)),\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels, kernel_size=3, dilation=3),\n","    )\n","\n","    self.branch_4 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=out_channels, kernel_size=1),\n","    )\n","\n","    self.downsampler = nn.LazyConv2d(out_channels=out_channels, kernel_size=3)\n","\n","\n","  def forward(self, x):\n","    branch_1 = self.branch_1(x)\n","    branch_2 = self.branch_2(x)\n","    branch_3 = self.branch_3(x)\n","    branch_4 = self.branch_4(x)\n","\n","    upsampled_branch_1 = F.interpolate(\n","        branch_1,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_1 #remove intermediate calculation from GPU RAM clear\n","    upsampled_branch_2 = F.interpolate(\n","        branch_2,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_2 #remove intermediate calculation from GPU RAM clear\n","\n","    branch_concat = torch.cat( (upsampled_branch_1,\n","                                upsampled_branch_2,\n","                                branch_3), dim=1 ) #concat on first dimension to allow for different number of channels\n","    del upsampled_branch_1, upsampled_branch_2, branch_3\n","\n","    # prep ready for element wise multiplication\n","    downsampled_branch_concat = self.downsampler(branch_concat)\n","    del branch_concat #remove intermediate calculation from GPU RAM clear\n","\n","    resized_branch_concat = F.interpolate(\n","        downsampled_branch_concat,\n","        size=[branch_4.size(2), branch_4.size(3)],\n","        mode=\"bilinear\",\n","    )\n","\n","    result = branch_4 * resized_branch_concat\n","    del branch_4, resized_branch_concat\n","    return result"],"metadata":{"id":"DIsXXyK3D72r","executionInfo":{"status":"ok","timestamp":1743283753146,"user_tz":0,"elapsed":4,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# TO DO:\n","# - experiment with residual connections\n","from functools import partial\n","from torchvision.ops import Conv2dNormActivation\n","\n","class FoveaBackbone(nn.Module):\n","  def __init__(self, in_channels: int):\n","    \"\"\"Integrates ShallowFovea and DeepFovea modules, CBAM attention and an FPN into\n","        a backbone ready to be integrated into Faster R-CNN\n","\n","      Each block of (ShallowFovea, DualFovea) is followed by a CBAM attention mechanism\n","      (Convolutional Block Attention Module). The feature maps produced at each stage\n","      in the network are then fed through a top-down FPN (Feature Pyramid Network)\n","      which infuses the lower level features with the semantic information extract\n","      from deeper convolutional layers.\n","\n","      Args:\n","          in_channels: the number of channels to be passed to the first block\n","\n","      Returns:\n","          a list of feature maps (ready to be passed to the Faster R-CNN Region\n","          Proposal Network)\n","\n","      Raises:\n","          None\n","      \"\"\"\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","    #stride not mention in EVM paper but needed for memory management\n","    self.block_1 = nn.Sequential(\n","        ConvBlock(in_channels=in_channels, out_channels=32, kernel_size=3, stride=2),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","    self.shallow_fovea_1 = ShallowFovea(num_channels=32, out_channels=64)\n","    self.deep_fovea_1 = DeepFovea(num_channels=32, out_channels=64)\n","\n","    self.shallow_fovea_2 = ShallowFovea(num_channels=64, out_channels=128)\n","    self.deep_fovea_2 = DeepFovea(num_channels=64, out_channels=128)\n","\n","    self.shallow_fovea_3 = ShallowFovea(num_channels=128, out_channels=256)\n","    self.deep_fovea_3 = DeepFovea(num_channels=128, out_channels=256)\n","\n","    self.shallow_fovea_4 = ShallowFovea(num_channels=256, out_channels=512)\n","    self.deep_fovea_4 = DeepFovea(num_channels=256, out_channels=512)\n","\n","    self.max_pooler = nn.MaxPool2d(kernel_size=2)\n","    self.fpn = FPN(in_channels=[64, 128, 256, 512], out_channels=512)\n","    # self.fpn = FPNDynamicTextureAttention(ll_feat_input_size=64, hl_feat_input_size=512)\n","\n","    self.cbam_scaler_1 = nn.Parameter(torch.zeros(1))\n","    self.cbam_1 = ScaledCBAM(in_channels=64, scaling_parameter=self.cbam_scaler_1)\n","    self.cbam_scaler_2 = nn.Parameter(torch.zeros(1))\n","    self.cbam_2 = ScaledCBAM(in_channels=128, scaling_parameter=self.cbam_scaler_2)\n","    self.cbam_scaler_3 = nn.Parameter(torch.zeros(1))\n","    self.cbam_3 = ScaledCBAM(in_channels=256, scaling_parameter=self.cbam_scaler_3)\n","    self.cbam_scaler_4 = nn.Parameter(torch.zeros(1))\n","    self.cbam_4 = ScaledCBAM(in_channels=512, scaling_parameter=self.cbam_scaler_4)\n","\n","  def forward(self, x):\n","    x = self.block_1(x)\n","\n","    shallow_feats_1 = self.shallow_fovea_1(x)\n","    deep_feats_1 = self.deep_fovea_1(x)\n","    stage_1_results = shallow_feats_1 + deep_feats_1\n","    stage_1_results_pooled = self.max_pooler(stage_1_results) #reduce computational load for successive layers\n","    stage_1_results_w_cbam = self.cbam_1(stage_1_results)\n","    # memory management\n","    del shallow_feats_1, deep_feats_1, stage_1_results\n","\n","    shallow_feats_2 = self.shallow_fovea_2(stage_1_results_pooled)\n","    deep_feats_2 = self.deep_fovea_2(stage_1_results_pooled)\n","    stage_2_results = shallow_feats_2 + deep_feats_2\n","    stage_2_results_pooled = self.max_pooler(stage_2_results) #reduce computational load for successive layers\n","    stage_2_results_w_cbam = self.cbam_2(stage_2_results)\n","    # memory management\n","    del shallow_feats_2, deep_feats_2, stage_2_results, stage_1_results_pooled,\n","\n","    shallow_feats_3 = self.shallow_fovea_3(stage_2_results_pooled)\n","    deep_feats_3 = self.deep_fovea_3(stage_2_results_pooled)\n","    stage_3_results = shallow_feats_3 + deep_feats_3\n","    stage_3_results_pooled = self.max_pooler(stage_3_results) #reduce computational load for successive layers\n","    stage_3_results_w_cbam = self.cbam_3(stage_3_results)\n","    # memory management\n","    del shallow_feats_3, deep_feats_3, stage_3_results, stage_2_results_pooled,\n","\n","    shallow_feats_4 = self.shallow_fovea_4(stage_3_results_pooled)\n","    deep_feats_4 = self.deep_fovea_4(stage_3_results_pooled)\n","    stage_4_results = shallow_feats_4 + deep_feats_4\n","    stage_4_results_w_cbam = self.cbam_4(stage_4_results)\n","    # memory management\n","    del shallow_feats_4, deep_feats_4, stage_4_results, stage_3_results_pooled,\n","\n","    feat_maps = self.fpn(hl_features = stage_4_results_w_cbam,\n","                        ml_1_features= stage_3_results_w_cbam,\n","                        ml_2_features = stage_2_results_w_cbam,\n","                        ll_features= stage_1_results_w_cbam)\n","\n","    return OrderedDict([(k, v) for k, v in zip([\"ll_feats\",\n","                                                \"ml_1_feats\",\n","                                                \"ml_2_feats\",\n","                                                \"hl_feats\"], feat_maps)])"],"metadata":{"id":"7trtihtbD8PK","executionInfo":{"status":"ok","timestamp":1743283753529,"user_tz":0,"elapsed":158,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","backbone = FoveaBackbone(3)\n","backbone.out_channels = 512\n","\n","anchor_sizes = ((8,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    # specify the feature maps to be used by the region proposal network\n","    featmap_names=[\"ll_feats\",\n","                    \"ml_1_feats\",\n","                    \"ml_2_feats\",\n","                    \"hl_feats\"],\n","    output_size=7,\n","    sampling_ratio=2\n",")\n","\n","fovea_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=3 #3 classes +1 to account for background\n","fovea_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"id":"zyyeDgAsEeXW","executionInfo":{"status":"ok","timestamp":1743283754110,"user_tz":0,"elapsed":510,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["fovea_model_wrapper.model.to(device)\n","# optimizer = torch.optim.SGD(model.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.Adam(fovea_model_wrapper.model.parameters(), lr=0.0005)\n","num_epochs = 15\n","model_pipeline(fovea_model_wrapper , optimizer, num_epochs, train_dataset_loader,\n","               val_dataset_loader, test_dataset_loader, device,\n","               True, \"Fovea Tracker\")"],"metadata":{"id":"vB9capf-EkWD","executionInfo":{"status":"aborted","timestamp":1743283640224,"user_tz":0,"elapsed":43485,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cross validation"],"metadata":{"id":"KLScwvV4DtUE"}},{"cell_type":"code","source":["backbone = ResNetCBAM(num_classes=3)\n","backbone.out_channels = 256\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256, 512),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=['0'],\n","    output_size=7, #output size for pooled region\n","    sampling_ratio=2 #sampling ratio for ROIAlign\n",")\n","\n","k_fold_results = k_fold_training(train_dataset=train_dataset,\n","                                 n_splits=3,\n","                                 random_state=3,\n","                                 batch_size=16,\n","                                 backbone=backbone,\n","                                 anchor_generator=anchor_generator,\n","                                 roi_pooler=roi_pooler,\n","                                 num_epochs=15)"],"metadata":{"id":"OO6rplmWEhw1","executionInfo":{"status":"aborted","timestamp":1743283640226,"user_tz":0,"elapsed":43486,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Export Biomimetic backbone (Custom backbone 2) to ONNX format"],"metadata":{"id":"VMjszF3pJ8Rk"}},{"cell_type":"code","source":["!pip install --upgrade onnx onnxscript"],"metadata":{"id":"v_WFWI_sJ8Fy","executionInfo":{"status":"aborted","timestamp":1743283640226,"user_tz":0,"elapsed":43485,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_img = train_dataset[0][0]\n","backbone = FoveaBackbone(3)\n","backbone(sample_img)\n","onnx_program = torch.onnx.export(backbone, sample_img, dynamo=True)"],"metadata":{"id":"G9zTfQB4KC--","executionInfo":{"status":"aborted","timestamp":1743283640227,"user_tz":0,"elapsed":43485,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEshHxxT2nOE"},"source":["## Pre-train custom backbone on large-scale dataset:"]},{"cell_type":"markdown","source":["### Pre-training on the SODA-A Dataset:"],"metadata":{"id":"KNJjXmEonMVC"}},{"cell_type":"code","source":["from torch.utils.data import random_split\n","SODA_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/SODA-A resized/resized_using_fx\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/SODA-A resized/SODA_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","SODA_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/SODA-A resized/smallest_resizing\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/SODA-A resized/SODA_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","SODA_train_dataset, SODA_val_dataset, SODA_test_dataset = random_split(SODA_dataset, [0.8, 0.1, 0.1])"],"metadata":{"executionInfo":{"status":"ok","timestamp":1743283728039,"user_tz":0,"elapsed":42846,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"id":"XDBmHmyLceoU"},"execution_count":38,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1743283728049,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"-3D7HEaAceoV"},"outputs":[],"source":["SODA_train_dataset_loader = torch.utils.data.DataLoader(SODA_train_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=6,\n","                                                   shuffle=True,\n","                                                  #  pin_memory=True,\n","                                                  # num_workers=2,\n","                                                  #  prefetch_factor=2\n","                                                   )\n","\n","SODA_val_dataset_loader = torch.utils.data.DataLoader(SODA_val_dataset, collate_fn=lambda x: x, batch_size=6,\n","                                                  ) #num_workers=2\n","\n","SODA_test_dataset_loader = torch.utils.data.DataLoader(SODA_test_dataset, collate_fn=lambda x: x, batch_size=6,\n","                                                  ) #num_workers=2"]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","backbone = FoveaBackbone(3)\n","backbone.out_channels = 512\n","\n","anchor_sizes = ((8,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    # specify the feature maps to be used by the region proposal network\n","    featmap_names=[\"ll_feats\",\n","                    \"ml_1_feats\",\n","                    \"ml_2_feats\",\n","                    \"hl_feats\"],\n","    output_size=7,\n","    sampling_ratio=2\n",")\n","\n","SODA_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=10, #SODA-A dataset has 9 classes (exlcuding the 'ignore class')\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=10 #9 classes +1 to account for background\n","SODA_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1743283766916,"user_tz":0,"elapsed":482,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"id":"td1sxBPocno3"},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","SODA_model_wrapper.model.to(device)\n","# optimizer = torch.optim.SGD(model.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.Adam(SODA_model_wrapper.model.parameters(), lr=0.0005)\n","num_epochs = 5\n","\n","\n","start = time.time()\n","# model_pipeline(SODA_model_wrapper , optimizer, num_epochs, SODA_train_dataset_loader,\n","#                SODA_val_dataset_loader, SODA_test_dataset_loader, device,\n","#                False, \"Fovea Tracker\")\n","\n","SODA_model_wrapper.train(num_epochs=15,\n","                         optimizer=optimizer,\n","                         train_dataset=SODA_train_dataset_loader,\n","                         is_console_logging=True,\n","                         calc_running_loss=False)\n","end = time.time()\n","print(end - start)"],"metadata":{"executionInfo":{"status":"error","timestamp":1743284942725,"user_tz":0,"elapsed":1175487,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"colab":{"base_uri":"https://localhost:8080/","height":755},"id":"UqMzzQkTcno4","outputId":"020b05b7-bcb4-49bc-caa4-c621b394cecb"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 of 15\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n","batch complete\n"]},{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 7.65 GiB. GPU 0 has a total capacity of 39.56 GiB of which 6.70 GiB is free. Process 2909 has 32.85 GiB memory in use. Of the allocated memory 31.26 GiB is allocated by PyTorch, and 278.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-9a973600e3db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#                False, \"Fovea Tracker\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m SODA_model_wrapper.train(num_epochs=15,\n\u001b[0m\u001b[1;32m     15\u001b[0m                          \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                          \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSODA_train_dataset_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-28305c756508>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, optimizer, train_dataset, val_dataset, is_console_logging, calc_running_loss)\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#allow time for GPU RAM to be cleared before proceeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m           \u001b[0mbatch_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m           \u001b[0;32mdel\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# memory management\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m           \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#allow time for GPU RAM to be cleared before proceeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-28305c756508>\u001b[0m in \u001b[0;36m__train_batch\u001b[0;34m(self, prepped_batch, optimizer)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# use mixed precision to address CUDA out of memory error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m       \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"targets should not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_gt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_targets_to_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_gt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             loss_objectness, loss_rpn_box_reg = self.compute_loss(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36massign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mlabels_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mmatch_quality_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_quality_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0;31m# get the targets corresponding GT for each proposal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_iou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0minter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_box_inter_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mrb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [N,M]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 7.65 GiB. GPU 0 has a total capacity of 39.56 GiB of which 6.70 GiB is free. Process 2909 has 32.85 GiB memory in use. Of the allocated memory 31.26 GiB is allocated by PyTorch, and 278.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}]},{"cell_type":"code","source":["# save weights\n","SODA_DOTA_pretrained_weights_path = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/saved_models/SODA_pretrained_weights.pth'\n","torch.save(SODA_model_wrapper.model.state_dict(), SODA_DOTA_pretrained_weights_path)"],"metadata":{"id":"wkCUtO4mcpFl","executionInfo":{"status":"aborted","timestamp":1743283640238,"user_tz":0,"elapsed":43492,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"KZvrbhf6yNlj","executionInfo":{"status":"aborted","timestamp":1743283640240,"user_tz":0,"elapsed":43493,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### UAVVaste"],"metadata":{"id":"9aY-v7ez9PQ3"}},{"cell_type":"code","source":["# !pip install roboflow supervision --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i8iWvLWi9QpM","executionInfo":{"status":"ok","timestamp":1743286251925,"user_tz":0,"elapsed":6324,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"outputId":"a21e7d4a-c8e4-4b02-fa50-05e4053a93e4"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m118.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# /content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/UAVVaste - prertraining.v1i.yolov8/test"],"metadata":{"id":"tiyYytJF9Sy6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pre-training on the DOTA dataset:"],"metadata":{"id":"oGMXWfamU4vC"}},{"cell_type":"code","source":["# DOTA\n","DOTA_train_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/DOTA formatted/train/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/DOTA formatted/DOTA_train_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","DOTA_val_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/DOTA formatted/valid/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/DOTA formatted/DOTA_valid_pd.csv\",\n","                              classes_start_at_zero=False)\n","\n","DOTA_test_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/DOTA formatted/test/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/Data for pre-training/DOTA formatted/DOTA_test_pd.csv\",\n","                               classes_start_at_zero=False)"],"metadata":{"executionInfo":{"status":"aborted","timestamp":1743283640240,"user_tz":0,"elapsed":43492,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"id":"jNbEH-vXMx5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43491,"status":"aborted","timestamp":1743283640240,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"0Hmc_x4yMx5j"},"outputs":[],"source":["DOTA_train_dataset_loader = torch.utils.data.DataLoader(DOTA_train_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=4, #was 8 for inception experiments\n","                                                   shuffle=True,\n","                                                  #  pin_memory=True,\n","                                                  num_workers=2,\n","                                                  #  prefetch_factor=2\n","                                                   )\n","\n","\n","DOTA_val_dataset_loader = torch.utils.data.DataLoader(DOTA_val_dataset, collate_fn=lambda x: x, batch_size=4, #was 8 for inception experiments\n","                                                  num_workers=2)\n","\n","DOTA_test_dataset_loader = torch.utils.data.DataLoader(DOTA_test_dataset, collate_fn=lambda x: x, batch_size=4, #was 8 for inception experiments\n","                                                  num_workers=2)"]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","backbone = FoveaBackbone(3)\n","backbone.out_channels = 512\n","\n","anchor_sizes = ((8,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    # specify the feature maps to be used by the region proposal network\n","    featmap_names=[\"ll_feats\",\n","                    \"ml_1_feats\",\n","                    \"ml_2_feats\",\n","                    \"hl_feats\"],\n","    output_size=7,\n","    sampling_ratio=2\n",")\n","\n","DOTA_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=17, #DOTA dataset has 16 classes (exlcuding the 'ignore class')\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=17 #16 classes +1 to account for background\n","DOTA_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"id":"_RClAgz7TUYb","executionInfo":{"status":"aborted","timestamp":1743283640242,"user_tz":0,"elapsed":43492,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","DOTA_model_wrapper.model.to(device)\n","# optimizer = torch.optim.SGD(model.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.Adam(DOTA_model_wrapper.model.parameters(), lr=0.0005)\n","num_epochs = 15\n","\n","\n","start = time.time()\n","model_pipeline(DOTA_model_wrapper , optimizer, num_epochs, DOTA_train_dataset_loader,\n","               DOTA_val_dataset_loader, DOTA_test_dataset_loader, device,\n","               False, \"Fovea Tracker\")\n","end = time.time()\n","print(end - start)"],"metadata":{"executionInfo":{"status":"aborted","timestamp":1743283640245,"user_tz":0,"elapsed":43495,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}},"id":"mWmII90QnQ5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save weights"],"metadata":{"id":"jnmFgyfUoDkq","executionInfo":{"status":"aborted","timestamp":1743283640253,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"TelXR1wEnVgq","executionInfo":{"status":"aborted","timestamp":1743283640254,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine-tune and test pre-trained model:"],"metadata":{"id":"6mz4HPxs2xGE"}},{"cell_type":"code","source":["backbone_fpn = ResNetCBAMCustomFPN(num_classes=3)\n","backbone_fpn.out_channels = 256\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","\n","anchor_sizes = ((16,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=[\"ll_feats\",\n","                  \"ml_1_feats\",\n","                  \"ml_2_feats\",\n","                  \"hl_feats\"],\n","    output_size=7, #output size for pooled region\n","    sampling_ratio=2 #sampling ratio for ROIAlign\n",")\n","\n","fine_tuned_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone_fpn,\n","    num_classes=17,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","# BEFORE OR AFTER OVERRIDING HEADS ?\n","#load pre-trained weights and apply to chosen dataset\n","fine_tuned_model_wrapper.model.load_state_dict(torch.load(DOTA_pretrained_weights_path, weights_only=True))\n","\n","num_classes=3 #2 classes +1 to account for background\n","fine_tuned_model_wrapper.override_prediction_heads(num_classes)\n","\n","# NEED TO EXPERIMENT WITH FREEZING LAYERS HERE"],"metadata":{"id":"RcD7gnm9YdpL","executionInfo":{"status":"aborted","timestamp":1743283640255,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fine_tuned_model_wrapper.model.to(device)\n","# optimizer = torch.optim.Adam(cbam_model_wrapper.model.parameters(), lr=0.0005)\n","optimizer = torch.optim.Adam(fine_tuned_model_wrapper.model.parameters(), lr=0.00025)\n","num_epochs = 15\n","model_pipeline(fine_tuned_model_wrapper, optimizer, num_epochs, train_dataset_loader,\n","               val_dataset_loader, test_dataset_loader, device,\n","               True, \"Custom Backbone Faster R-CNN Tracker\")"],"metadata":{"id":"Iei7MoedZSVM","executionInfo":{"status":"aborted","timestamp":1743283640255,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fine_tuned_DOTA_pretrained_weights_path = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/saved_models/fine_tuned_DOTA_model_weights.pth'\n","# torch.save(fine_tuned_model_wrapper.model.state_dict(), fine_tuned_DOTA_pretrained_weights_path)"],"metadata":{"id":"pC14IpW12lmL","executionInfo":{"status":"aborted","timestamp":1743283640256,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Faster R-CNN Hyperparameter tuning:"],"metadata":{"id":"va34Cr-JeZuz"}},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"AoshXH8qecov","executionInfo":{"status":"aborted","timestamp":1743283640257,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.models.detection import FasterRCNN\n","\n","def objective(trial):\n","  # set search space\n","  lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n","  anchor_sizes = trial.suggest_categorical(\"anchor_sizes\", [\n","                                                              ((4,), (32,), (64,), (128,)),\n","                                                              ((6,), (8,), (64,), (128,)),\n","                                                              ((8,), (32,), (64,), (128,)),\n","                                                              ((8,), (16,), (32,), (64,))\n","\n","                                                              ])\n","  optimizer_choice = trial.suggest_categorical(\"optimizer_choice\", [\"Adam\",\n","                                                                    \"AdamW\", #reportedly good for computer vision tasks\n","                                                                    ])\n","  num_epochs = trial.suggest_int(\"num_epochs\", 15, 20)\n","\n","  # The following hyperparameter adjusts the RPN parameters:\n","  # rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n","  #     considered as positive during training of the RPN.\n","\n","  # rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n","  #     considered as negative during training of the RPN.\n","  rpn_iou_thresh = trial.suggest_categorical(\"rpn_iou_thresh\", [\n","                                                              [0.7, #rpn_fg_iou_thresh\n","                                                               0.3], #rpn_bg_iou_thresh\n","                                                              [0.6, #rpn_fg_iou_thresh\n","                                                               0.4],\n","                                                                ])\n","  # rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training of the RPN\n","  rpn_positive_fraction = trial.suggest_float(\"rpn_positive_fraction\", 0.5, 0.8, step=0.1)\n","\n","\n","\n","\n","\n","  # load data\n","  train_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train/images\",\n","                                df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/train_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","  val_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/valid/images\",\n","                                  df_path=\"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/2 classes - image size test/valid_pd.csv\",\n","                                classes_start_at_zero=False)\n","\n","  # define data loaders\n","  train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=8,\n","                                                   shuffle=True,\n","                                                   pin_memory=True,\n","                                                  num_workers=12,\n","                                                   prefetch_factor=10\n","                                                   )\n","\n","\n","  val_dataset_loader = torch.utils.data.DataLoader(val_dataset, collate_fn=lambda x: x, batch_size=8,\n","                                                    num_workers=6)\n","\n","\n","  # define model\n","  backbone = FoveaBackbone(3)\n","  backbone.out_channels = 512\n","\n","  aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","  anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","  roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","      # specify the feature maps to be used by the region proposal network\n","      featmap_names=[\"ll_feats\",\n","                      \"ml_1_feats\",\n","                      \"ml_2_feats\",\n","                      \"hl_feats\"],\n","      output_size=7,\n","      sampling_ratio=2\n","  )\n","\n","  model = FasterRCNN(\n","      backbone=backbone,\n","      num_classes=3,\n","      box_roi_pool=roi_pooler,\n","      # hyperparameters for the model\n","      rpn_anchor_generator=anchor_generator,\n","      rpn_fg_iou_thresh = rpn_iou_thresh[0],\n","      rpn_bg_iou_thresh = rpn_iou_thresh[1],\n","      rpn_positive_fraction = rpn_positive_fraction,\n","  )\n","  num_classes=3 #3 classes +1 to account for background\n","  in_features = model.roi_heads.box_predictor.cls_score.in_features\n","  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","  # load model weights (from pretraining on the dataset SODA)\n","  # SODA_weights_path = '/content/drive/MyDrive/Colab Notebooks/UoL FP/my_data_model_test/saved_models/SODA_pretrained_weights.pth'\n","  # model.load_state_dict(torch.load(SODA_weights_path, weights_only=True))\n","\n","  device = torch.device(\"cuda:0\")\n","  model.to(device)\n","\n","  if optimizer_choice == \"Adam\":\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","  elif optimizer_choice == \"AdamW\":\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","\n","  model.train()\n","  for epoch in range(num_epochs):\n","      for batch in train_dataset_loader:\n","        batch_dict = format_batch(batch, model)\n","        del batch\n","        images, targets = batch_dict['images'], batch_dict['targets']\n","        loss_dict = model(images, targets)\n","        del images, targets, batch_dict\n","        training_loss = sum(v for v in loss_dict.values())\n","        del loss_dict\n","        # backward pass\n","        optimizer.zero_grad()\n","        training_loss.backward()\n","        optimizer.step()\n","\n","  # calculate validation mAP metric\n","  with torch.no_grad():\n","    metric_map50 = MeanAveragePrecision(box_format=\"xyxy\",iou_type=\"bbox\", iou_thresholds=[0.5], class_metrics=True, extended_summary=True)\n","    #no iou_threshold leads to 50-95 coco mAP\n","    metric_map50_95 = MeanAveragePrecision(box_format=\"xyxy\",iou_type=\"bbox\", class_metrics=True, extended_summary=True)\n","    predictions = []\n","    ground_truths = []\n","    model.eval()\n","    for batch in val_dataset_loader:\n","      formatted_batch = format_batch(batch, model)\n","      images, targets = formatted_batch['images'], formatted_batch['targets']\n","      batch_predictions = model(images)\n","      predictions.extend(batch_predictions)\n","      ground_truths.extend(targets)\n","\n","  # Update metric with predictions and respective ground truth\n","  metric_map50.update(predictions, ground_truths)\n","  metric_map50_95.update(predictions, ground_truths)\n","  # Compute the results\n","  map50_result = metric_map50.compute()\n","  map_50_95_result = metric_map50_95.compute()\n","\n","  free_gpu_ram(train_dataset)\n","  free_gpu_ram(train_dataset_loader)\n","  free_gpu_ram(val_dataset)\n","  free_gpu_ram(val_dataset_loader)\n","\n","  return map50_result['map'], map_50_95_result['map']\n",""],"metadata":{"id":"jQBEEYdWefbF","executionInfo":{"status":"aborted","timestamp":1743283640257,"user_tz":0,"elapsed":43501,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna\n","\n","study = optuna.create_study(direction=['maximize', 'maximize'], #maximise both metrics returned by each trial\n","                            sampler=optuna.samplers.TPESampler(seed=42, multivariate=True), #Bayesian hyperparameter optimization\n","                            )"],"metadata":{"id":"5ts4RJ9EgXy-","executionInfo":{"status":"aborted","timestamp":1743283640257,"user_tz":0,"elapsed":43501,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["study.optimize(objective, n_trials=20)"],"metadata":{"id":"ScwDatpJhIaH","executionInfo":{"status":"aborted","timestamp":1743283640258,"user_tz":0,"elapsed":43501,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_params = study.best_params\n","best_params"],"metadata":{"id":"eRuPDl-vi9Ix","executionInfo":{"status":"aborted","timestamp":1743283640258,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analysis of hyperparameter tuning results:"],"metadata":{"id":"jfgCbXt-l-jA"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","optuna.visualization.plot_optimization_history(study)"],"metadata":{"id":"OY1rhUiti_HY","executionInfo":{"status":"aborted","timestamp":1743283640259,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.plot_parallel_coordinate(study)"],"metadata":{"id":"1SV2Pxl0jtsA","executionInfo":{"status":"aborted","timestamp":1743283640260,"user_tz":0,"elapsed":43501,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optuna.visualization.plot_slice(study, params=['learning_rate'])\n","# light blue represents early trials, dark blue represents trials towards end of search"],"metadata":{"id":"48nT43lkjxnu","executionInfo":{"status":"aborted","timestamp":1743283640262,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparameter importance\n","optuna.visualization.plot_param_importances(study)"],"metadata":{"id":"kOulrR5okj3Q","executionInfo":{"status":"aborted","timestamp":1743283640262,"user_tz":0,"elapsed":43501,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test pre-trained model on test set from different source:"],"metadata":{"id":"r0kvzCBD21SA"}},{"cell_type":"code","source":["alternate_test_dataset = UAVImageDataset(data_dir_imgs=\"/content/drive/MyDrive/UoL Final Project/Project Data/alternate testing sets/annotated_images_labels/images\",\n","                                data_dir_targets=\"/content/drive/MyDrive/UoL Final Project/Project Data/alternate testing sets/annotated_images_labels/labels\",\n","                                        classes_start_at_zero=False)\n","\n","alternate_test_dataset_loader = torch.utils.data.DataLoader(alternate_test_dataset,\n","                                                   collate_fn=lambda x: x,\n","                                                   batch_size=16,\n","                                                   shuffle=True,\n","                                                   pin_memory=True,)"],"metadata":{"id":"U3t9_HYF25BQ","executionInfo":{"status":"aborted","timestamp":1743283640262,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["backbone_fpn = ResNetCBAMCustomFPN(num_classes=3)\n","backbone_fpn.out_channels = 256\n","\n","anchor_generator = AnchorGenerator(\n","    sizes=((32, 64, 128, 256),),\n","    aspect_ratios=((0.5, 1.0, 2.0),)\n",")\n","\n","anchor_sizes = ((16,), (32,), (64,), (128,))\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=[\"ll_feats\",\n","                  \"ml_1_feats\",\n","                  \"ml_2_feats\",\n","                  \"hl_feats\"],\n","    output_size=7, #output size for pooled region\n","    sampling_ratio=2 #sampling ratio for ROIAlign\n",")\n","\n","fine_tuned_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone_fpn,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","#load pre-trained weights and apply to chosen dataset\n","fine_tuned_model_wrapper.model.load_state_dict(torch.load(fine_tuned_DOTA_pretrained_weights_path, weights_only=True))\n","\n","num_classes=3 #2 classes +1 to account for background\n","fine_tuned_model_wrapper.override_prediction_heads(num_classes)\n","fine_tuned_model_wrapper.model.to(device)\n","# test on alternate test set\n","fine_tuned_model_wrapper.test(alternate_test_dataset_loader)"],"metadata":{"id":"DCgKktNV3GMU","executionInfo":{"status":"aborted","timestamp":1743283640265,"user_tz":0,"elapsed":43503,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UEtnxLieozD"},"source":["## Faster R-CNN with SAHI"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43503,"status":"aborted","timestamp":1743283640266,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"yMdQj8QsPsK9"},"outputs":[],"source":["# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","# from sahi.prediction import visualize_object_predictions\n","# import numpy\n","# import sahi\n","\n","# # TEST !!!\n","# def sahi_inference_eval(detection_model: AutoDetectionModel,\n","#                         folder_path: str,\n","#                         slice_h_w: tuple[int, int],\n","#                         overlap_h_w: tuple[float, float],\n","#                         test_dataset_loader: torch.utils.data.dataloader,\n","#                         img_export_path: str) -> dict[str, float]:\n","\n","#   \"\"\"Executes Slicing Aided Hyper Inference (SAHI) for the given model\n","\n","#     SAHI is implemented using the sahi pip library\n","#     Computes and returns the mAP50 and mAP75 values\n","\n","#   Args:\n","#       detection_model: the model wrapped in a sahi AutoDetectionModel class\n","#       folder_path: the path to the folder of images\n","#       slice_h_w: the height and width of slices\n","#       overlap_h_w: the height and width of overlap ratio\n","#       test_dataset_loader: the PyTorch dataset loader\n","#       img_export_path: the path to the folder where resulting image predictions will be saved\n","\n","#   Returns:\n","#       A dictionary of mAP values: mAP50, mAP75 and class wise mAP\n","\n","#   Raises:\n","#       None\n","#   \"\"\"\n","\n","#   img_file_names = os.listdir(path=folder_path)\n","\n","#   map_metric = MeanAveragePrecision(iou_type=\"bbox\", iou_thresholds=[0.5, 0.75], class_metrics=True, extended_summary=True) #\n","#   all_predictions = []\n","#   all_ground_truths = []\n","#   i=0\n","#   for index, batch in enumerate(test_dataset_loader):\n","#     for img, targets, file_path in batch:\n","#       # get sliced predictions\n","#       img_predictions = get_sliced_prediction(\n","#         detection_model=detection_model,\n","#         image = file_path,\n","#         slice_height=slice_h_w[0],\n","#         slice_width=slice_h_w[1],\n","#         overlap_height_ratio=overlap_h_w[0],\n","#         overlap_width_ratio=overlap_h_w[1],\n","#         verbose=0\n","#       )\n","\n","#       # visualise\n","#       img = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)\n","#       img_converted = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","#       numpy_img= numpy.asarray(img_converted)\n","#       visualize_object_predictions(numpy_img,\n","#                                    object_prediction_list=img_predictions.object_prediction_list,\n","#                                    output_dir=img_export_path,\n","#                                    file_name=f\"{i}\",\n","#                                    hide_labels=1,\n","#                                    export_format=\"jpg\"\n","#                                    )\n","#       i=i+1\n","\n","#       # format predictions ready to calculate mAP metric\n","#       img_predictions = img_predictions.to_coco_annotations()\n","#       img_predictions = {\n","#           \"boxes\": torch.tensor([pred['bbox'] for pred in img_predictions]),\n","#           \"scores\": torch.tensor([pred['score'] for pred in img_predictions]),\n","#           \"labels\": torch.tensor([pred['category_id'] for pred in img_predictions]) #needs to be int ?\n","#       }\n","#       # convert bboxes annotations to same format as ground truth\n","#       for index, box in enumerate(img_predictions['boxes']):\n","#         x,y,w,h = box\n","#         img_predictions['boxes'][index][2] = x+w\n","#         img_predictions['boxes'][index][3] = y+h\n","\n","\n","#       boxes = img_predictions['boxes']\n","#       # apply nms\n","#       if len(boxes) > 1:\n","#         scores = img_predictions['scores']\n","#         indices_to_keep = torchvision.ops.nms(boxes, scores, 0.8) #0.8\n","#         #print(len(pred['boxes']))\n","#         #print(indices_to_keep)\n","#         pred = {\n","#             'boxes': img_predictions['boxes'][indices_to_keep],\n","#             'labels': img_predictions['labels'][indices_to_keep],\n","#             'scores': img_predictions['scores'][indices_to_keep],\n","#         }\n","#         all_predictions.append(pred)\n","\n","#       else:\n","#         all_predictions.append(img_predictions)\n","\n","#       all_ground_truths.append({\"boxes\": targets['boxes'], \"labels\": targets['labels']})\n","\n","#       # visualisePredictions(0, test_dataset, [img_predictions])\n","\n","#   map_metric.update(all_predictions, all_ground_truths)\n","#   map_metric = map_metric.compute()\n","#   return {'map50': map_metric['map_50'], 'map75': map_metric['map_75'], 'map_per_class': map_metric['map_per_class']}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43502,"status":"aborted","timestamp":1743283640266,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"3mIjbnGjKTAN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43502,"status":"aborted","timestamp":1743283640267,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"Hcrfk8YqeuVV"},"outputs":[],"source":["import os\n","# import required functions, classes\n","from sahi import AutoDetectionModel\n","from sahi.predict import get_sliced_prediction, predict, get_prediction\n","from sahi.utils.file import download_from_url\n","from sahi.utils.cv import read_image\n","from IPython.display import Image\n","\n","detection_model = AutoDetectionModel.from_pretrained(\n","    model_type='torchvision',\n","    # model=cbam_model_wrapper.model,\n","    model = custom_model_wrapper.model,\n","    confidence_threshold=0.8,\n","    # image_size=640,\n","    device=\"cuda:0\",\n","    load_at_init=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N70tuRs9O-DJ","executionInfo":{"status":"aborted","timestamp":1743283640267,"user_tz":0,"elapsed":43502,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"outputs":[],"source":["source_image_dir = \"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/COCO_2_classes_test/images\"\n","ground_truth = \"/content/drive/MyDrive/UoL Final Project/Project Data/official project data/COCO_2_classes_test/_annotations.coco.json\"\n","predictions = \"/content/drive/MyDrive/Colab Notebooks/sahi_visuals/COCO_preds/pretrained_fasterrcnn.json\"\n","\n","slice_height = 256\n","slice_width = 256\n","overlap_height_ratio = 0.5\n","overlap_width_ratio = 0.5\n","\n","sahi_inference_eval(detection_model=detection_model,\n","                    folder_path=source_image_dir,\n","                    slice_h_w=(slice_height, slice_width),\n","                    overlap_h_w=(overlap_height_ratio, overlap_width_ratio),\n","                    test_dataset_loader=test_dataset_loader,\n","                    img_export_path=\"/content/drive/MyDrive/Colab Notebooks/sahi_visuals/pretrained_fasterrcnn\",\n","                    prediction_json_path=predictions,\n","                    ground_truth_json_path=ground_truth\n","                    )"]},{"cell_type":"markdown","metadata":{"id":"59FMkDBXnmpd"},"source":["## Appendix A: <a class=\"anchor\" name=\"AppenA\"></a>\n","Attempts to override FasterRCNN forward() method"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43502,"status":"aborted","timestamp":1743283640268,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"Vgv8c_3brIOb"},"outputs":[],"source":["##***************************************TEST\n","from torchvision.models.detection import FasterRCNN\n","\n","import warnings\n","\n","class ModifiedFasterRCNN(FasterRCNN):\n","  def forward(self, images, targets=None):\n","        # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n","        \"\"\"\n","        Args:\n","            images (list[Tensor]): images to be processed\n","            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n","\n","        Returns:\n","            result (list[BoxList] or dict[Tensor]): the output from the model.\n","                During training, it returns a dict[Tensor] which contains the losses.\n","                During testing, it returns list[BoxList] contains additional fields\n","                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n","\n","        \"\"\"\n","        if self.training and targets is None:\n","            raise ValueError(\"In training mode, targets should be passed\")\n","        if self.training:\n","            assert targets is not None\n","            for target in targets:\n","                boxes = target[\"boxes\"]\n","                if isinstance(boxes, torch.Tensor):\n","                    if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n","                        raise ValueError(f\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\")\n","                else:\n","                    raise ValueError(f\"Expected target boxes to be of type Tensor, got {type(boxes)}.\")\n","\n","        original_image_sizes: List[Tuple[int, int]] = []\n","        for img in images:\n","            val = img.shape[-2:]\n","            assert len(val) == 2\n","            original_image_sizes.append((val[0], val[1]))\n","\n","        images, targets = self.transform(images, targets)\n","\n","        # Check for degenerate boxes\n","        # TODO: Move this to a function\n","        if targets is not None:\n","            for target_idx, target in enumerate(targets):\n","                boxes = target[\"boxes\"]\n","                degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n","                if degenerate_boxes.any():\n","                    # print the first degenerate box\n","                    bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n","                    degen_bb: List[float] = boxes[bb_idx].tolist()\n","                    raise ValueError(\n","                        \"All bounding boxes should have positive height and width.\"\n","                        f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n","                    )\n","\n","        features = self.backbone(images.tensors)\n","        if isinstance(features, torch.Tensor):\n","            features = OrderedDict([(\"0\", features)])\n","        proposals, proposal_losses = self.rpn(images, features, targets)\n","        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n","        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n","\n","        losses = {}\n","        losses.update(detector_losses)\n","        losses.update(proposal_losses)\n","\n","        if torch.jit.is_scripting():\n","            if not self._has_warned:\n","                warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n","                self._has_warned = True\n","            return losses, detections\n","        else:\n","            #MODIFIED THIS LINE\n","            return losses, detections\n","\n","from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor, _validate_trainable_layers, _mobilenet_extractor\n","from torchvision.ops import misc as misc_nn_ops\n","from torchvision.models.resnet import resnet50\n","from torchvision._internally_replaced_utils import load_state_dict_from_url\n","from torchvision.models.detection._utils import overwrite_eps\n","\n","model_urls = {\n","    \"fasterrcnn_resnet50_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\",\n","    \"fasterrcnn_mobilenet_v3_large_320_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_320_fpn-907ea3f9.pth\",\n","    \"fasterrcnn_mobilenet_v3_large_fpn_coco\": \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\",\n","}\n","\n","def modified_fasterrcnn_resnet50_fpn(\n","    pretrained=False, progress=True, num_classes=91, pretrained_backbone=True, trainable_backbone_layers=None, **kwargs\n","):\n","    \"\"\"\n","    Constructs a Faster R-CNN model with a ResNet-50-FPN backbone.\n","\n","    Reference: `\"Faster R-CNN: Towards Real-Time Object Detection with\n","    Region Proposal Networks\" <https://arxiv.org/abs/1506.01497>`_.\n","\n","    The input to the model is expected to be a list of tensors, each of shape ``[C, H, W]``, one for each\n","    image, and should be in ``0-1`` range. Different images can have different sizes.\n","\n","    The behavior of the model changes depending if it is in training or evaluation mode.\n","\n","    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n","    containing:\n","\n","        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (``Int64Tensor[N]``): the class label for each ground-truth box\n","\n","    The model returns a ``Dict[Tensor]`` during training, containing the classification and regression\n","    losses for both the RPN and the R-CNN.\n","\n","    During inference, the model requires only the input tensors, and returns the post-processed\n","    predictions as a ``List[Dict[Tensor]]``, one for each input image. The fields of the ``Dict`` are as\n","    follows, where ``N`` is the number of detections:\n","\n","        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n","          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n","        - labels (``Int64Tensor[N]``): the predicted labels for each detection\n","        - scores (``Tensor[N]``): the scores of each detection\n","\n","    For more details on the output, you may refer to :ref:`instance_seg_output`.\n","\n","    Faster R-CNN is exportable to ONNX for a fixed batch size with inputs images of fixed size.\n","\n","    Example::\n","\n","        >>> model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","        >>> # For training\n","        >>> images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n","        >>> boxes[:, :, 2:4] = boxes[:, :, 0:2] + boxes[:, :, 2:4]\n","        >>> labels = torch.randint(1, 91, (4, 11))\n","        >>> images = list(image for image in images)\n","        >>> targets = []\n","        >>> for i in range(len(images)):\n","        >>>     d = {}\n","        >>>     d['boxes'] = boxes[i]\n","        >>>     d['labels'] = labels[i]\n","        >>>     targets.append(d)\n","        >>> output = model(images, targets)\n","        >>> # For inference\n","        >>> model.eval()\n","        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n","        >>> predictions = model(x)\n","        >>>\n","        >>> # optionally, if you want to export the model to ONNX:\n","        >>> torch.onnx.export(model, x, \"faster_rcnn.onnx\", opset_version = 11)\n","\n","    Args:\n","        pretrained (bool): If True, returns a model pre-trained on COCO train2017\n","        progress (bool): If True, displays a progress bar of the download to stderr\n","        num_classes (int): number of output classes of the model (including the background)\n","        pretrained_backbone (bool): If True, returns a model with backbone pre-trained on Imagenet\n","        trainable_backbone_layers (int): number of trainable (not frozen) resnet layers starting from final block.\n","            Valid values are between 0 and 5, with 5 meaning all backbone layers are trainable. If ``None`` is\n","            passed (the default) this value is set to 3.\n","    \"\"\"\n","    trainable_backbone_layers = _validate_trainable_layers(\n","        pretrained or pretrained_backbone, trainable_backbone_layers, 5, 3\n","    )\n","\n","    if pretrained:\n","        # no need to download the backbone if pretrained is set\n","        pretrained_backbone = False\n","\n","    backbone = resnet50(pretrained=pretrained_backbone, progress=progress, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n","    backbone = _resnet_fpn_extractor(backbone, trainable_backbone_layers)\n","    #MODIFIED THE LINE BELOW\n","    model = ModifiedFasterRCNN(backbone, num_classes, **kwargs)\n","\n","    if pretrained:\n","        state_dict = load_state_dict_from_url(model_urls[\"fasterrcnn_resnet50_fpn_coco\"], progress=progress)\n","        model.load_state_dict(state_dict)\n","        overwrite_eps(model, 0.0)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43501,"status":"aborted","timestamp":1743283640268,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"kHcdohwJsk8J"},"outputs":[],"source":["#TEST\n","model = modified_fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","num_classes = 4  #+1 for background - there are 3 classes\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43501,"status":"aborted","timestamp":1743283640269,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"INtN-Rj0ItMS"},"outputs":[],"source":["# custom_model_state = custom_model.state_dict().__str__()\n","# model_state = model.state_dict().__str__()\n","# model_state == custom_model_state"]},{"cell_type":"markdown","metadata":{"id":"uI8xAC1p-jET"},"source":["## Appendix B: <a class=\"anchor\" name=\"AppenB\"></a>\n","Code used to check validity of Faster R-CNN implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43502,"status":"aborted","timestamp":1743283640270,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"4b0670af-ea6a-4b59-ae58-79767e4fad07"},"outputs":[],"source":["#model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #- 0.1568 mAP50\n","#model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True) - 0.1080 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True) - 0.0434 mAP50\n","#model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True) - 0.0161 mAP50\n","\n","#needs separate fine tuning process for this model\n","#model = torchvision.models.detection.ssd300_vgg16(pretrained=True)\n","\n","# BASELINE CHECK\n","# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","# from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights\n","\n","# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n","\n","# num_classes = 4  #+1 for background - there are 3 classes\n","# in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":43501,"status":"aborted","timestamp":1743283640270,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"},"user_tz":0},"id":"BTktjy3jvw-s"},"outputs":[],"source":["# optimizer = torch.optim.SGD(model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","# num_epochs = 15\n","# model_pipeline(model, optimizer, num_epochs, train_dataset_loader,  val_dataset_loader, test_dataset_loader, device, False)"]},{"cell_type":"markdown","source":["## Appendix C: Alternate version of Fovea backbone\n","took longer and had slightly worse performance:"],"metadata":{"id":"AnTbGh_DDZTa"}},{"cell_type":"code","source":["# the code in this cell is adapted from Kapil Sachdeva:\n","# https://www.youtube.com/watch?v=FKsgO0U7CUw&list=PLjRC2jqp3e7QNio7NJXRh_gvAdLV6_aZC&index=132\n","# the code has been modified so that it accomodates all 4 layers (feature maps) of the ResNetCBAM backbone (defined above)\n","from functools import partial\n","from torchvision.ops.misc import Conv2dNormActivation\n","\n","class FPN3Layers(nn.Module):\n","  def __init__(self, in_channels: list[int], out_channels: int):\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","    self.hl_channel_reducer = ConvBlock(\n","        in_channels=in_channels[0],\n","        out_channels=out_channels,\n","        kernel_size=1,\n","    )\n","\n","    self.ml_1_channel_reducer = ConvBlock(\n","        in_channels=in_channels[1],\n","        out_channels=out_channels,\n","        kernel_size=1,\n","    )\n","\n","    self.ll_channel_reducer = ConvBlock(\n","        in_channels=in_channels[2],\n","        out_channels=out_channels,\n","        kernel_size=1,\n","    )\n","\n","    self.ml_1_smoother = ConvBlock(\n","        in_channels=out_channels,\n","        out_channels=out_channels,\n","    )\n","\n","    self.ll_smoother = ConvBlock(\n","        in_channels=out_channels,\n","        out_channels=out_channels,\n","    )\n","\n","  def forward(self,\n","              hl_features:torch.Tensor,\n","              ml_1_features:torch.Tensor,\n","              ll_features:torch.Tensor):\n","\n","    hl_channel_r = self.hl_channel_reducer(hl_features)\n","    del hl_features\n","    ml_channel_1_r = self.ml_1_channel_reducer(ml_1_features)\n","    del ml_1_features\n","    ll_channel_r = self.ll_channel_reducer(ll_features)\n","    del ll_features\n","\n","    # addition 1\n","    upsample_hl = F.interpolate(\n","        hl_channel_r,\n","        size=[ml_channel_1_r.size(2), ml_channel_1_r.size(3)],\n","        mode=\"nearest\",\n","    )\n","\n","    fused_hl_ml_1 = upsample_hl + ml_channel_1_r\n","    smoothed_ml_1_features = self.ml_1_smoother(fused_hl_ml_1)\n","    # memory management\n","    del ml_channel_1_r, upsample_hl, fused_hl_ml_1\n","\n","    # addition 3\n","    upsample_ml_1 = F.interpolate(\n","        smoothed_ml_1_features,\n","        size=[ll_channel_r.size(2), ll_channel_r.size(3)],\n","        mode=\"nearest\",\n","    )\n","    fused_ml_ll = upsample_ml_1 + ll_channel_r\n","    smoothed_ll_features = self.ll_smoother(fused_ml_ll)\n","    # memory management\n","    del ll_channel_r, fused_ml_ll, upsample_ml_1\n","\n","\n","    out = [hl_channel_r,\n","           smoothed_ml_1_features,\n","           smoothed_ll_features]\n","    return out"],"metadata":{"id":"CTTHH1EUD2AA","executionInfo":{"status":"aborted","timestamp":1743283640270,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ShallowFovea(nn.Module):\n","  def __init__(self, num_channels: int, out_channels: int):\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","    channel_increment_halves = out_channels-num_channels // 2\n","    channel_increment_quarter = out_channels-num_channels // 4\n","\n","    self.branch_1 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels+channel_increment_halves, kernel_size=1),\n","        ConvBlock(in_channels=num_channels+channel_increment_halves, out_channels=num_channels+(channel_increment_halves*2), kernel_size=7),\n","    )\n","\n","    self.branch_2 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels+channel_increment_quarter, kernel_size=1),\n","        ConvBlock(in_channels=num_channels+channel_increment_quarter, out_channels=num_channels+(channel_increment_quarter*2), kernel_size=(1,7)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*2), out_channels=num_channels+(channel_increment_quarter*3), kernel_size=(7,1)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*3), out_channels=num_channels+(channel_increment_quarter*4), kernel_size=7, dilation=3),\n","    )\n","\n","    self.branch_3 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels+channel_increment_quarter, kernel_size=1),\n","        ConvBlock(in_channels=num_channels+channel_increment_quarter, out_channels=num_channels+(channel_increment_quarter*2), kernel_size=(3,1)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*2), out_channels=num_channels+(channel_increment_quarter*3), kernel_size=(1,3)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*3), out_channels=num_channels+(channel_increment_quarter*4), kernel_size=7, dilation=3),\n","    )\n","\n","    self.branch_4 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=out_channels, kernel_size=1),\n","    )\n","\n","    self.downsampler = nn.LazyConv2d(out_channels=out_channels, kernel_size=3)\n","\n","\n","  def forward(self, x):\n","    branch_1 = self.branch_1(x)\n","    branch_2 = self.branch_2(x)\n","    branch_3 = self.branch_3(x)\n","    branch_4 = self.branch_4(x)\n","\n","    upsampled_branch_1 = F.interpolate(\n","        branch_1,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_1 #remove intermediate calculation from GPU RAM clear\n","    upsampled_branch_2 = F.interpolate(\n","        branch_2,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_2 #remove intermediate calculation from GPU RAM clear\n","\n","    branch_concat = torch.cat( (upsampled_branch_1,\n","                                upsampled_branch_2,\n","                                branch_3), dim=1 ) #concat on first dimension to allow for different number of channels\n","    del upsampled_branch_1, upsampled_branch_2, branch_3\n","\n","\n","    # prep ready for element wise multiplication\n","    downsampled_branch_concat = self.downsampler(branch_concat)\n","    del branch_concat #remove intermediate calculation from GPU RAM clear\n","\n","    resized_branch_concat = F.interpolate(\n","        downsampled_branch_concat,\n","        size=[branch_4.size(2), branch_4.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del downsampled_branch_concat\n","    result = branch_4 * resized_branch_concat\n","    del branch_4, resized_branch_concat\n","    return result"],"metadata":{"id":"DsSgt62MOUpQ","executionInfo":{"status":"aborted","timestamp":1743283640271,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class DeepFovea(nn.Module):\n","  def __init__(self, num_channels: int, out_channels: int):\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","    channel_increment_halves = out_channels-num_channels // 2\n","    channel_increment_quarter = out_channels-num_channels // 4\n","\n","    self.branch_1 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels+channel_increment_halves, kernel_size=1),\n","        ConvBlock(in_channels=num_channels+channel_increment_halves, out_channels=num_channels+(channel_increment_halves*2), kernel_size=3),\n","    )\n","\n","    self.branch_2 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels+channel_increment_quarter, kernel_size=1),\n","        ConvBlock(in_channels=num_channels+channel_increment_quarter, out_channels=num_channels+(channel_increment_quarter*2), kernel_size=(1,3)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*2), out_channels=num_channels+(channel_increment_quarter*3), kernel_size=(3,1)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*3), out_channels=num_channels+(channel_increment_quarter*4), kernel_size=3, dilation=3),\n","    )\n","\n","    self.branch_3 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=num_channels+channel_increment_quarter, kernel_size=1),\n","        ConvBlock(in_channels=num_channels+channel_increment_quarter, out_channels=num_channels+(channel_increment_quarter*2), kernel_size=(3,1)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*2), out_channels=num_channels+(channel_increment_quarter*3), kernel_size=(1,3)),\n","        ConvBlock(in_channels=num_channels+(channel_increment_quarter*3), out_channels=num_channels+(channel_increment_quarter*4), kernel_size=3, dilation=3),\n","    )\n","\n","    self.branch_4 = nn.Sequential(\n","        ConvBlock(in_channels=num_channels, out_channels=out_channels, kernel_size=1),\n","    )\n","\n","    self.downsampler = nn.LazyConv2d(out_channels=out_channels, kernel_size=3)\n","\n","\n","  def forward(self, x):\n","    branch_1 = self.branch_1(x)\n","    branch_2 = self.branch_2(x)\n","    branch_3 = self.branch_3(x)\n","    branch_4 = self.branch_4(x)\n","\n","    upsampled_branch_1 = F.interpolate(\n","        branch_1,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_1 #remove intermediate calculation from GPU RAM clear\n","\n","    upsampled_branch_2 = F.interpolate(\n","        branch_2,\n","        size=[branch_3.size(2), branch_3.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del branch_2 #remove intermediate calculation from GPU RAM clear\n","\n","    branch_concat = torch.cat( (upsampled_branch_1,\n","                                upsampled_branch_2,\n","                                branch_3), dim=1 ) #concat on first dimension to allow for different number of channels\n","\n","    del upsampled_branch_1, upsampled_branch_2, branch_3\n","\n","    # prep ready for element wise multiplication\n","    downsampled_branch_concat = self.downsampler(branch_concat)\n","    del branch_concat  #remove intermediate calculation from GPU RAM clear\n","\n","    resized_branch_concat = F.interpolate(\n","        downsampled_branch_concat,\n","        size=[branch_4.size(2), branch_4.size(3)],\n","        mode=\"bilinear\",\n","    )\n","    del downsampled_branch_concat\n","\n","    result = branch_4 * resized_branch_concat\n","    del branch_4, resized_branch_concat\n","    return result"],"metadata":{"id":"NWMCAsfERb3x","executionInfo":{"status":"aborted","timestamp":1743283640271,"user_tz":0,"elapsed":43499,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TO DO:\n","# - experiment with residual connections\n","\n","\n","class FoveaBackbone(nn.Module):\n","  def __init__(self, in_channels):\n","    super().__init__()\n","\n","    LeakyRelu_Inplace = partial(\n","        nn.LeakyReLU,\n","        negative_slope=0.1,\n","        inplace=True,\n","    )\n","\n","    ConvBlock = partial(Conv2dNormActivation, activation_layer=LeakyRelu_Inplace)\n","\n","    #stride not mention in EVM paper but needed as imgs so large\n","    self.block_1 = nn.Sequential(\n","        ConvBlock(in_channels=in_channels, out_channels=32, kernel_size=3, stride=2),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","\n","    self.shallow_fovea_1 = ShallowFovea(num_channels=32, out_channels=64)\n","    self.deep_fovea_1 = DeepFovea(num_channels=32, out_channels=64)\n","\n","    self.shallow_fovea_2 = ShallowFovea(num_channels=64, out_channels=128)\n","    self.deep_fovea_2 = DeepFovea(num_channels=64, out_channels=128)\n","\n","    self.shallow_fovea_3 = ShallowFovea(num_channels=128, out_channels=256)\n","    self.deep_fovea_3 = DeepFovea(num_channels=128, out_channels=256)\n","\n","    # self.shallow_fovea_4 = ShallowFovea(num_channels=256, out_channels=512)\n","    # self.deep_fovea_4 = DeepFovea(num_channels=256, out_channels=512)\n","\n","    self.max_pooler = nn.MaxPool2d(kernel_size=2)\n","    self.fpn = FPN3Layers(in_channels=[64, 128, 256], out_channels=256)\n","\n","    self.cbam_scaler_1 = nn.Parameter(torch.zeros(1))\n","    self.cbam_1 = ScaledCBAM(in_channels=64, scaling_parameter=self.cbam_scaler_1)\n","    self.cbam_scaler_2 = nn.Parameter(torch.zeros(1))\n","    self.cbam_2 = ScaledCBAM(in_channels=128, scaling_parameter=self.cbam_scaler_2)\n","    self.cbam_scaler_3 = nn.Parameter(torch.zeros(1))\n","    self.cbam_3 = ScaledCBAM(in_channels=256, scaling_parameter=self.cbam_scaler_3)\n","    # self.cbam_scaler_4 = nn.Parameter(torch.zeros(1))\n","    # self.cbam_4 = ScaledCBAM(in_channels=512, scaling_parameter=self.cbam_scaler_4)\n","\n","  def forward(self, x):\n","    x = self.block_1(x)\n","\n","    shallow_feats_1 = self.shallow_fovea_1(x)\n","    deep_feats_1 = self.deep_fovea_1(x)\n","    stage_1_results = shallow_feats_1 + deep_feats_1\n","    stage_1_results_pooled = self.max_pooler(stage_1_results) #reduce computational load for successive layers\n","    stage_1_results_w_cbam = self.cbam_1(stage_1_results)\n","    # memory management\n","    del shallow_feats_1, deep_feats_1, stage_1_results\n","    # upsample and add residual connection - or downsample ?\n","    # try one with upsample and one with downsample\n","    # print(x.size())\n","    # print(stage_1_results_w_cbam.size())\n","    # stop\n","    # torch.Size([8, 32, 200, 272])\n","    # torch.Size([8, 64, 200, 272])\n","\n","    shallow_feats_2 = self.shallow_fovea_2(stage_1_results_pooled)\n","    deep_feats_2 = self.deep_fovea_2(stage_1_results_pooled)\n","    stage_2_results = shallow_feats_2 + deep_feats_2\n","    stage_2_results_pooled = self.max_pooler(stage_2_results) #reduce computational load for successive layers\n","    stage_2_results_w_cbam = self.cbam_2(stage_2_results)\n","    # memory management\n","    del shallow_feats_2, deep_feats_2, stage_2_results, stage_1_results_pooled,\n","\n","    shallow_feats_3 = self.shallow_fovea_3(stage_2_results_pooled)\n","    deep_feats_3 = self.deep_fovea_3(stage_2_results_pooled)\n","    stage_3_results = shallow_feats_3 + deep_feats_3\n","    stage_3_results_pooled = self.max_pooler(stage_3_results) #reduce computational load for successive layers\n","    stage_3_results_w_cbam = self.cbam_3(stage_3_results)\n","    # memory management\n","    del shallow_feats_3, deep_feats_3, stage_3_results, stage_2_results_pooled,\n","\n","    # shallow_feats_4 = self.shallow_fovea_4(stage_3_results_pooled)\n","    # deep_feats_4 = self.deep_fovea_4(stage_3_results_pooled)\n","    # stage_4_results = shallow_feats_4 + deep_feats_4\n","    # stage_4_results_w_cbam = self.cbam_4(stage_4_results)\n","    # # memory management\n","    # del shallow_feats_4, deep_feats_4, stage_4_results, stage_3_results_pooled,\n","\n","    feat_maps = self.fpn(hl_features = stage_1_results_w_cbam,\n","                        ml_1_features= stage_2_results_w_cbam,\n","                        ll_features = stage_3_results_w_cbam, )\n","\n","    del stage_1_results_w_cbam, stage_2_results_w_cbam, stage_3_results_w_cbam, #stage_4_results_w_cbam\n","\n","    return OrderedDict([(k, v) for k, v in zip([\"hl_feats\",\n","                                                \"ml_1_feats\",\n","                                                # \"ml_2_feats\",\n","                                                \"ll_feats\"], feat_maps)])"],"metadata":{"id":"v0KHg4j26rOg","executionInfo":{"status":"aborted","timestamp":1743283640272,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision\n","from torchvision.models.detection import FasterRCNN\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","backbone = FoveaBackbone(3)\n","backbone.out_channels = 256\n","\n","anchor_sizes = ((8,), (44,), (128,))  #16, 32, 64, 128\n","aspect_ratios = (((0.5,), (1.0,), (2.0,)),) * len(anchor_sizes)\n","anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n","\n","# Feature maps to perform RoI cropping.\n","# If backbone returns a Tensor, `featmap_names` is expected to\n","# be [0]. We can choose which feature maps to use.\n","roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n","    featmap_names=[\"hl_feats\",\n","                    \"ml_1_feats\",\n","                    # \"ml_2_feats\",\n","                    \"ll_feats\"],\n","    output_size=7,\n","    sampling_ratio=2\n",")\n","\n","fovea_model_wrapper = FasterRCNNWrapper(\n","    backbone=backbone,\n","    num_classes=3,\n","    rpn_anchor_generator=anchor_generator,\n","    box_roi_pool=roi_pooler\n",")\n","num_classes=3 #3 classes +1 to account for background\n","fovea_model_wrapper.override_prediction_heads(num_classes)"],"metadata":{"id":"J8GF4BnO7KET","executionInfo":{"status":"aborted","timestamp":1743283640272,"user_tz":0,"elapsed":43499,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fovea_model_wrapper.model.to(device)\n","# optimizer = torch.optim.SGD(model.model.parameters() , lr=0.0005, momentum=0.9, weight_decay=0.0005)\n","optimizer = torch.optim.Adam(fovea_model_wrapper.model.parameters(), lr=0.0005)\n","num_epochs = 15\n","model_pipeline(fovea_model_wrapper , optimizer, num_epochs, train_dataset_loader,\n","               val_dataset_loader, test_dataset_loader, device,\n","               True, \"Fovea Tracker\")"],"metadata":{"id":"9Va3HYp87OU7","executionInfo":{"status":"aborted","timestamp":1743283640274,"user_tz":0,"elapsed":43500,"user":{"displayName":"Amy Tidd","userId":"11893287281362884867"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"toc_visible":true,"collapsed_sections":["KjpmwmZo4Spp"],"gpuType":"A100","mount_file_id":"1JCtwPguP-eUqZEATKRbuAm_E21850bjI","authorship_tag":"ABX9TyPUIAEmlxU8xtzo877jscOW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}